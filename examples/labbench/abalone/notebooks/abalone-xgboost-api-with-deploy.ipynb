{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bb6a4a-2139-4b1d-af8b-7d66d7f39264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression with Amazon SageMaker XGBoost algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4758ce3-3eb7-45b5-9c32-071a39ec40f1",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Source of this notebook: https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_abalone.html\n",
    "\n",
    "Please refer to this document for citations and detailled explanations\n",
    "\n",
    "This notebook has been adapted to run inside Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74de2430-c663-4124-85a2-7a950f17c94e",
   "metadata": {},
   "source": [
    "# Requirements \n",
    "\n",
    "- preinstalled SageMaker Studio image Python3 for DataScience (tested on DataScience 3.0)\n",
    "- packages: sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e3ff9a-755c-480e-91dd-83b0462703db",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d88311e-5f23-4d45-8085-04d93ab81aac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 875 ms, sys: 171 ms, total: 1.05 s\n",
      "Wall time: 1.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "s3_client = boto3.client(\"s3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f6b3f9-5704-4f42-8b1c-74e319812b37",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "Kind of, data is not split. Same file is used as train and test datasets. \n",
    "\n",
    "TODO Check methods at end of this file and implement the data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29713323-7b31-4359-8f5b-229d68415fe3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download sagemaker-sample-files datasets/tabular/uci_abalone/train/abalone.train -> generated/abalone.train\n",
      "upload generated/abalone.train -> sagemaker-eu-west-1-102959664345 stage-labbench/abalone/xgboost-default/train/abalone.train\n",
      "download sagemaker-sample-files datasets/tabular/uci_abalone/test/abalone.test -> generated/abalone.test\n",
      "upload generated/abalone.test -> sagemaker-eu-west-1-102959664345 stage-labbench/abalone/xgboost-default/test/abalone.test\n",
      "download sagemaker-sample-files datasets/tabular/uci_abalone/validation/abalone.validation -> generated/abalone.validation\n",
      "upload generated/abalone.validation -> sagemaker-eu-west-1-102959664345 stage-labbench/abalone/xgboost-default/validation/abalone.validation\n",
      "CPU times: user 226 ms, sys: 30.6 ms, total: 257 ms\n",
      "Wall time: 1.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "# S3 bucket where the training data is located.\n",
    "data_bucket = \"sagemaker-sample-files\"\n",
    "data_prefix = \"datasets/tabular/uci_abalone\"\n",
    "data_bucket_path = f\"s3://{data_bucket}\"\n",
    "\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "# Feel free to specify a different bucket and prefix\n",
    "output_bucket = sagemaker.Session().default_bucket()\n",
    "output_prefix = \"stage-labbench/abalone/xgboost-default\"\n",
    "output_bucket_path = f\"s3://{output_bucket}\"\n",
    "\n",
    "# tmp directory\n",
    "output_folder=\"generated\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for data_category in [\"train\", \"test\", \"validation\"]:\n",
    "    data_key = f\"{data_prefix}/{data_category}/abalone.{data_category}\"\n",
    "    output_key = f\"{output_prefix}/{data_category}/abalone.{data_category}\"\n",
    "    data_filename = os.path.join(output_folder, f\"abalone.{data_category}\")\n",
    "    print(f\"download {data_bucket} {data_key} -> {data_filename}\")\n",
    "    s3_client.download_file(data_bucket, data_key, data_filename)\n",
    "    print(f\"upload {data_filename} -> {output_bucket} {output_key}\")\n",
    "    s3_client.upload_file(data_filename, output_bucket, output_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31211ba2-4ae5-4e6d-ad28-c74faf0aed93",
   "metadata": {},
   "source": [
    "# Training the XGBoost model\n",
    "\n",
    "Training takes between 5 and 6 minutes.\n",
    "\n",
    "Training can be done by either calling SageMaker Training with a set of hyperparameters values to train with, or by leveraging hyperparameter tuning (HPO) which finds the best version of a model by running many training jobs on your dataset using the algorithm and ranges of hyperparameters that you specify. It then chooses the hyperparameter values that result in a model that performs the best, as measured by a metric that you choose.\n",
    "\n",
    "In this notebook, both methods are used for demonstration purposes, but the model that the HPO job creates is the one that is eventually hosted. You can instead choose to deploy the model created by the standalone training job by changing the below variable deploy_amt_model to False.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93bac6d4-7fa1-4388-87b9-dd9dd324e857",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initializing common variables\n",
    "\n",
    "container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.5-1\")\n",
    "client = boto3.client(\"sagemaker\", region_name=region)\n",
    "deploy_amt_model = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f017b7c0-965a-45a9-b741-737320d4977c",
   "metadata": {},
   "source": [
    "## Training with hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b09368eb-a1d2-4d09-b2f8-16a072bf431f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a training job with name: DEMO-xgboost-regression-2022-12-26-20-10-20. It will take between 5 and 6 minutes to complete.\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "Completed\n",
      "CPU times: user 52.6 ms, sys: 13.6 ms, total: 66.2 ms\n",
      "Wall time: 3min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "import time\n",
    "\n",
    "training_job_name = f\"DEMO-xgboost-regression-{strftime('%Y-%m-%d-%H-%M-%S', gmtime())}\"\n",
    "\n",
    "# Ensure that the training and validation data folders generated above are reflected in the \"InputDataConfig\" parameter below.\n",
    "\n",
    "create_training_params = {\n",
    "    \"AlgorithmSpecification\": {\"TrainingImage\": container, \"TrainingInputMode\": \"File\"},\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\"S3OutputPath\": f\"{output_bucket_path}/{output_prefix}/single-xgboost\"},\n",
    "    \"ResourceConfig\": {\"InstanceCount\": 1, \"InstanceType\": \"ml.m5.2xlarge\", \"VolumeSizeInGB\": 5},\n",
    "    \"TrainingJobName\": training_job_name,\n",
    "    \"HyperParameters\": {\n",
    "        \"max_depth\": \"5\",\n",
    "        \"eta\": \"0.2\",\n",
    "        \"gamma\": \"4\",\n",
    "        \"min_child_weight\": \"6\",\n",
    "        \"subsample\": \"0.7\",\n",
    "        \"objective\": \"reg:linear\",\n",
    "        \"num_round\": \"50\",\n",
    "        \"verbosity\": \"2\",\n",
    "    },\n",
    "    \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 3600},\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": f\"{output_bucket_path}/{output_prefix}/train\",\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"libsvm\",\n",
    "            \"CompressionType\": \"None\",\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": f\"{output_bucket_path}/{output_prefix}/validation\",\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"libsvm\",\n",
    "            \"CompressionType\": \"None\",\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "print(f\"Creating a training job with name: {training_job_name}. It will take between 5 and 6 minutes to complete.\")\n",
    "client.create_training_job(**create_training_params)\n",
    "status = client.describe_training_job(TrainingJobName=training_job_name)[\"TrainingJobStatus\"]\n",
    "print(status)\n",
    "while status != \"Completed\" and status != \"Failed\":\n",
    "    time.sleep(60)\n",
    "    status = client.describe_training_job(TrainingJobName=training_job_name)[\"TrainingJobStatus\"]\n",
    "    print(status)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4701da9a-07ce-442d-8b46-976e59153136",
   "metadata": {},
   "source": [
    "Note that the “validation” channel has been initialized too. The SageMaker XGBoost algorithm actually calculates RMSE and writes it to the CloudWatch logs on the data passed to the “validation” channel.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803b82e9-d729-4b1a-afa0-394f25f3bf5b",
   "metadata": {},
   "source": [
    "## Tuning with SageMaker Automatic Model Tuning\n",
    "\n",
    "To create a tuning job using the AWS SageMaker Automatic Model Tuning API, you need to define 3 attributes.\n",
    "\n",
    "- the tuning job name (string)\n",
    "- the tuning job config (to specify settings for the hyperparameter tuning job - JSON object)\n",
    "- training job definition (to configure the training jobs that the tuning job launches - JSON object).\n",
    "\n",
    "To learn more about that, refer to the Configure and Launch a Hyperparameter Tuning Job documentation.\n",
    "\n",
    "Note that the tuning job will 12-17 minutes to complete.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbeea193-6281-493a-b0b1-59711e1f5fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a tuning job with name: DEMO-xgboost-reg-HPO-26-20-37-34. It will take between 12 and 17 minutes to complete.\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "Completed\n",
      "CPU times: user 105 ms, sys: 7.34 ms, total: 112 ms\n",
      "Wall time: 6min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from time import gmtime, strftime, sleep\n",
    "\n",
    "tuning_job_name = \"DEMO-xgboost-reg-HPO-\" + strftime(\"%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "tuning_job_config = {\n",
    "    \"ParameterRanges\": {\n",
    "        \"CategoricalParameterRanges\": [],\n",
    "        \"ContinuousParameterRanges\": [\n",
    "            {\n",
    "                \"MaxValue\": \"0.5\",\n",
    "                \"MinValue\": \"0.1\",\n",
    "                \"Name\": \"eta\",\n",
    "            },\n",
    "            {\n",
    "                \"MaxValue\": \"5\",\n",
    "                \"MinValue\": \"0\",\n",
    "                \"Name\": \"gamma\",\n",
    "            },\n",
    "            {\n",
    "                \"MaxValue\": \"120\",\n",
    "                \"MinValue\": \"0\",\n",
    "                \"Name\": \"min_child_weight\",\n",
    "            },\n",
    "            {\n",
    "                \"MaxValue\": \"1\",\n",
    "                \"MinValue\": \"0.5\",\n",
    "                \"Name\": \"subsample\",\n",
    "            },\n",
    "            {\n",
    "                \"MaxValue\": \"2\",\n",
    "                \"MinValue\": \"0\",\n",
    "                \"Name\": \"alpha\",\n",
    "            },\n",
    "        ],\n",
    "        \"IntegerParameterRanges\": [\n",
    "            {\n",
    "                \"MaxValue\": \"10\",\n",
    "                \"MinValue\": \"0\",\n",
    "                \"Name\": \"max_depth\",\n",
    "            },\n",
    "            {\n",
    "                \"MaxValue\": \"4000\",\n",
    "                \"MinValue\": \"1\",\n",
    "                \"Name\": \"num_round\",\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    # SageMaker sets the following default limits for resources used by automatic model tuning:\n",
    "    # https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-limits.html\n",
    "    \"ResourceLimits\": {\n",
    "        # Increase the max number of training jobs for increased accuracy (and training time).\n",
    "        \"MaxNumberOfTrainingJobs\": 6,\n",
    "        # Change parallel training jobs run by AMT to reduce total training time. Constrained by your account limits.\n",
    "        # if max_jobs=max_parallel_jobs then Bayesian search turns to Random.\n",
    "        \"MaxParallelTrainingJobs\": 2\n",
    "    },\n",
    "    \"Strategy\": \"Bayesian\",\n",
    "    \"HyperParameterTuningJobObjective\": {\"MetricName\": \"validation:rmse\", \"Type\": \"Minimize\"},\n",
    "}\n",
    "\n",
    "training_job_definition = {\n",
    "    \"AlgorithmSpecification\": {\"TrainingImage\": container, \"TrainingInputMode\": \"File\"},\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": f\"{output_bucket_path}/{output_prefix}/train\",\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"libsvm\",\n",
    "            \"CompressionType\": \"None\",\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": f\"{output_bucket_path}/{output_prefix}/validation\",\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"libsvm\",\n",
    "            \"CompressionType\": \"None\",\n",
    "        },\n",
    "    ],\n",
    "    \"OutputDataConfig\": {\"S3OutputPath\": f\"{output_bucket_path}/{output_prefix}/single-xgboost\"},\n",
    "    \"ResourceConfig\": {\"InstanceCount\": 1, \"InstanceType\": \"ml.m5.2xlarge\", \"VolumeSizeInGB\": 5},\n",
    "    \"RoleArn\": role,\n",
    "    \"StaticHyperParameters\": {\n",
    "        \"objective\": \"reg:linear\",\n",
    "        \"verbosity\": \"2\",\n",
    "    },\n",
    "    \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 43200},\n",
    "}\n",
    "\n",
    "print(f\"Creating a tuning job with name: {tuning_job_name}. It will take between 12 and 17 minutes to complete.\")\n",
    "client.create_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuning_job_name,\n",
    "    HyperParameterTuningJobConfig=tuning_job_config,\n",
    "    TrainingJobDefinition=training_job_definition,\n",
    ")\n",
    "\n",
    "status = client.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuning_job_name)[\n",
    "    \"HyperParameterTuningJobStatus\"\n",
    "]\n",
    "print(status)\n",
    "while status != \"Completed\" and status != \"Failed\":\n",
    "    time.sleep(60)\n",
    "    status = client.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuning_job_name)[\n",
    "            \"HyperParameterTuningJobStatus\"\n",
    "    ]\n",
    "    print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2af6b5-e622-4313-b1b0-7e36758eb34e",
   "metadata": {},
   "source": [
    "# Set up hosting for the mode\n",
    "\n",
    "In order to set up hosting, we have to import the model from training to hosting.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e38b56a-82da-48a0-a1a7-a6c0fb4a8437",
   "metadata": {},
   "source": [
    "## Import model into hosting\n",
    "Register the model with hosting. This allows the flexibility of importing models trained elsewhere.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2856ab0d-aac2-4294-8d67-83fe8f0c1796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEMO-xgboost-reg-26-20-13-21-005-582c76ff-model\n",
      "s3://sagemaker-eu-west-1-102959664345/stage-labbench/abalone/xgboost-default/single-xgboost/DEMO-xgboost-reg-26-20-13-21-005-582c76ff/output/model.tar.gz\n",
      "arn:aws:sagemaker:eu-west-1:102959664345:model/demo-xgboost-reg-26-20-13-21-005-582c76ff-model\n",
      "CPU times: user 25.3 ms, sys: 3.03 ms, total: 28.3 ms\n",
      "Wall time: 527 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "if deploy_amt_model == True:\n",
    "    training_of_model_to_be_hosted = client.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuning_job_name)[\"BestTrainingJob\"][\"TrainingJobName\"]\n",
    "else:\n",
    "    training_of_model_to_be_hosted = training_job_name\n",
    "\n",
    "model_name = f\"{training_of_model_to_be_hosted}-model\"\n",
    "print(model_name)\n",
    "\n",
    "info = client.describe_training_job(TrainingJobName=training_of_model_to_be_hosted)\n",
    "model_data = info[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "print(model_data)\n",
    "\n",
    "primary_container = {\"Image\": container, \"ModelDataUrl\": model_data}\n",
    "\n",
    "create_model_response = client.create_model(\n",
    "    ModelName=model_name, ExecutionRoleArn=role, PrimaryContainer=primary_container\n",
    ")\n",
    "\n",
    "print(create_model_response[\"ModelArn\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dab6b0c-3f9c-4e0a-ac1e-4614a394ec1f",
   "metadata": {},
   "source": [
    "## Create endpoint configuration\n",
    "\n",
    "SageMaker supports configuring REST endpoints in hosting with multiple models, e.g. for A/B testing purposes. In order to support this, customers create an endpoint configuration, that describes the distribution of traffic across the models, whether split, shadowed, or sampled in some way. In addition, the endpoint configuration describes the instance type required for model deployment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb9ba41b-5419-4e26-aab5-bf815399b0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating endpoint config with name: DEMO-XGBoostEndpointConfig-2022-12-26-20-23-03.\n",
      "Endpoint Config Arn: arn:aws:sagemaker:eu-west-1:102959664345:endpoint-config/demo-xgboostendpointconfig-2022-12-26-20-23-03\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "endpoint_config_name = f\"DEMO-XGBoostEndpointConfig-{strftime('%Y-%m-%d-%H-%M-%S', gmtime())}\"\n",
    "print(f\"Creating endpoint config with name: {endpoint_config_name}.\")\n",
    "create_endpoint_config_response = client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.m5.xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(f\"Endpoint Config Arn: {create_endpoint_config_response['EndpointConfigArn']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819949c4-8d03-4136-b509-93742c1dddae",
   "metadata": {},
   "source": [
    "## Create endpoint \n",
    "\n",
    "Lastly, the customer creates the endpoint that serves up the model, through specifying the name and configuration defined above. The end result is an endpoint that can be validated and incorporated into production applications. This takes 9-11 minutes to complete.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bede0ecb-a9bd-4a60-ad1a-44eb630480b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating endpoint with name: DEMO-XGBoostEndpoint-2022-12-26-20-23-26. This will take between 9 and 11 minutes to complete.\n",
      "arn:aws:sagemaker:eu-west-1:102959664345:endpoint/demo-xgboostendpoint-2022-12-26-20-23-26\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Arn: arn:aws:sagemaker:eu-west-1:102959664345:endpoint/demo-xgboostendpoint-2022-12-26-20-23-26\n",
      "Status: InService\n",
      "CPU times: user 53.6 ms, sys: 15 ms, total: 68.6 ms\n",
      "Wall time: 3min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "endpoint_name = f'DEMO-XGBoostEndpoint-{strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())}'\n",
    "print(f\"Creating endpoint with name: {endpoint_name}. This will take between 9 and 11 minutes to complete.\")\n",
    "create_endpoint_response = client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(create_endpoint_response[\"EndpointArn\"])\n",
    "\n",
    "resp = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "while status == \"Creating\":\n",
    "    print(f\"Status: {status}\")\n",
    "    time.sleep(60)\n",
    "    resp = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "\n",
    "print(f\"Arn: {resp['EndpointArn']}\")\n",
    "print(f\"Status: {status}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcf45e5-1322-468e-a88d-8ea025d098ea",
   "metadata": {},
   "source": [
    "# Validate the model for use\n",
    "\n",
    "Finally, the customer can now validate the model for use. They can obtain the endpoint from the client library using the result from previous operations, and generate classifications from the trained model using that endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a91eb1c7-a103-40c7-a6b1-ceb52498ded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_client = boto3.client(\"runtime.sagemaker\", region_name=region)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97459098-58fc-4227-a70e-3e6fe243ea48",
   "metadata": {},
   "source": [
    "## Download test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1eb436c-e04b-4a79-aa12-a41e25307cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download sagemaker-sample-files datasets/tabular/uci_abalone/test/abalone.test -> generated/abalone.test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "FILE_TEST = \"abalone.test\"\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# tmp directory\n",
    "output_folder=\"generated\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "test_data_key = f\"{data_prefix}/test/{FILE_TEST}\"\n",
    "test_data_filename = os.path.join(output_folder, FILE_TEST)\n",
    "print(f\"download {data_bucket} {test_data_key} -> {test_data_filename}\")\n",
    "s3.download_file(data_bucket, test_data_key, test_data_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7e0191-334e-4b4c-94f3-81910f2311aa",
   "metadata": {},
   "source": [
    "## Start with a single prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6676da3-b874-4928-81be-b93482ef1a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -1 generated/abalone.test > generated/abalone.single.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fbbea2c-f24d-4df5-9a08-9d227bf80164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 12\n",
      "Prediction: 19\n",
      "CPU times: user 13.8 ms, sys: 4.31 ms, total: 18.1 ms\n",
      "Wall time: 121 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "from itertools import islice\n",
    "import math\n",
    "import struct\n",
    "\n",
    "single_test_data_filename = os.path.join(output_folder, \"abalone.single.test\" )\n",
    "file_name = single_test_data_filename\n",
    "with open(file_name, \"r\") as f:\n",
    "    payload = f.read().strip()\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, ContentType=\"text/x-libsvm\", Body=payload\n",
    ")\n",
    "result = response[\"Body\"].read()\n",
    "result = result.decode(\"utf-8\")\n",
    "result = result.split(\",\")\n",
    "result = [math.ceil(float(i)) for i in result]\n",
    "label = payload.strip(\" \").split()[0]\n",
    "print(f\"Label: {label}\\nPrediction: {result[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374c6a14-6cf4-45cc-9bdd-bbd1b5f0015e",
   "metadata": {},
   "source": [
    "## Run prediction against the batch file and compute the predictions accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734a5d06-857f-497d-8cb4-f2e6f33d5c5f",
   "metadata": {},
   "source": [
    "The following functions are helpers to run the prediction agains each item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caa9585a-c17a-49ed-ae57-badd1d87110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "\n",
    "def do_predict(data, endpoint_name, content_type):\n",
    "    payload = \"\\n\".join(data)\n",
    "    response = runtime_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=content_type, Body=payload\n",
    "    )\n",
    "    result = response[\"Body\"].read()\n",
    "    result = result.decode(\"utf-8\")\n",
    "    result = result.strip(\"\\n\").split(\"\\n\")\n",
    "    preds = [float(num) for num in result]\n",
    "    preds = [math.ceil(num) for num in preds]\n",
    "    return preds\n",
    "\n",
    "\n",
    "def batch_predict(data, batch_size, endpoint_name, content_type):\n",
    "    items = len(data)\n",
    "    arrs = []\n",
    "\n",
    "    for offset in range(0, items, batch_size):\n",
    "        if offset + batch_size < items:\n",
    "            results = do_predict(data[offset : (offset + batch_size)], endpoint_name, content_type)\n",
    "            arrs.extend(results)\n",
    "        else:\n",
    "            arrs.extend(do_predict(data[offset:items], endpoint_name, content_type))\n",
    "        sys.stdout.write(\".\")\n",
    "    return arrs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba7997c-d9db-46b6-be07-ca2e04fee0dd",
   "metadata": {},
   "source": [
    "The following helps us calculate the Median Absolute Percent Error (MdAPE) on the batch dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "291e6a1d-6566-4840-9a7f-c6c1751cc5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......\n",
      " Median Absolute Percent Error (MdAPE) =  0.14285714285714285\n",
      "CPU times: user 23.8 ms, sys: 6.31 ms, total: 30.1 ms\n",
      "Wall time: 197 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "with open(test_data_filename, \"r\") as f:\n",
    "    payload = f.read().strip()\n",
    "\n",
    "labels = [int(line.split(\" \")[0]) for line in payload.split(\"\\n\")]\n",
    "test_data = [line for line in payload.split(\"\\n\")]\n",
    "preds = batch_predict(test_data, 100, endpoint_name, \"text/x-libsvm\")\n",
    "\n",
    "print(\n",
    "    \"\\n Median Absolute Percent Error (MdAPE) = \",\n",
    "    np.median(np.abs(np.array(labels) - np.array(preds)) / np.array(labels)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cd5002-3b19-4db9-99d2-3419f70944ba",
   "metadata": {},
   "source": [
    "# Delete Endpoint\n",
    "\n",
    "Once you are done using the endpoint, you can use the following to delete it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "225ea521-ec11-432e-af14-97aed1f3cef6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'e14fb2f2-99e8-490d-aa9f-85a676cf96a9',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'e14fb2f2-99e8-490d-aa9f-85a676cf96a9',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Mon, 26 Dec 2022 20:33:47 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5264b59-f734-4f6b-8957-9ce894a6678b",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "TODO  data used in this notebook is not splitted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ece656b-d72c-4915-ab37-b40e72451644",
   "metadata": {},
   "source": [
    "## Data split and upload\n",
    "\n",
    "Following methods split the data into train/test/validation datasets and upload files to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f43447-5fc7-41b3-afb3-e26de28a9c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import boto3\n",
    "import random\n",
    "\n",
    "\n",
    "def data_split(\n",
    "    FILE_DATA,\n",
    "    FILE_TRAIN,\n",
    "    FILE_VALIDATION,\n",
    "    FILE_TEST,\n",
    "    PERCENT_TRAIN,\n",
    "    PERCENT_VALIDATION,\n",
    "    PERCENT_TEST,\n",
    "):\n",
    "    data = [l for l in open(FILE_DATA, \"r\")]\n",
    "    train_file = open(FILE_TRAIN, \"w\")\n",
    "    valid_file = open(FILE_VALIDATION, \"w\")\n",
    "    tests_file = open(FILE_TEST, \"w\")\n",
    "\n",
    "    num_of_data = len(data)\n",
    "    num_train = int((PERCENT_TRAIN / 100.0) * num_of_data)\n",
    "    num_valid = int((PERCENT_VALIDATION / 100.0) * num_of_data)\n",
    "    num_tests = int((PERCENT_TEST / 100.0) * num_of_data)\n",
    "\n",
    "    data_fractions = [num_train, num_valid, num_tests]\n",
    "    split_data = [[], [], []]\n",
    "\n",
    "    rand_data_ind = 0\n",
    "\n",
    "    for split_ind, fraction in enumerate(data_fractions):\n",
    "        for i in range(fraction):\n",
    "            rand_data_ind = random.randint(0, len(data) - 1)\n",
    "            split_data[split_ind].append(data[rand_data_ind])\n",
    "            data.pop(rand_data_ind)\n",
    "\n",
    "    for l in split_data[0]:\n",
    "        train_file.write(l)\n",
    "\n",
    "    for l in split_data[1]:\n",
    "        valid_file.write(l)\n",
    "\n",
    "    for l in split_data[2]:\n",
    "        tests_file.write(l)\n",
    "\n",
    "    train_file.close()\n",
    "    valid_file.close()\n",
    "    tests_file.close()\n",
    "\n",
    "\n",
    "def write_to_s3(fobj, bucket, key):\n",
    "    return (\n",
    "        boto3.Session(region_name=region)\n",
    "        .resource(\"s3\")\n",
    "        .Bucket(bucket)\n",
    "        .Object(key)\n",
    "        .upload_fileobj(fobj)\n",
    "    )\n",
    "\n",
    "\n",
    "def upload_to_s3(bucket, channel, filename):\n",
    "    fobj = open(filename, \"rb\")\n",
    "    key = f\"{prefix}/{channel}\"\n",
    "    url = f\"s3://{bucket}/{key}/{filename}\"\n",
    "    print(f\"Writing to {url}\")\n",
    "    write_to_s3(fobj, bucket, key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0907a577-ba9c-425a-92c2-07a0de75b0ef",
   "metadata": {},
   "source": [
    "## Data ingestion\n",
    "\n",
    "Next, we read the dataset from the existing repository into memory, for preprocessing prior to training. This processing could be done in situ by Amazon Athena, Apache Spark in Amazon EMR, Amazon Redshift, etc., assuming the dataset is present in the appropriate location. Then, the next step would be to transfer the data to S3 for use in training. For small datasets, such as this one, reading into memory isn’t onerous, though it would be for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3628a458-49bb-4738-a3f2-f68e16623ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = \"sagemaker/DEMO-xgboost-abalone-default\"\n",
    "# Load the dataset\n",
    "FILE_DATA = \"abalone\"\n",
    "s3.download_file(\n",
    "    \"sagemaker-sample-files\", f\"datasets/tabular/uci_abalone/abalone.libsvm\", FILE_DATA\n",
    ")\n",
    "\n",
    "# split the downloaded data into train/test/validation files\n",
    "FILE_TRAIN = \"abalone.train\"\n",
    "FILE_VALIDATION = \"abalone.validation\"\n",
    "FILE_TEST = \"abalone.test\"\n",
    "PERCENT_TRAIN = 70\n",
    "PERCENT_VALIDATION = 15\n",
    "PERCENT_TEST = 15\n",
    "data_split(\n",
    "    FILE_DATA,\n",
    "    FILE_TRAIN,\n",
    "    FILE_VALIDATION,\n",
    "    FILE_TEST,\n",
    "    PERCENT_TRAIN,\n",
    "    PERCENT_VALIDATION,\n",
    "    PERCENT_TEST,\n",
    ")\n",
    "\n",
    "# upload the files to the S3 bucket\n",
    "upload_to_s3(bucket, \"train\", FILE_TRAIN)\n",
    "upload_to_s3(bucket, \"validation\", FILE_VALIDATION)\n",
    "upload_to_s3(bucket, \"test\", FILE_TEST)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
