{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrate Jobs to Train and Evaluate Models \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Notebook Description\n",
    "\n",
    "**Dataset Reference:** https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression.html\n",
    "\n",
    "**Type of problem:** Linear Regression\n",
    "\n",
    "**Type of solution:** XGBoost using SageMaker Training Job\n",
    "\n",
    "\n",
    "Source of this notebook: https://docs.aws.amazon.com/sagemaker/latest/dg/define-pipeline.html\n",
    "\n",
    "This notebook has been adapted to run outside of SageMaker Pipeline\n",
    "\n",
    "\n",
    "**Stack:**\n",
    "- pandas, numpy \n",
    "- SageMaker Training Job\n",
    "- Studio's prebuilt image DataScience 3.0 (conda) and XGBoost Stack\n",
    "\n",
    "**Steps:**\n",
    "- download data\n",
    "- do some data preparation\n",
    "- split the datasets and upload the datasets to S3\n",
    "- configure and run the training job\n",
    "- check the model evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset you use is the [UCI Machine Learning Abalone Dataset](https://archive.ics.uci.edu/ml/datasets/abalone) [1].  The aim for this task is to determine the age of an abalone snail from its physical measurements. At the core, this is a regression problem.\n",
    "\n",
    "The dataset contains several features: length (the longest shell measurement), diameter (the diameter perpendicular to length), height (the height with meat in the shell), whole_weight (the weight of whole abalone), shucked_weight (the weight of meat), viscera_weight (the gut weight after bleeding), shell_weight (the weight after being dried), sex ('M', 'F', 'I' where 'I' is Infant), and rings (integer).\n",
    "\n",
    "The number of rings turns out to be a good approximation for age (age is rings + 1.5). However, to obtain this number requires cutting the shell through the cone, staining the section, and counting the number of rings through a microscope, which is a time-consuming task. However, the other physical measurements are easier to determine. You use the dataset to build a predictive model of the variable rings through these other physical measurements.\n",
    "\n",
    "Before you upload the data to an S3 bucket, install the SageMaker Python SDK and gather some constants you can use later in this notebook.\n",
    "\n",
    "> [1] Dua, D. and Graff, C. (2019). [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml). Irvine, CA: University of California, School of Information and Computer Science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker>=2.121.0\n",
      "  Using cached sagemaker-2.126.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.121.0) (1.0.1)\n",
      "Collecting boto3<2.0,>=1.26.28\n",
      "  Using cached boto3-1.26.41-py3-none-any.whl (132 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.121.0) (1.23.5)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.121.0) (4.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.121.0) (21.3)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.121.0) (0.1.5)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.121.0) (1.4.4)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.121.0) (3.20.3)\n",
      "Requirement already satisfied: attrs<23,>=20.3.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.121.0) (21.4.0)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.121.0) (0.2.0)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.121.0) (0.7.5)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.121.0) (0.3.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.26.28->sagemaker>=2.121.0) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.26.28->sagemaker>=2.121.0) (0.10.0)\n",
      "Collecting botocore<1.30.0,>=1.29.41\n",
      "  Using cached botocore-1.29.41-py3-none-any.whl (10.3 MB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker>=2.121.0) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->sagemaker>=2.121.0) (3.0.9)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker>=2.121.0) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker>=2.121.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker>=2.121.0) (2022.1)\n",
      "Requirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker>=2.121.0) (0.3.6)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker>=2.121.0) (0.70.14)\n",
      "Requirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker>=2.121.0) (0.3.2)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker>=2.121.0) (1.7.6.6)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.10/site-packages (from schema->sagemaker>=2.121.0) (21.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.41->boto3<2.0,>=1.26.28->sagemaker>=2.121.0) (1.26.13)\n",
      "Installing collected packages: botocore, boto3, sagemaker\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.29.24\n",
      "    Uninstalling botocore-1.29.24:\n",
      "      Successfully uninstalled botocore-1.29.24\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.26.24\n",
      "    Uninstalling boto3-1.26.24:\n",
      "      Successfully uninstalled boto3-1.26.24\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.120.0\n",
      "    Uninstalling sagemaker-2.120.0:\n",
      "      Successfully uninstalled sagemaker-2.120.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.27.24 requires botocore==1.29.24, but you have botocore 1.29.41 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed boto3-1.26.41 botocore-1.29.41 sagemaker-2.126.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install \"sagemaker>=2.121.0\"  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "#pipeline_session = PipelineSession()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "#model_package_group_name = f\"AbaloneModelPackageGroupName\"\n",
    "model_package_group_name = f\"LabBench-Abalone-Jobs-GroupName\"\n",
    "#ipeline_name = f\"AbalonePipeline\"\n",
    "pipeline_name = f\"LabBench-Abalone-Jobs\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "import time\n",
    "\n",
    "run_id = f\"{strftime('%y%m%d%H%M', gmtime())}\"\n",
    "\n",
    "stage_prefix = \"L\"\n",
    "project_prefix = \"abalone\"\n",
    "variant_prefix = \"xgbjob\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job_prefix_short = f\"{variant_prefix}/{run_id}\"\n",
    "job_prefix_long = f\"{stage_prefix}/{project_prefix}/{job_prefix_short}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job_prefix_short='xgbjob/2301031407'\n",
      "job_prefix_long='L/abalone/xgbjob/2301031407'\n"
     ]
    }
   ],
   "source": [
    "print(f\"{job_prefix_short=}\")\n",
    "print(f\"{job_prefix_long=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "base_folder = os.path.join(\"./generated\", job_prefix_short)\n",
    "\n",
    "base_uri = f\"s3://{default_bucket}/{job_prefix_long}\"\n",
    "base_uri_for_jobs = f\"s3://{default_bucket}/{stage_prefix}-jobs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_folder='./generated/xgbjob/2301031407'\n",
      "base_uri='s3://sagemaker-eu-west-1-102959664345/L/abalone/xgbjob/2301031407'\n",
      "base_uri_for_jobs='s3://sagemaker-eu-west-1-102959664345/L-jobs'\n"
     ]
    }
   ],
   "source": [
    "print(f\"{base_folder=}\")\n",
    "print(f\"{base_uri=}\")\n",
    "print(f\"{base_uri_for_jobs=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, upload the data into the default bucket. You can select our own data set for the `input_data_uri` as is appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_path='./generated/xgbjob/2301031407/data/raw/abalone-dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# tmp directory\n",
    "data_folder = os.path.join(base_folder, \"data\")\n",
    "\n",
    "raw_data_folder = os.path.join(data_folder, \"raw\")\n",
    "os.makedirs(raw_data_folder, exist_ok=True)\n",
    "\n",
    "local_path = os.path.join(raw_data_folder, \"abalone-dataset.csv\")\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "s3.Bucket(f\"sagemaker-sample-files\").download_file(\n",
    "    \"datasets/tabular/uci_abalone/abalone.csv\", local_path\n",
    ")\n",
    "\n",
    "print(f\"{local_path=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data_uri='s3://sagemaker-eu-west-1-102959664345/L/abalone/xgbjob/2301031407/abalone-dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "input_data_uri = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=local_path,\n",
    "    desired_s3_uri=base_uri,\n",
    ")\n",
    "print(f\"{input_data_uri=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters \n",
    "\n",
    "The parameters defined in this workflow include:\n",
    "\n",
    "* `processing_instance_count` - The instance count of the processing job.\n",
    "* `instance_type` - The `ml.*` instance type of the training job.\n",
    "* `input_data` - The S3 bucket URI location of the input data.\n",
    "* `batch_data` - The S3 bucket URI location of the batch data.\n",
    "* `mse_threshold` - The Mean Squared Error (MSE) threshold used to verify the accuracy of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "processing_instance_count = 1\n",
    "#instance_type = \"ml.m5.xlarge\"\n",
    "#instance_type = \"ml.m5.2xlarge\"\n",
    "instance_type = \"ml.t3.medium\"\n",
    "input_data = input_data_uri\n",
    "mse_threshold = 6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_job_name = f\"{stage_prefix}-{project_prefix}-{variant_prefix}\"\n",
    "# a long time stamp will be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_job_name='L-abalone-xgbjob'\n"
     ]
    }
   ],
   "source": [
    "print(f\"{base_job_name=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Processing Step for Feature Engineering\n",
    "\n",
    "First, develop a preprocessing script that is specified in the Processing step.\n",
    "\n",
    "This notebook cell writes a file `preprocessing_abalone.py`, which contains the preprocessing script. You can update the script, and rerun this cell to overwrite. The preprocessing script uses `scikit-learn` to do the following:\n",
    "\n",
    "* Fill in missing sex category data and encode it so that it is suitable for training.\n",
    "* Scale and normalize all numerical fields, aside from sex and rings numerical data.\n",
    "* Split the data into training, validation, and test datasets.\n",
    "\n",
    "The Processing step executes the script on the input data. The Training step uses the preprocessed training features and labels to train a model. The Evaluation step uses the trained model and preprocessed test features and labels to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# tmp directory\n",
    "code_folder=\"generated/code\"\n",
    "os.makedirs(code_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting generated/code/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile generated/code/preprocessing.py\n",
    "import argparse\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "# Since we get a headerless CSV file, we specify the column names here.\n",
    "feature_columns_names = [\n",
    "    \"sex\",\n",
    "    \"length\",\n",
    "    \"diameter\",\n",
    "    \"height\",\n",
    "    \"whole_weight\",\n",
    "    \"shucked_weight\",\n",
    "    \"viscera_weight\",\n",
    "    \"shell_weight\",\n",
    "]\n",
    "label_column = \"rings\"\n",
    "\n",
    "feature_columns_dtype = {\n",
    "    \"sex\": str,\n",
    "    \"length\": np.float64,\n",
    "    \"diameter\": np.float64,\n",
    "    \"height\": np.float64,\n",
    "    \"whole_weight\": np.float64,\n",
    "    \"shucked_weight\": np.float64,\n",
    "    \"viscera_weight\": np.float64,\n",
    "    \"shell_weight\": np.float64,\n",
    "}\n",
    "label_column_dtype = {\"rings\": np.float64}\n",
    "\n",
    "\n",
    "def merge_two_dicts(x, y):\n",
    "    z = x.copy()\n",
    "    z.update(y)\n",
    "    return z\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = \"/opt/ml/processing\"\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        f\"{base_dir}/input/abalone-dataset.csv\",\n",
    "        header=None,\n",
    "        names=feature_columns_names + [label_column],\n",
    "        dtype=merge_two_dicts(feature_columns_dtype, label_column_dtype),\n",
    "    )\n",
    "    numeric_features = list(feature_columns_names)\n",
    "    numeric_features.remove(\"sex\")\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    "    )\n",
    "\n",
    "    categorical_features = [\"sex\"]\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"cat\", categorical_transformer, categorical_features),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    y = df.pop(\"rings\")\n",
    "    X_pre = preprocess.fit_transform(df)\n",
    "    y_pre = y.to_numpy().reshape(len(y), 1)\n",
    "\n",
    "    X = np.concatenate((y_pre, X_pre), axis=1)\n",
    "\n",
    "    np.random.shuffle(X)\n",
    "    train, validation, test = np.split(X, [int(0.7 * len(X)), int(0.85 * len(X))])\n",
    "\n",
    "    pd.DataFrame(train).to_csv(f\"{base_dir}/train/train.csv\", header=False, index=False)\n",
    "    pd.DataFrame(validation).to_csv(\n",
    "        f\"{base_dir}/validation/validation.csv\", header=False, index=False\n",
    "    )\n",
    "    pd.DataFrame(test).to_csv(f\"{base_dir}/test/test.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create an instance of a `SKLearnProcessor` processor and use that in our `ProcessingStep`.\n",
    "\n",
    "You also specify the `framework_version` to use throughout this notebook.\n",
    "\n",
    "Use sagemaker_session in order to run this immediately.\n",
    "\n",
    "API Reference \n",
    "- SK Framework https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/sagemaker.sklearn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "\n",
    "framework_version = \"0.23-1\"\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    instance_type=instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=f\"{base_job_name}-preprocess\",\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we take the output of the processor's `run` method. \n",
    "Note the `\"train_data\"` and `\"test_data\"` named channels specified in the output configuration for the processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name L-abalone-xgbjob-preprocess-2023-01-03-14-08-05-087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  L-abalone-xgbjob-preprocess-2023-01-03-14-08-05-087\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-eu-west-1-102959664345/L/abalone/xgbjob/2301031407/abalone-dataset.csv', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-eu-west-1-102959664345/L-abalone-xgbjob-preprocess-2023-01-03-14-08-05-087/input/code/preprocessing.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'train', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-eu-west-1-102959664345/L-abalone-xgbjob-preprocess-2023-01-03-14-08-05-087/output/train', 'LocalPath': '/opt/ml/processing/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'validation', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-eu-west-1-102959664345/L-abalone-xgbjob-preprocess-2023-01-03-14-08-05-087/output/validation', 'LocalPath': '/opt/ml/processing/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'test', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-eu-west-1-102959664345/L-abalone-xgbjob-preprocess-2023-01-03-14-08-05-087/output/test', 'LocalPath': '/opt/ml/processing/test', 'S3UploadMode': 'EndOfJob'}}]\n",
      "........................................................................\n",
      "..CPU times: user 1.26 s, sys: 64.3 ms, total: 1.33 s\n",
      "Wall time: 12min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "preprocessor_job = sklearn_processor.run(\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    code=\"generated/code/preprocessing.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pprint(sklearn_processor.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pprint(sklearn_processor.latest_job.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pprint(sklearn_processor.latest_job.outputs[0].__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset_uri = sklearn_processor.latest_job.outputs[0].destination\n",
    "validation_dataset_uri = sklearn_processor.latest_job.outputs[1].destination\n",
    "test_dataset_uri = sklearn_processor.latest_job.outputs[2].destination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Training Step to Train a Model\n",
    "\n",
    "In this section, use Amazon SageMaker's [XGBoost Algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) to train on this dataset. Configure an Estimator for the XGBoost algorithm and the input dataset. A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to `model_dir` so that it can be hosted later.\n",
    "\n",
    "The model path where the models from training are saved is also specified.\n",
    "\n",
    "Use sagemaker_session in order to run this immediately.\n",
    "\n",
    "Note the `instance_type` parameter may be used in multiple places in the pipeline. In this case, the `instance_type` is passed into the estimator.\n",
    "\n",
    "API Reference\n",
    "- XGBoost Framework https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: L-abalone-xgbjob-train-2023-01-03-15-13-51-089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-03 15:13:51 Starting - Starting the training job...\n",
      "2023-01-03 15:14:05 Starting - Preparing the instances for training......\n",
      "2023-01-03 15:15:13 Downloading - Downloading input data...\n",
      "2023-01-03 15:15:38 Training - Downloading the training image...\n",
      "2023-01-03 15:16:13 Training - Training image download completed. Training in progress..\u001b[34m[2023-01-03 15:16:18.803 ip-10-0-184-13.eu-west-1.compute.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value reg:linear to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34m[15:16:18] 2923x10 matrix with 29230 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[15:16:18] 627x10 matrix with 6270 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2023-01-03 15:16:18.907 ip-10-0-184-13.eu-west-1.compute.internal:7 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-01-03 15:16:18.907 ip-10-0-184-13.eu-west-1.compute.internal:7 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-01-03 15:16:18.908 ip-10-0-184-13.eu-west-1.compute.internal:7 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-01-03 15:16:18.908 ip-10-0-184-13.eu-west-1.compute.internal:7 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-01-03 15:16:18.909 ip-10-0-184-13.eu-west-1.compute.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mINFO:root:Debug hook created from config\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 2923 rows\u001b[0m\n",
      "\u001b[34mINFO:root:Validation matrix has 627 rows\u001b[0m\n",
      "\u001b[34m[15:16:18] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\u001b[0m\n",
      "\u001b[34m[15:16:18] WARNING: /workspace/src/learner.cc:328: \u001b[0m\n",
      "\u001b[34mParameters: { num_round } might not be used.\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\u001b[0m\n",
      "\u001b[34m[0]#011train-rmse:8.12400#011validation-rmse:7.98466\u001b[0m\n",
      "\u001b[34m[2023-01-03 15:16:18.916 ip-10-0-184-13.eu-west-1.compute.internal:7 INFO hook.py:423] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2023-01-03 15:16:18.919 ip-10-0-184-13.eu-west-1.compute.internal:7 INFO hook.py:486] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[1]#011train-rmse:6.65382#011validation-rmse:6.55328\u001b[0m\n",
      "\u001b[34m[2]#011train-rmse:5.49203#011validation-rmse:5.43467\u001b[0m\n",
      "\u001b[34m[3]#011train-rmse:4.59163#011validation-rmse:4.57557\u001b[0m\n",
      "\u001b[34m[4]#011train-rmse:3.89299#011validation-rmse:3.91686\u001b[0m\n",
      "\u001b[34m[5]#011train-rmse:3.35805#011validation-rmse:3.42313\u001b[0m\n",
      "\u001b[34m[6]#011train-rmse:2.95600#011validation-rmse:3.05879\u001b[0m\n",
      "\u001b[34m[7]#011train-rmse:2.66258#011validation-rmse:2.79045\u001b[0m\n",
      "\u001b[34m[8]#011train-rmse:2.44104#011validation-rmse:2.58370\u001b[0m\n",
      "\u001b[34m[9]#011train-rmse:2.28285#011validation-rmse:2.45453\u001b[0m\n",
      "\u001b[34m[10]#011train-rmse:2.16549#011validation-rmse:2.37073\u001b[0m\n",
      "\u001b[34m[11]#011train-rmse:2.09338#011validation-rmse:2.32743\u001b[0m\n",
      "\u001b[34m[12]#011train-rmse:2.03998#011validation-rmse:2.29102\u001b[0m\n",
      "\u001b[34m[13]#011train-rmse:2.00172#011validation-rmse:2.26086\u001b[0m\n",
      "\u001b[34m[14]#011train-rmse:1.96017#011validation-rmse:2.23529\u001b[0m\n",
      "\u001b[34m[15]#011train-rmse:1.93817#011validation-rmse:2.23259\u001b[0m\n",
      "\u001b[34m[16]#011train-rmse:1.91351#011validation-rmse:2.22386\u001b[0m\n",
      "\u001b[34m[17]#011train-rmse:1.89159#011validation-rmse:2.21479\u001b[0m\n",
      "\u001b[34m[18]#011train-rmse:1.87722#011validation-rmse:2.21801\u001b[0m\n",
      "\u001b[34m[19]#011train-rmse:1.86896#011validation-rmse:2.21799\u001b[0m\n",
      "\u001b[34m[20]#011train-rmse:1.86166#011validation-rmse:2.22306\u001b[0m\n",
      "\u001b[34m[21]#011train-rmse:1.85297#011validation-rmse:2.22065\u001b[0m\n",
      "\u001b[34m[22]#011train-rmse:1.84742#011validation-rmse:2.21972\u001b[0m\n",
      "\u001b[34m[23]#011train-rmse:1.83479#011validation-rmse:2.22487\u001b[0m\n",
      "\u001b[34m[24]#011train-rmse:1.82711#011validation-rmse:2.22783\u001b[0m\n",
      "\u001b[34m[25]#011train-rmse:1.81727#011validation-rmse:2.22992\u001b[0m\n",
      "\u001b[34m[26]#011train-rmse:1.80597#011validation-rmse:2.23126\u001b[0m\n",
      "\u001b[34m[27]#011train-rmse:1.78911#011validation-rmse:2.23953\u001b[0m\n",
      "\u001b[34m[28]#011train-rmse:1.78092#011validation-rmse:2.24919\u001b[0m\n",
      "\u001b[34m[29]#011train-rmse:1.76610#011validation-rmse:2.24354\u001b[0m\n",
      "\u001b[34m[30]#011train-rmse:1.75842#011validation-rmse:2.23874\u001b[0m\n",
      "\u001b[34m[31]#011train-rmse:1.75303#011validation-rmse:2.24194\u001b[0m\n",
      "\u001b[34m[32]#011train-rmse:1.75106#011validation-rmse:2.24048\u001b[0m\n",
      "\u001b[34m[33]#011train-rmse:1.74179#011validation-rmse:2.23899\u001b[0m\n",
      "\u001b[34m[34]#011train-rmse:1.73658#011validation-rmse:2.23826\u001b[0m\n",
      "\u001b[34m[35]#011train-rmse:1.72789#011validation-rmse:2.23380\u001b[0m\n",
      "\u001b[34m[36]#011train-rmse:1.72197#011validation-rmse:2.23280\u001b[0m\n",
      "\u001b[34m[37]#011train-rmse:1.71383#011validation-rmse:2.23360\u001b[0m\n",
      "\u001b[34m[38]#011train-rmse:1.70481#011validation-rmse:2.23026\u001b[0m\n",
      "\u001b[34m[39]#011train-rmse:1.70015#011validation-rmse:2.23395\u001b[0m\n",
      "\u001b[34m[40]#011train-rmse:1.69462#011validation-rmse:2.23351\u001b[0m\n",
      "\u001b[34m[41]#011train-rmse:1.68929#011validation-rmse:2.23048\u001b[0m\n",
      "\u001b[34m[42]#011train-rmse:1.68386#011validation-rmse:2.23703\u001b[0m\n",
      "\u001b[34m[43]#011train-rmse:1.67047#011validation-rmse:2.22988\u001b[0m\n",
      "\u001b[34m[44]#011train-rmse:1.66156#011validation-rmse:2.22445\u001b[0m\n",
      "\u001b[34m[45]#011train-rmse:1.65539#011validation-rmse:2.22297\u001b[0m\n",
      "\u001b[34m[46]#011train-rmse:1.64593#011validation-rmse:2.22255\u001b[0m\n",
      "\u001b[34m[47]#011train-rmse:1.64206#011validation-rmse:2.22535\u001b[0m\n",
      "\u001b[34m[48]#011train-rmse:1.63700#011validation-rmse:2.22768\u001b[0m\n",
      "\u001b[34m[49]#011train-rmse:1.63209#011validation-rmse:2.23192\u001b[0m\n",
      "\n",
      "2023-01-03 15:16:40 Uploading - Uploading generated training model\n",
      "2023-01-03 15:16:40 Completed - Training job completed\n",
      "Training seconds: 88\n",
      "Billable seconds: 88\n",
      "CPU times: user 351 ms, sys: 31 ms, total: 382 ms\n",
      "Wall time: 3min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "model_path = f\"{base_uri}/model\"\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.0-1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=instance_type,  \n",
    ")\n",
    "xgb_train = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    instance_type=\"ml.m5.large\",  #instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=f\"{base_job_name}-train\",\n",
    "    output_path=model_path,\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "xgb_train.set_hyperparameters(\n",
    "    objective=\"reg:linear\",\n",
    "    num_round=50,\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.7,\n",
    ")\n",
    "\n",
    "train_job = xgb_train.fit(\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=train_dataset_uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=validation_dataset_uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pprint(xgb_train.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pprint(xgb_train.latest_training_job.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pprint(xgb_train.output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job_name = xgb_train.latest_training_job.job_name\n",
    "model_uri = f\"{xgb_train.output_path}/{job_name}/output/model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Trained Model\n",
    "\n",
    "First, develop an evaluation script that is specified in a Processing step that performs the model evaluation.\n",
    "\n",
    "After execution, you can examine the resulting `evaluation.json` for analysis.\n",
    "\n",
    "The evaluation script uses `xgboost` to do the following:\n",
    "\n",
    "* Load the model.\n",
    "* Read the test data.\n",
    "* Issue predictions against the test data.\n",
    "* Build a classification report, including accuracy and ROC curve.\n",
    "* Save the evaluation report to the evaluation directory.\n",
    "\n",
    "Need a script processor to run the model while it is not deployed on an endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting generated/code/evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile generated/code/evaluation.py\n",
    "import json\n",
    "import pathlib\n",
    "import pickle\n",
    "import tarfile\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = f\"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "\n",
    "    model = pickle.load(open(\"xgboost-model\", \"rb\"))\n",
    "\n",
    "    test_path = \"/opt/ml/processing/test/test.csv\"\n",
    "    df = pd.read_csv(test_path, header=None)\n",
    "\n",
    "    y_test = df.iloc[:, 0].to_numpy()\n",
    "    df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "    X_test = xgboost.DMatrix(df.values)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    std = np.std(y_test - predictions)\n",
    "    report_dict = {\n",
    "        \"regression_metrics\": {\n",
    "            \"mse\": {\"value\": mse, \"standard_deviation\": std},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    evaluation_path = f\"{output_dir}/evaluation.json\"\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        f.write(json.dumps(report_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create an instance of a `ScriptProcessor` processor.\n",
    "\n",
    "API Reference\n",
    "- https://sagemaker.readthedocs.io/en/stable/api/training/processing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name L-abalone-xgbjob-eval-2023-01-03-15-45-24-065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  L-abalone-xgbjob-eval-2023-01-03-15-45-24-065\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-eu-west-1-102959664345/L/abalone/xgbjob/2301031407/model/L-abalone-xgbjob-train-2023-01-03-15-13-51-089/output/model.tar.gz', 'LocalPath': '/opt/ml/processing/model', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'input-2', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-eu-west-1-102959664345/L-abalone-xgbjob-preprocess-2023-01-03-14-08-05-087/output/test', 'LocalPath': '/opt/ml/processing/test', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-eu-west-1-102959664345/L-abalone-xgbjob-eval-2023-01-03-15-45-24-065/input/code/evaluation.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'evaluation', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-eu-west-1-102959664345/L-abalone-xgbjob-eval-2023-01-03-15-45-24-065/output/evaluation', 'LocalPath': '/opt/ml/processing/evaluation', 'S3UploadMode': 'EndOfJob'}}]\n",
      "...........................................................................\n",
      "\u001b[34m[15:57:43] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\u001b[0m\n",
      "CPU times: user 1.4 s, sys: 79.4 ms, total: 1.48 s\n",
      "Wall time: 12min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "script_eval = ScriptProcessor(\n",
    "    image_uri=image_uri,\n",
    "    command=[\"python3\"],\n",
    "    instance_type=instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=f\"{base_job_name}-eval\",\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "eval_job = script_eval.run(\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=model_uri,\n",
    "            destination=\"/opt/ml/processing/model\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=test_dataset_uri,\n",
    "            destination=\"/opt/ml/processing/test\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "    ],\n",
    "    code=\"generated/code/evaluation.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pprint(script_eval.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_result_uri = script_eval.latest_job.outputs[0].destination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining the Evaluation\n",
    "\n",
    "Examine the resulting model evaluation after the pipeline completes. Download the resulting evaluation.json file from S3 and print the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "evaluation_json = sagemaker.s3.S3Downloader.read_file(\n",
    "    \"{}/evaluation.json\".format(test_result_uri)\n",
    ")\n",
    "pprint(json.loads(evaluation_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
