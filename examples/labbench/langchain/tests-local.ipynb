{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b35fe35-e78b-44d5-9cb6-3faf6013f987",
   "metadata": {},
   "source": [
    "# Local models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c8bfc1-c7d7-4015-92cf-b824420b4ab4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6dfe9f45-e1ed-4260-89db-950276000a8c",
   "metadata": {},
   "source": [
    "# Fake model\n",
    "\n",
    "this model may be use to run tests before swithching to the real model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428aa644-d515-4e36-8eb9-7876ee59701f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eefbc3f0-e174-4413-b744-185c0fac8d89",
   "metadata": {},
   "source": [
    "# Local model through Hugging Face local pipelines\n",
    "\n",
    "Hugging Face models can be run locally through the HuggingFacePipeline class.\n",
    "\n",
    "The Hugging Face Model Hub hosts over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together.\n",
    "\n",
    "\n",
    "\n",
    "##Resources**\n",
    "> - https://python.langchain.com/docs/modules/model_io/models/llms/integrations/huggingface_pipelines\n",
    "> - Hugging Face hub models  https://huggingface.co/models\n",
    "> - Hugging Face Hub documentation https://python.langchain.com/docs/modules/model_io/models/llms/integrations/huggingface_hub.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d4c6d6-7320-4a4e-9d28-bdf491bcc2e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1abbfe53-3c80-4cb5-aa6c-c1c4cf53678a",
   "metadata": {},
   "source": [
    "## prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c47eb9d2-9119-4ac1-b882-37b2040a79bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating pip definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0064987d-609f-4f31-a64a-ff8934121c08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip  1>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44189243-4738-496e-85a5-614a4bc51fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instakling C++ compiler for deep learning frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bb9aa97-c54a-4aab-a00d-73d9ca51a84f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://security.debian.org/debian-security bullseye-security InRelease\n",
      "Hit:2 http://deb.debian.org/debian bullseye InRelease\n",
      "Hit:3 http://deb.debian.org/debian bullseye-updates InRelease\n",
      "Reading package lists... Done\n"
     ]
    }
   ],
   "source": [
    "!apt-get update && apt-get install -y build-essential 1>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84450466-ad81-4b36-b1f8-0fcb98613454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a21bc0a-7d2e-4b05-b753-cf3d009a8fc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install langchain==0.0.230 1>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e22f0c3-ff44-4f21-be87-697e20d962fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use case requests either oytorck, tensorflow or flax\n",
    "# installing pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484673de-2d75-4a3d-a86c-687ed047daaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0d76012-6e42-49eb-919a-5983fd5e3262",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision 1>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b2eef21-1f18-4a8f-82cc-a13b8b716b51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0930, 0.6304, 0.1576],\n",
      "        [0.6497, 0.5905, 0.1008],\n",
      "        [0.5401, 0.9838, 0.1094],\n",
      "        [0.0197, 0.3558, 0.7318],\n",
      "        [0.5117, 0.9673, 0.6725]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "872b9e37-181a-4d77-a9eb-625707610973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing tranformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "563bf531-9884-4c7c-8c35-f8476124979c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers 1>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac18b617-e3c1-4445-a9eb-81a18e5ce160",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# installing an accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82ca3e94-f0ea-4175-91ec-5be7d334749a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install xformers 1>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1b9c6c-8f47-4f51-996f-9d9b75aafbd2",
   "metadata": {},
   "source": [
    "## loading the model\n",
    "\n",
    "Loads and build the model locally leveraging a Pytorch or Tensoflow or Flax pipeline\n",
    "\n",
    "bloom 560m\n",
    "Used a ml.m5.large  2vCPU 8GB. Otherwise the Kernel crashes.\n",
    "bloom 1b\n",
    "ml.g4dn.xlarge 4CPU + 1GPU 16GB\n",
    "\n",
    "**Resources**\n",
    "> - https://huggingface.co/docs/transformers/main/main_classes/pipelines\n",
    "> - https://pytorch.org/get-started/locally/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "386a5167-815b-407b-a123-f0bbaee0a65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may have to restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cbd2a2a-2479-43bd-8462-b35d7326d7e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Python VERSION: 3.10.6 (main, Oct  7 2022, 20:19:58) [GCC 11.2.0]\n",
      "__pyTorch VERSION: 2.0.1+cu117\n",
      "__CUDA VERSION\n",
      "/bin/bash: line 1: nvcc: command not found\n",
      "__CUDNN VERSION: 8500\n",
      "__Number CUDA Devices: 1\n",
      "__Devices\n",
      "index, name, driver_version, memory.total [MiB], memory.used [MiB], memory.free [MiB]\n",
      "0, Tesla T4, 470.57.02, 15109 MiB, 0 MiB, 15109 MiB\n",
      "Active CUDA Device: GPU 0\n",
      "Available devices  1\n",
      "Current cuda device  0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA VERSION')\n",
    "from subprocess import call\n",
    "# call([\"nvcc\", \"--version\"]) does not work\n",
    "! nvcc --version\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Devices')\n",
    "call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n",
    "print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
    "print ('Available devices ', torch.cuda.device_count())\n",
    "print ('Current cuda device ', torch.cuda.current_device())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6254e93-13d2-4080-9f48-9806598d39a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cuda')\n"
     ]
    }
   ],
   "source": [
    "# Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, ort, xla, lazy, vulkan, mps, meta, hpu, mtia, privateuseone device type at start of device string\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"{device=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c2aae7e-cff7-43b8-a45f-4cc96b592580",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cuda')\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"{device=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e8e7654-5427-45ac-8ba7-cee512620925",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device has 1 GPUs available. Provide device={deviceId} to `from_model_id` to use availableGPUs for execution. deviceId is -1 (default) for CPU and can be a positive integer associated with CUDA device id.\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "model_id = \"bigscience/bloom-1b7\"\n",
    "mini_model_id = \"bigscience/bloom-560m\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_id,\n",
    "    task=\"text-generation\",\n",
    "    #device=device,\n",
    "    model_kwargs={\"temperature\": 0, \"max_length\": 256}, # was 64\n",
    "    #model_kwargs={\"temperature\": 0, \"max_new_tokens\":1000},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9278bc93-28fe-4465-b3f8-56493faf1329",
   "metadata": {},
   "source": [
    "tried to fix the warning - but the model fail to be built after the edition\n",
    "\n",
    "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (64) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de776b3-23d9-48ca-9bec-98f156d6b934",
   "metadata": {},
   "source": [
    "setting the device caused the kernel to crash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0857cd24-b293-4b2b-892b-664061c66adf",
   "metadata": {},
   "source": [
    "## integrate in a LLM Chian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ef4986a-34ed-48ea-83b3-0902bb1beb32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (256) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " First, we need to understand what is an electroencephalogram. An electroencephalogram is a recording of brain activity. It is a recording of brain activity that is made by placing electrodes on the scalp. The electrodes are placed in a way that they are connected to the brain. The electrodes are connected to the brain by wires. The wires are connected to the electrodes by a cable. The cable is connected to a computer. The computer is connected to a monitor. The monitor is connected to a television. The television is connected to a radio. The radio is connected to a speaker. The speaker is connected to a microphone. The microphone is connected to a receiver. The receiver is connected to a computer. The computer is connected to a monitor. The monitor is connected to a television. The television is connected to a radio. The radio is connected to a speaker. The speaker is connected to a microphone. The microphone is connected to a receiver. The receiver is connected to a computer. The computer is connected to a monitor. The monitor is connected to a television. The television is connected to a radio. The radio is connected to a speaker. The speaker is connected to a microphone.\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"What is electroencephalography?\"\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b04eccd-a8f7-4ece-a7f3-449fb7cce461",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " First, we need to know what a pen is. A pen is a tool that is used to write on paper. A pen is a tool that is used to write on paper. A pen is a tool that is used to write on paper. A pen is a tool that is used to write on paper. A pen is a tool that is used to write on paper. A pen is a tool that is used to write on paper. A pen is a tool that is used to write on paper. A pen is a tool that is used to write on paper. A pen is a tool that is used to write on paper. A pen is a tool that is used to write on paper. A pen is a tool that is used to write on paper. A pen is a tool that is used to write on paper. A pen is a tool that is used to write on paper. A pen is a tool that is used to write on paper. A pen is a tool that is used to write on paper. A pen is a tool that is used to write on paper. A pen is a tool that is used to write on paper. A pen is a tool that is used to write\n"
     ]
    }
   ],
   "source": [
    "question = \"What is a pen?\"\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9bde8b7-a51e-47ae-b39f-530ed5f6b7ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (256) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It is a technique used to measure brain activity. It is a non-invasive technique that measures brain activity by measuring electrical activity in the brain. It is a technique that is used to measure brain activity. It is a technique that is used to measure brain activity. It is a technique that is used to measure brain activity. It is a technique that is used to measure brain activity. It is a technique that is used to measure brain activity. It is a technique that is used to measure brain activity. It is a technique that is used to measure brain activity. It is a technique that is used to measure brain activity. It is a technique that is used to measure brain activity. It is a technique that is used to measure brain activity. It is a technique that is used to measure brain activity. It is a technique that is used to measure brain activity. It is a technique that is used to measure brain activity. It is a technique that is used to measure brain activity. It is a technique that is used to measure brain activity. It is a technique that is used to measure brain activity. It is a technique that is used to measure brain activity. It is a technique that is used to\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Put the answer here.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"What is electroencephalography?\"\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7308734f-be6f-4937-ba8c-90a3fec097a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-3:615547856133:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
