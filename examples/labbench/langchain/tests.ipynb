{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00afbf75-6012-4c57-9dad-4080e36c5687",
   "metadata": {},
   "source": [
    "# LangChain Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dc41cd-690a-4c2d-8253-dd71b609560a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c1be20e-ba74-42e4-a144-3d05841998bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## What is LangChain?\n",
    "\n",
    "LangChain is an open source framework that allows AI developers to combine Large Language Models (LLMs) with external data. \n",
    "\n",
    "**Resources**\n",
    "\n",
    "> LangChain resources\n",
    "> - Landpage: https://readthedocs.org/projects/langchain/db2d\n",
    "> - Comonents: https://docs.langchain.com/docs/category/components\n",
    "> - git: https://github.com/hwchase17/langchain.git\n",
    "> - API Reference: https://api.python.langchain.com/en/latest/\n",
    "\n",
    "> LangChain applications\n",
    "> - [LangChain Awesome](https://github.com/kyrolabs/awesome-langchain)\n",
    "\n",
    "> This notebook is largely based on Greg Kamradt's videos and cookbooks\n",
    "> - [Langchain tuorial suite](https://www.youtube.com/playlist?list=PLqZXAkvF1bPNQER9mLmDbntNfSpzdDIU5)\n",
    "> - [LangChain cookbooks](https://github.com/gkamradt/langchain-tutorials)\n",
    "\n",
    "> Additonal resources and tutorial\n",
    "> - [Cookbook Comprehensive Guide](https://nathankjer.com/introduction-to-langchain/)\n",
    "> - [A Gentle Intro to Chaining LLMs, Agents, and utils via LangChain](https://towardsdatascience.com/a-gentle-intro-to-chaining-llms-agents-and-utils-via-langchain-16cd385fca81)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ab8418-6cca-4c79-9f4c-95604c87214c",
   "metadata": {},
   "source": [
    "## This notebook\n",
    "\n",
    "This notebook collects Python examples. The chapters are based oo the LangChain compoents documented here https://docs.langchain.com/docs/category/components.\n",
    "\n",
    "Some changes though:\n",
    "- use Annoy instead of FAISS as a vector database\n",
    "- use Google Search API instead of SerpAPI\n",
    "- change in examples and additional examples \n",
    "- change in API keys setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff9bffb-d536-4f22-b5dc-2153c49d8340",
   "metadata": {},
   "source": [
    "This notebook has been tested in June 2023 on AWS SageMaker using DataScience 3.0 image.\n",
    "\n",
    "Test environment:\n",
    "> - AWS SageMaker Studio's notebook \n",
    ">> - Kernel image Data Science 3.0\n",
    ">> - t3.medium 2CPU - 4GB\n",
    ">> - Python 3.9.15\n",
    ">> - Linux default 4.14.304-226.531.amzn2.x86_64\n",
    "> - installed packages:\n",
    ">> - langchain 0.0.218\n",
    ">> - openai 0.27.8\n",
    ">> - google_api_python_client 2.90.0\n",
    ">> - tikitoken 0.4.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b092304-ba4c-4392-9668-9a614b16ebe7",
   "metadata": {},
   "source": [
    "More examples in dedicated notebooks in the same folder\n",
    "- tests-large-documents\n",
    "- tests-vdb-chroma\n",
    "- tests-sql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562017b1-fd67-475a-bc77-2019581bf2d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "<div style=\"background-color:green;color:black;text-align:center;padding:1rem;font-size:1.5rem;\">NOTEBOOK SETUP</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf59e64-4e2f-4277-a3af-485c2bf7a8e8",
   "metadata": {},
   "source": [
    "**Instructions**\n",
    "\n",
    "All setups are at the top of the notebook so that you can run all this section initialize the notebook.\n",
    "\n",
    "Notebook chapters are not dependant on each other and may be run in isolation.\n",
    "\n",
    "Before running the setup you may need to create the following resources\n",
    "- request an OpenAI API keys. OpenAI APIs are not free.\n",
    "\n",
    "Additonal requirements for some examples\n",
    "- create a Custom Search Engine in Google Search. it is free.\n",
    "- request an API key for the Google Search service. It is free.\n",
    "- request a Kaggle API\n",
    "\n",
    "Confer to the setup sections for instruction on how to create those resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614b5320-9d11-4e53-9f04-4c3dba3b769d",
   "metadata": {},
   "source": [
    "---\n",
    "## API keys and environment\n",
    "\n",
    "Langchain will get the API keys from environment variables or function parameters.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Never show the keys in shared notebooks, whether it part of the code or a log. A simple way to avoid key leakage, is to use environement variables.  You set the environment variable in the terminal or some local configuration. If so you do not have to set the key here.\n",
    "\n",
    "- If it is easier for you to set the key here by assigning the value, do not forget to empty the string right after you run this block. The environment will be kept in memory as long as the kernel runs.\n",
    "\n",
    "- Be careful when printing the keys. Ensure that you remove the outputs. \n",
    "\n",
    "- Before sharing check that the keys are not printed out by some features of the libraries. Avoid to print libraries' objects. They often hold the API keys as a property and may disclose the key value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4397c800-3ee5-40aa-b0a7-5021985e334a",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "I Store API keys and configuration information in AWS Secrets Manager. The code below retrieves the secret holding the keys. The secret is a JSON string consisting in key/value pairs. It will be used later to set various environnement variables.\n",
    "\n",
    "When using Notebooks and SageMaker do not forget to give permissions to read this secret to SageMaker execution role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0dc42257-8c7a-422c-973a-9e8f9499bc8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://deb.debian.org/debian bullseye InRelease\n",
      "Hit:2 http://deb.debian.org/debian-security bullseye-security InRelease\n",
      "Hit:3 http://deb.debian.org/debian bullseye-updates InRelease\n",
      "Reading package lists... Done\n"
     ]
    }
   ],
   "source": [
    "!apt-get update && apt-get install -y jq 1>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6671d484-9c5d-4818-8449-6df0189fd307",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash --out secrets \n",
    "# using AWS's Secret Manager to store keys\n",
    "# garb the keys and store it into a Pytthon variable\n",
    "export RESPONSE=$(aws secretsmanager get-secret-value --secret-id 'salvia/labbench/tests' )\n",
    "export SECRETS=$( echo $RESPONSE | jq '.SecretString | fromjson')\n",
    "\n",
    "echo $SECRETS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7db74d4-5077-43f1-af34-b3a0bb11bd41",
   "metadata": {},
   "source": [
    "---\n",
    "## pip upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "47403d79-cd44-4f3a-bfcc-179139febbdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip  1>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7188bf6d-17d8-41c0-8c24-6db9ff6e07bb",
   "metadata": {},
   "source": [
    "---\n",
    "## LangChain Setup\n",
    "\n",
    "**Resources**\n",
    "> - [LangChain GetStarted](https://python.langchain.com/docs/get_started/quickstart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c8713a09-0a7a-416c-807a-e2560796f7d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install langchain==0.0.230 1>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51de5b76-5e76-4c8f-bd11-9445f98c6941",
   "metadata": {},
   "source": [
    "---\n",
    "## OpenAI Setup\n",
    "\n",
    "**Resources**\n",
    "> - [OpenAI tutorial on API keys](https://platform.openai.com/docs/quickstart)\n",
    "> - [OpenAI package on Pypi](https://pypi.org/project/openai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "be008522-bd17-47d4-bd6d-58da90214050",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = eval(secrets)[\"OPENAI_API_KEY\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6bc38381-5ee0-4107-9b3a-40dd9cb9302e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install openai==0.27.8 1>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31520149-8e0f-4b08-b4a5-08c118fb638f",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Google Search setup\n",
    "\n",
    "**Resources**\n",
    "\n",
    "> How to configure the Google search in LangChain \n",
    "> - https://python.langchain.com/docs/ecosystem/integrations/google_search\n",
    "\n",
    "> Custom Search Engine configuration \n",
    "> - https://stackoverflow.com/questions/37083058/programmatically-searching-google-in-python-using-custom-search\n",
    "\n",
    "> CSE API \n",
    "> - repo: https://github.com/google/google-api-python-client\n",
    "> - more info: https://developers.google.com/api-client-library/python/apis/customsearch/v1\n",
    "> - complete docs: https://api-python-client-doc.appspot.com/\n",
    "\n",
    "> Get an API key\n",
    "> - https://developers.google.com/custom-search/v1/introduction\n",
    "\n",
    "> Package information\n",
    "> - [Google API client package on Pypi](https://pypi.org/project/google-api-python-client/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "09c65606-95d2-4b96-813f-db44b9ffcbb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Unlock the API and get a key \n",
    "os.environ[\"GOOGLE_API_KEY\"] = eval(secrets)[\"GOOGLE_API_KEY\"]\n",
    "# Create or use an existing Custom Search Engine\n",
    "# on the CSE page under Searcg Engone ID\n",
    "os.environ[\"GOOGLE_CSE_ID\"] = eval(secrets)[\"GOOGLE_CSE_ID\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2e169ba1-9d2d-4bfa-843c-afe61e1228e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install google-api-python-client==2.90.0 1>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eac58b-33e7-49cb-981a-a8c2a3dd573b",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup Annoy as a vector database \n",
    "\n",
    "Some examples requires a Vector Database (document selector, document retrieval).\n",
    "\n",
    "LangChain use ChromaDB by default. For whatever reason it failed to install. Used Annoy instead. An alterntive is FAIIS. You may also want to use online Vector database like Pinecone or Weaviate. \n",
    "\n",
    "Most of these packages include c++ code and requires GCC at the install time. It is not included in SageMaker DataScience 3 image. So the first step is installing GCC. \n",
    "\n",
    "NOTE: Annoy is read-only - once the index is built you cannot add any more emebddings.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Resources**\n",
    "> - [Annoy package on Pypi](https://pypi.org/project/annoy/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460da5ad-e37a-4607-acff-742e34140b27",
   "metadata": {},
   "source": [
    "Install GCC C++ compiler as a prerequiite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "43d6d859-637f-4187-8270-8063f2c4abd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://deb.debian.org/debian bullseye InRelease\n",
      "Hit:2 http://deb.debian.org/debian-security bullseye-security InRelease\n",
      "Hit:3 http://deb.debian.org/debian bullseye-updates InRelease\n",
      "Reading package lists... Done\n"
     ]
    }
   ],
   "source": [
    "!apt-get update && apt-get install -y build-essential 1>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fb5d265a-34ef-4c53-940b-2b762c09859d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install annoy==1.17.3 1>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f13c2955-afd5-4224-bcd9-4810d146220d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install chromadb==0.3.27 1>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037c9e8b-6a56-4014-96f3-a3007138c211",
   "metadata": {},
   "source": [
    "---\n",
    "## SQL database setup\n",
    "\n",
    "- sqlite3: db engine\n",
    "- sqlalchemy: ORM for databases\n",
    "- ipython-sql: SQL magic function\n",
    "- pandas:  data science/data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a173230-e24e-4f21-b6ec-b834c2c64cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "97ddf1fd-6697-439b-bb52-3901c1bca487",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pysqlite3==0.5.1 1>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d12f37f5-cdfc-468c-9e7d-7125fe7d9ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas==1.4.4 1>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e3f7eef0-fb17-4b2d-95fd-1d20522f08e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install sqlalchemy==2.0.18 1>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9087d024-e9dc-4cd4-9378-9d048b2fb056",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ipython-sql==0.5.0 1>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465dcd67-f5de-4e37-a0c8-f61e8b3c6356",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup additional datasets tools\n",
    "<div class=\"alert alert-block alert-warning\"> \n",
    "    TODO <br>\n",
    "</div>\n",
    "\n",
    "Kaggle is used to get some datasets\n",
    "\n",
    "Setup the folowing API Keys\n",
    "- os.environ['KAGGLE_USERNAME'] = 'YOUR_USERNAME'\n",
    "- os.environ['KAGGLE_KEY'] = 'YOUR_KEY'\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Resources**\n",
    "\n",
    "> - https://lindevs.com/set-up-kaggle-api\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5397941c-f16b-4902-bd65-0aa18280961f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get An API Token\n",
    "os.environ[\"KAGGLE_USERNAME\"] = eval(secrets)[\"KAGGLE_USERNAME\"]\n",
    "os.environ[\"KAGGLE_KEY\"] = eval(secrets)[\"KAGGLE_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "81929aef-b48b-4694-af96-e989f2574fb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install kaggle==1.5.15 1>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1abcaa39-8611-4564-a26a-fd39f2e5300d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia==1.4.0 1>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04436cf8-3983-487f-9688-a929ba1ab86b",
   "metadata": {},
   "source": [
    "## Setup additional text managelment tools\n",
    "\n",
    "When working with embeddings additonal packages are required.\n",
    "\n",
    "- tiktoken, as a encoder and tokenizer\n",
    "\n",
    "**Resources**\n",
    "> - [Tiktoken package on Pypi](https://pypi.org/project/tiktoken/)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d3f94302-b020-415a-a118-9dbae639b0bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install lxml 1>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2343f77a-5cd6-4754-a94c-3394c6d064bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4 1>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "189b27d4-8afc-481d-b5bb-610a28171566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken==0.4.0 1>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6986800-5d16-496f-aaa2-3e48587a492f",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"background-color:green;color:black;text-align:center;padding:1rem;font-size:1.5rem;\">LANGCHAIN OVERVIEW</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6bd9f8-efef-4797-ba5d-9eb135e6b862",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Basic features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da068c2f-26b2-4c5e-b72c-23a1d50c8543",
   "metadata": {},
   "source": [
    "---\n",
    "## Get prediction from a langage model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "68e8dd21-8ada-4cd4-a532-43c2e1c3fc63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Switzerland \n",
      "2. Germany \n",
      "3. Sweden \n",
      "4. Norway \n",
      "5. Denmark\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "# OPENAI_API_KEY is requested. Get it from the OpenAI site.\n",
    "# a paid account and available units are requested to be able to place a request.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "text = \"what are the 5 best countries in Europe\"\n",
    "\n",
    "# Actual API call - may tale a while.\n",
    "print(llm(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17250fa0-fe5d-47c1-9406-9822dd38fc9e",
   "metadata": {},
   "source": [
    "---\n",
    "## Manage prompts with templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2bdf0d54-abaa-4aa6-aeb5-275bebb5246b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# setup a prompt\n",
    "prompt = PromptTemplate (\n",
    "    input_variables=[\"interest\"],\n",
    "    template=\"what are the 5 best countries in Europe ranked by {interest}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f09393d7-6674-42f3-97c0-08cd12439d3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='what are the 5 best countries in Europe ranked by food'\n",
      "\n",
      "\n",
      "1. Italy\n",
      "2. Spain\n",
      "3. France\n",
      "4. Greece\n",
      "5. Portugal\n"
     ]
    }
   ],
   "source": [
    "text = prompt.format(interest=\"food\")\n",
    "print(f\"{text=}\")\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "87e3e1e6-871b-42e3-9106-96249b3f726e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='what are the 5 best countries in Europe ranked by siteseeing'\n",
      "\n",
      "\n",
      "1. Italy \n",
      "2. France \n",
      "3. Spain \n",
      "4. Greece \n",
      "5. Germany\n"
     ]
    }
   ],
   "source": [
    "text = prompt.format(interest=\"siteseeing\")\n",
    "print(f\"{text=}\")\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db89d7fa-10d1-495d-8e89-24ab1dce738f",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ae61a5-823a-4585-b308-8e83ea8b6a7a",
   "metadata": {},
   "source": [
    "Chains are sequences of modular components (or other chains) combined in a particular way to accomplish a common use case.\n",
    "\n",
    "\n",
    "Example:\n",
    "- chaining LLM and tool\n",
    "- summarization chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d607ac-9d7f-40f3-86f4-243670f42fe2",
   "metadata": {},
   "source": [
    "---\n",
    "## Built-in chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0d68f8d0-4ee4-48c5-a6b6-5f515c581eea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mdef solution():\n",
      "    \"\"\"If my age is half of my dad's age and he is going to be 60 next year, what is my current age?\"\"\"\n",
      "    dad_age_next_year = 60\n",
      "    my_age_next_year = dad_age_next_year / 2\n",
      "    my_age_current = my_age_next_year - 1\n",
      "    result = my_age_current\n",
      "    return result\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'29.0'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import PALChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "palchain = PALChain.from_math_prompt(llm=llm, verbose=True)\n",
    "\n",
    "\n",
    "text = \"\"\"If my age is half of my dad's age \n",
    "and he is going to be 60 next year, \n",
    "what is my current age?\"\"\"\n",
    "#palchain.run(\"If my age is half of my dad's age and he is going to be 60 next year, what is my current age?\")\n",
    "palchain.run(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ed407-348c-4c00-a405-c8233fe07bb4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "    TODO <br>\n",
    "    - different result each run <br>\n",
    "    - and should be 29.5\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8337725d-b79a-41b9-99f8-d7d55339c149",
   "metadata": {},
   "source": [
    "**OUTPUT**\n",
    "\n",
    "Most of time the response is wrong. It neglects the fact that the father will be 60 necxt year, so he is 59 actually.\n",
    "```\n",
    "> Entering new  chain...\n",
    "def solution():\n",
    "    \"\"\"If my age is half of my dad's age and he is going to be 60 next year, what is my current age?\"\"\"\n",
    "    dad_age_next_year = 60\n",
    "    my_age_fraction = 0.5\n",
    "    my_age_now = dad_age_next_year * my_age_fraction\n",
    "    result = my_age_now\n",
    "    return result\n",
    "\n",
    "> Finished chain.\n",
    "'30.0'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8acdd9d-e9d5-4ca1-b3db-6e8b2a47d81b",
   "metadata": {},
   "source": [
    "**OUTPUT**\n",
    "\n",
    "Once in a while it yields the correct answer.\n",
    "\n",
    "```\n",
    "> Entering new  chain...\n",
    "def solution():\n",
    "    \"\"\"If my age is half of my dad's age and he is going to be 60 next year, what is my current age?\"\"\"\n",
    "    dad_age_current = 59\n",
    "    my_age_current = dad_age_current / 2\n",
    "    result = my_age_current\n",
    "    return result\n",
    "\n",
    "> Finished chain.\n",
    "'29.5'\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b09463-2f2e-4840-9602-1a22478bdd5c",
   "metadata": {},
   "source": [
    "---\n",
    "## Multi-step workflow to feed prompt into the model\n",
    "\n",
    "Output of model 1 is feed into the model 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3fe1a4f0-4c08-47aa-9304-0b6a7002968b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# setup a prompt\n",
    "prompt = PromptTemplate (\n",
    "    input_variables=[\"interest\"],\n",
    "    template=\"what are the 5 best countries in Europe ranked on {interest}\"\n",
    ")\n",
    "\n",
    "# chain feeds the prompt into the langage mmodel.\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "97622a8b-eb22-4218-803d-34d1a76c1af2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' and technology\\n\\n1. Germany \\n2. United Kingdom \\n3. Switzerland \\n4. Sweden \\n5. Finland'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7f73c33b-634d-48fb-9552-7329658c4e23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. United Kingdom \n",
      "2. Germany \n",
      "3. France \n",
      "4. Italy \n",
      "5. Spain\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"tv shows\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c72f8b-7623-4e34-a2b2-c8bf785fa719",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Using the OpenAI Chat API (less expensive) as a chain\n",
    "requires a chain to feed the prompt into the chat \n",
    "\n",
    "**Resources**\n",
    "> - Other Chat APIs: https://api.python.langchain.com/en/latest/modules/chat_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f18f9ea2-c494-462a-aa3e-5e7ca5952d31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking the \"best\" countries for food in Europe is subjective and can vary depending on personal taste preferences. However, here are five countries known for their culinary traditions and diverse gastronomy:\n",
      "\n",
      "1. Italy: Italy is famous for its iconic dishes such as pasta, pizza, gelato, and espresso. Each region has its own unique specialties, making Italian cuisine incredibly diverse and flavorful.\n",
      "\n",
      "2. France: French cuisine is renowned worldwide for its elegance and sophistication. From escargots to foie gras, and from croissants to coq au vin, France offers a rich variety of dishes that celebrate fresh ingredients and culinary excellence.\n",
      "\n",
      "3. Spain: Spain is known for its vibrant and diverse food culture. Tapas, paella, and jamón ibérico are just a few examples of the delicious dishes available. Spanish cuisine often combines bold flavors with fresh ingredients, and each region has its own culinary treasures.\n",
      "\n",
      "4. Greece: Greek cuisine is characterized by its use of fresh, locally sourced ingredients and a focus on simple yet delicious flavors. From moussaka to souvlaki, and from tzatziki to baklava, Greek food offers a delightful mix of Mediterranean flavors.\n",
      "\n",
      "5. Turkey: Turkish cuisine combines influences from the Middle East, the Mediterranean, and Central Asia. It features a wide range of dishes, including kebabs, mezze, and baklava. Turkish cuisine is known for its use of aromatic spices, fresh vegetables, and succulent meats.\n",
      "\n",
      "It's important to note that this list is not exhaustive, and there are many other countries with exceptional culinary traditions in Europe.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chatopenai = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "prompt = PromptTemplate (\n",
    "    input_variables=[\"interest\"],\n",
    "    template=\"what are the 5 best countries in Europe ranked on {interest}\"\n",
    ")\n",
    "\n",
    "llmchain_chat = LLMChain(llm=chatopenai, prompt=prompt)\n",
    "print(llmchain_chat.run(\"food\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3204392e-b1de-47f0-983e-ae05901179ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Leverage LLM Math\n",
    "\n",
    "Evaluating chains that know how to do math.\n",
    "\n",
    "**Resources**\n",
    "> - Langchain module LLM_Math: ttps://python.langchain.com/docs/guides/evaluation/llm_math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "26ed6528-f540-4b2f-9617-e5ae8ffe6d26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `_type` key found, defaulting to `prompt`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: 19\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "from langchain.chains import LLMMathChain\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "prompt = load_prompt('lc://prompts/llm_math/prompt.json')\n",
    "\n",
    "# deprecated\n",
    "##chain = LLMMathChain(llm=llm, prompt=prompt)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "print(chain.run(\"what is the largest prime number lower than 20\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb52e51-3733-4ff5-a8cb-1094a391b98f",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Agent\n",
    "\n",
    "LangChain define agents as decision making engines:\n",
    "> Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an unknown chain that depends on the user's input. In these types of chains, there is a “agent” which has access to a suite of tools. Depending on the user input, the agent can then decide which, if any, of these tools to call.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61edc5bd-e7fa-49ab-85b1-cf08351ea471",
   "metadata": {},
   "source": [
    "---\n",
    "## Test with LLM model only \n",
    "\n",
    "Since models are now updated regularly, I fored an model that is not updated in order to check that it gets an old answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2bb08bb0-a363-40d2-b119-0a2bd0805272",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The Prime Minister of France since May 2022 is Jean Castex.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "# OPENAI_API_KEY is requested. Get it from the OpenAI site.\n",
    "# a paid account and available units are requested to be able to place a request.\n",
    "# low temperature to avoid randomness\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "text = \"Who is the prime minister of France since may 2022\"\n",
    "\n",
    "# Actual API call - may tale a while.\n",
    "print(llm(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af68d10d-eaf3-4f27-b102-7b28e62a9b20",
   "metadata": {},
   "source": [
    "**OUTPUT**\n",
    "\n",
    "'The Prime Minister of France since May 2022 is Jean Castex.'\n",
    "\n",
    "This answer is wrong. Since the model has been trained mid 2021, it is not up-to-date. Elisabeth Borne is Prime Minister since may 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c68946-559e-497d-a971-45c473ea5770",
   "metadata": {},
   "source": [
    "---\n",
    "## Agent leveraging Google Search\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "Make sure:\n",
    "- Google API client is installed\n",
    "- a Custome Search Engine is available (CSE)\n",
    "- the API key has been setup up\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> FIXME it does not work.\n",
    "    <br/>\n",
    "    it seems to work until the model is forced. \n",
    "    model defaults to DPT3.5 turbo which is updated and presummably knows the correct answer.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c64311c2-8e52-4278-a4ac-42a22bceec08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# create a model\n",
    "# forcing an model that doesnot know the correct information\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "\n",
    "# load some tools\n",
    "tools = load_tools([\"google-search\"], llm=llm)\n",
    "\n",
    "# setup an agent\n",
    "agent = initialize_agent(tools, \n",
    "                         llm, \n",
    "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "                         verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ff155cf5-99f9-4ba4-8dbc-9d9366825475",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should research this online\n",
      "Action: google_search\n",
      "Action Input: \"prime minister of France since may 2022\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mÉlisabeth Borne has served as Prime Minister since 16 May 2022. Fifth Republic recordsEdit. Length of the successive governments of the French Fifth ... May 16, 2022 ... The last woman prime minister, Edith Cresson, briefly headed the cabinet from May 1991 to April 1992 under President Francois Mitterrand. Élisabeth Borne is a French politician who has served as Prime Minister of France since May 2022. She is a member of President Emmanuel Macron's party ... May 16, 2022 ... Élisabeth Borne, the minister of labor who previously was in ... https://www.nytimes.com/2022/05/16/world/europe/macron-prime-minister.html. France's newly appointed Prime Minister Elisabeth Borne looks on during a handover ceremony in the courtyard. May 17th 2022 | PARIS. Prime Minister Shri Narendra Modi paid an official visit to France on May 04, 2022 on his way back from the 2nd India-Nordic Summit in Copenhagen. May 16, 2022 ... Borne is the first French female prime minister since Édith Cresson, who briefly headed the cabinet from May 1991 to April 1992 under the ... May 16, 2022 ... France's Labour Minister Elisabeth Borne leaves the Élysée presidential palace after the weekly cabinet meeting in 01:42. France's Labour ... She was given the job by President Emmanuel Macron on 16 May 2022. Prime ministers since 1958Edit. Political parties. Independent May 6, 2022 ... Palais de l'Élysée, Wednesday May 4th, 2022. 1. President of the French Republic Mr. Emmanuel Macron hosted Prime Minister of India, ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Élisabeth Borne has been the Prime Minister of France since May 2022.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Élisabeth Borne has been the Prime Minister of France since May 2022.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Who is the prime minister of France since may 2022\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d37e05f-0f19-4f59-9852-271cac3c6b23",
   "metadata": {},
   "source": [
    "**OUYPUT**\n",
    "\n",
    "with da vinci model. The API ssems to be called but for whatever reason the LLM did not get the correct answer. It seems that it only looked for the confirmation of who it thinks is the prime minister.\n",
    "\n",
    "Interrestingly the query to Google Search is only \"prime minister of France\" while the default model would issue the full sentence (check next test).\n",
    "\n",
    "The response list all the prime ministers as well as trandom information and the bot did not get that the first in the list is the current one.\n",
    "\n",
    "\n",
    "```\n",
    "> Entering new  chain...\n",
    " I need to find out who is the current prime minister of France\n",
    "Action: google_search\n",
    "Action Input: \"prime minister of France\"\n",
    "Observation: The prime minister of France officially the prime minister of the French Republic, is the head of government of the French Republic and the leader of the ... May 16, 2022 ... French President Emmanuel Macron picked Labour Minister Elisabeth Borne as his new prime minister on Monday as he prepares for legislative ... Usually, the Chief Ministers were members of the King's Council (the archaic form of cabinet) or high members of the French nobility or the Catholic clergy. Jun 24, 2023 ... President Biden spoke today with President Emmanuel Macron of France, Chancellor Olaf Scholz of Germany, and Prime Minister Rishi Sunak of ... The head of the government of France has been called the prime minister of France (French: Premier ministre) since 1959, when Michel Debré became the first ... Aug 21, 2022 ... President Biden spoke with President Emmanuel Macron of France, Chancellor Olaf Scholz of Germany, and Prime Minister Boris Johnson of the ... On 3 July 2020, Macron appointed the centre-right Jean Castex as the Prime Minister of France. Castex has been described as being seen to be a social ... 6 days ago ... In February 2015 Prime Minister Manuel Valls was forced to invoke Article 49 of the French constitution, a rarely used measure that allows a ... Archives · Visit of Paul Reynaud, former Prime Minister of France, 4:00PM. Emmanuel Macron was elected eighth President of the French Republic on 7 May 2017. The President of the Republic appoints the Prime Minister, who proposes the ...\n",
    "Thought: I now know the final answer\n",
    "Final Answer: Jean Castex is the current prime minister of France since July 2020.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4fd07136-93bb-47fa-925d-84d022a45920",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# create a model\n",
    "# using default model gpt3.5\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "\n",
    "# load some tools\n",
    "tools = load_tools([\"google-search\"], llm=llm)\n",
    "\n",
    "# setup an agent\n",
    "agent = initialize_agent(tools, \n",
    "                         llm, \n",
    "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "                         verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1650c713-06ad-4446-b664-fcf2254693bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should research this online\n",
      "Action: google_search\n",
      "Action Input: \"prime minister of France since may 2022\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mÉlisabeth Borne has served as Prime Minister since 16 May 2022. Fifth Republic recordsEdit. Length of the successive governments of the French Fifth ... May 16, 2022 ... The last woman prime minister, Edith Cresson, briefly headed the cabinet from May 1991 to April 1992 under President Francois Mitterrand. Élisabeth Borne is a French politician who has served as Prime Minister of France since May 2022. She is a member of President Emmanuel Macron's party ... May 16, 2022 ... Élisabeth Borne, the minister of labor who previously was in ... https://www.nytimes.com/2022/05/16/world/europe/macron-prime-minister.html. France's newly appointed Prime Minister Elisabeth Borne looks on during a handover ceremony in the courtyard. May 17th 2022 | PARIS. Prime Minister Shri Narendra Modi paid an official visit to France on May 04, 2022 on his way back from the 2nd India-Nordic Summit in Copenhagen. May 16, 2022 ... Borne is the first French female prime minister since Édith Cresson, who briefly headed the cabinet from May 1991 to April 1992 under the ... May 16, 2022 ... France's Labour Minister Elisabeth Borne leaves the Élysée presidential palace after the weekly cabinet meeting in 01:42. France's Labour ... She was given the job by President Emmanuel Macron on 16 May 2022. Prime ministers since 1958Edit. Political parties. Independent May 6, 2022 ... Palais de l'Élysée, Wednesday May 4th, 2022. 1. President of the French Republic Mr. Emmanuel Macron hosted Prime Minister of India, ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Élisabeth Borne has been the Prime Minister of France since May 2022.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Élisabeth Borne has been the Prime Minister of France since May 2022.'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Who is the prime minister of France since may 2022\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb7aeec-170c-4491-8344-1191a1102a80",
   "metadata": {},
   "source": [
    "**OUTPUT**\n",
    "\n",
    "The default model yields the correct answer. \n",
    "\n",
    "Weirly enough the query to Google Search is different : \"prime minister of France since may 2022\"\n",
    "\n",
    "Google Search's response is less misleading.\n",
    "\n",
    "```\n",
    "> Entering new  chain...\n",
    " I should research this online\n",
    "Action: google_search\n",
    "Action Input: \"prime minister of France since may 2022\"\n",
    "Observation: Élisabeth Borne has served as Prime Minister since 16 May 2022. Fifth Republic recordsEdit. Length of the successive governments of the French Fifth ... May 16, 2022 ... The last woman prime minister, Edith Cresson, briefly headed the cabinet from May 1991 to April 1992 under President Francois Mitterrand. Élisabeth Borne is a French politician who has served as Prime Minister of France since May 2022. She is a member of President Emmanuel Macron's party ... May 16, 2022 ... Élisabeth Borne, the minister of labor who previously was in ... https://www.nytimes.com/2022/05/16/world/europe/macron-prime-minister.html. France's newly appointed Prime Minister Elisabeth Borne looks on during a handover ceremony in the courtyard. May 17th 2022 | PARIS. Prime Minister Shri Narendra Modi paid an official visit to France on May 04, 2022 on his way back from the 2nd India-Nordic Summit in Copenhagen. May 16, 2022 ... Borne is the first French female prime minister since Édith Cresson, who briefly headed the cabinet from May 1991 to April 1992 under the ... May 16, 2022 ... France's Labour Minister Elisabeth Borne leaves the Élysée presidential palace after the weekly cabinet meeting in 01:42. France's Labour ... She was given the job by President Emmanuel Macron on 16 May 2022. Prime ministers since 1958Edit. Political parties. Independent May 6, 2022 ... Palais de l'Élysée, Wednesday May 4th, 2022. 1. President of the French Republic Mr. Emmanuel Macron hosted Prime Minister of India, ...\n",
    "Thought: I now know the final answer\n",
    "Final Answer: Élisabeth Borne has been the Prime Minister of France since May 2022.\n",
    "\n",
    "> Finished chain.\n",
    "'Élisabeth Borne has been the Prime Minister of France since May 2022.'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ca262458-0284-4558-a632-6090d2717058",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The Prime Minister of France since May 2022 is Jean Castex.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# create a model\n",
    "# using defauklt model alone\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "text = \"Who is the prime minister of France since may 2022\"\n",
    "\n",
    "print(llm(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ee67950c-6e31-4da6-a192-3e0aa0e1d07e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/langchain/llms/openai.py:173: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/langchain/llms/openai.py:753: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI language model, I cannot provide real-time information as my responses are based on data available up until September 2021. As of my last update, the Prime Minister of France is Jean Castex, who assumed office on July 3, 2020. However, please note that political positions can change, and it is always best to refer to the latest news sources for the most up-to-date information.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# create a model\n",
    "# using defauklt model alone\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "text = \"Who is the prime minister of France since may 2022\"\n",
    "\n",
    "print(llm(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "43a171bf-7776-476c-b12e-2d30573d7884",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI language model, I do not have access to real-time information. However, as of May 2022, the current Prime Minister of France is Jean Castex.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# create a model\n",
    "# using defauklt model alone\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-0301\", temperature=0)\n",
    "\n",
    "text = \"Who is the prime minister of France since may 2022\"\n",
    "\n",
    "print(llm(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28631b0b-d1ec-4d14-bf44-091f8abd1584",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Memory - Conversation\n",
    "\n",
    "maintaining a conversation with a user in natural language and simulating the way a human would behave as a conversational partner. \n",
    "\n",
    "**Resources**\n",
    "> -https://python.langchain.com/docs/modules/memory/how_to/conversational_customization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "13541ec0-684d-48da-a205-ef1c6e3e7937",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi There\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi there! It's nice to meet you. How can I help you today?\""
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import OpenAI, ConversationChain\n",
    "\n",
    "# create a model\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "conversation.predict(input=\"Hi There\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7271c38f-0a3e-44fb-9d8a-1014667c3467",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi There\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: What is the first thing that I said to you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' You said \"Hi there!\"'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is the first thing that I said to you?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e2613f8b-beb3-47b3-b842-e5f2b4746143",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi There\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: What is the first thing that I said to you?\n",
      "AI:  You said \"Hi there!\"\n",
      "Human: What is an alternative for the first thing that I said to you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' An alternative for the first thing you said to me is \"Hello!\"'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is an alternative for the first thing that I said to you?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3912d8-050d-4c4a-89e8-1d9ce0a1f4cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "<div style=\"background-color:green;color:black;text-align:center;padding:1rem;font-size:1.5rem;\">LANGCHAIN COMPONENTS</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684914b4-7b16-4f0b-bde2-6ef6c7df5079",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Schemas\n",
    "\n",
    "Basic data types and schemas that are used throughout the codebase.\n",
    "\n",
    "There are 3 types of schemas\n",
    "- Text (see above)\n",
    "- Prompts\n",
    "- Messages \n",
    "- Document\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Resources**\n",
    "> - Schhemas component:  https://docs.langchain.com/docs/components/schema/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7bb2c3-a6cd-4fee-b6a8-964a5f811433",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e8843ce9-1f71-4b43-a704-d456dc5653e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Germany\n",
      "2. Switzerland\n",
      "3. Netherlands\n",
      "4. Sweden\n",
      "5. Finland\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "# OPENAI_API_KEY is requested. Get it from the OpenAI site.\n",
    "# a paid account and available units are requested to be able to place a request.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "text = \"what are the 5 best countries in Europe\"\n",
    "\n",
    "# Actual API call - may tale a while.\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf610897-44b3-4d72-8e38-3dd50b0776bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Chat messages\n",
    "Chat messages are like text with a type\n",
    "\n",
    "There are 3 types\n",
    "- System: background context that tells the AI what to do\n",
    "- Human: inputs sent by the user\n",
    "- AI : response of the AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0c8d5a96-f105-45f2-ba15-0f0b108cd9ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2f7a0269-e45c-445b-b44f-b19fc4be5e53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [ SystemMessage(content=\"You are a nice AI and help users to feature out what to eat.\")]\n",
    "     \n",
    "messages.append( HumanMessage(content=\"I like tuna, list some recipes.\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3e203228-09cd-4405-9679-c4eebfbd7c1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here are a few tuna recipes you might enjoy:\n",
      "\n",
      "1. Tuna Salad: Mix canned tuna with mayo, diced celery, chopped red onion, and a squeeze of lemon juice. Serve it on a bed of lettuce, in a sandwich, or with crackers.\n",
      "\n",
      "2. Tuna Pasta: Cook your favorite pasta according to package instructions. In a separate pan, sauté garlic and cherry tomatoes in olive oil. Add in drained canned tuna, a pinch of red pepper flakes, and salt. Toss the cooked pasta in the sauce and sprinkle with fresh parsley.\n",
      "\n",
      "3. Tuna Poke Bowl: Combine diced fresh tuna with soy sauce, sesame oil, rice vinegar, and a pinch of sugar. Serve the marinated tuna over a bowl of steamed rice, and add toppings like avocado, cucumber, edamame, and sesame seeds.\n",
      "\n",
      "4. Tuna Steaks: Marinate fresh tuna steaks in a mixture of soy sauce, ginger, garlic, and honey. Grill or sear the steaks until cooked to your desired level of doneness. Serve with a side of steamed vegetables or a salad.\n",
      "\n",
      "5. Tuna Niçoise Salad: Arrange cooked baby potatoes, blanched green beans, cherry tomatoes, sliced hard-boiled eggs, and canned tuna on a bed of mixed greens. Drizzle with a vinaigrette dressing made with olive oil, Dijon mustard, red wine vinegar, and herbs.\n",
      "\n",
      "Remember to adjust the recipes to your taste preferences and dietary needs. Enjoy your tuna dishes!\n"
     ]
    }
   ],
   "source": [
    "response = chat(messages)\n",
    "messages.append( AIMessage(content=response.content) )\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d140f70f-a86d-4634-899e-12401f0315cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here's a quick and easy recipe for Tuna Salad:\n",
      "\n",
      "Ingredients:\n",
      "- 2 cans of tuna, drained\n",
      "- 1/4 cup mayonnaise\n",
      "- 1/4 cup diced celery\n",
      "- 2 tablespoons chopped red onion\n",
      "- 1 tablespoon lemon juice (optional)\n",
      "- Salt and pepper, to taste\n",
      "\n",
      "Instructions:\n",
      "1. In a mixing bowl, add the drained tuna and break it up into smaller pieces using a fork.\n",
      "2. Add mayonnaise, diced celery, chopped red onion, and lemon juice (if desired) to the bowl with the tuna.\n",
      "3. Mix all the ingredients together until well combined. If you prefer a creamier texture, you can add more mayonnaise.\n",
      "4. Season the tuna salad with salt and pepper according to your taste.\n",
      "5. Serve the tuna salad on a bed of lettuce, as a sandwich filling, or with crackers for a tasty snack.\n",
      "\n",
      "Feel free to adjust the ingredients and measurements to suit your preferences. Enjoy your homemade tuna salad!\n"
     ]
    }
   ],
   "source": [
    "messages.append( HumanMessage(content=\"show the first one.\") )\n",
    "\n",
    "response = chat(messages)\n",
    "messages.append( AIMessage(content=response.content) )\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af92d18-1d2b-4c55-aa71-e5a91fde59bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Examples\n",
    "An list of input output pairs thet represent the input and expected output.\n",
    "\n",
    "Used to fine tune a model or do in-context learning.\n",
    "\n",
    "**Resources**\n",
    "> - Prompt Template:  https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/few_shot_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ca97e03b-8d6c-4156-8358-a9504901d733",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== exemple prompt ===\n",
      "question: red bold\n",
      "color:red; font-style:bold;\n",
      "\n",
      "=== prompt ===\n",
      "question: red bold\n",
      "color:red; font-style:bold;\n",
      "\n",
      "question: green italic\n",
      "color:green; font-style:italic;\n",
      "\n",
      "question: blue bold\n",
      "color:blue; font-style:bold;\n",
      "\n",
      "question: pink\n",
      "color:pink;\n",
      "\n",
      "question: green\n",
      "color:green;\n",
      "\n",
      "question: pink italic\n",
      "color:pink; font-style:italic;\n",
      "\n",
      "question: pink bold\n",
      "\n",
      "=== answer ===\n",
      "\n",
      "color:pink; font-style:bold;\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# create the example set\n",
    "\n",
    "examples = [\n",
    "    { \"question\": \"red bold\", \"answer\": \"color:red; font-style:bold;\"},\n",
    "    { \"question\": \"green italic\", \"answer\":  \"color:green; font-style:italic;\"},\n",
    "    { \"question\": \"blue bold\", \"answer\":  \"color:blue; font-style:bold;\"},\n",
    "    { \"question\": \"pink\", \"answer\":  \"color:pink;\"},\n",
    "    { \"question\": \"green\", \"answer\":  \"color:green;\"},\n",
    "    { \"question\": \"pink italic\", \"answer\":  \"color:pink; font-style:italic;\"}\n",
    "    \n",
    "]    \n",
    "\n",
    "# Configure a formatter that will format the few shot examples into a string. \n",
    "# This formatter should be a PromptTemplate object.\n",
    "\n",
    "example_prompt = PromptTemplate (\n",
    "    input_variables=[\"question\", \"answer\"], \n",
    "    template=\"question: {question}\\n{answer}\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== exemple prompt ===\")\n",
    "print(example_prompt.format(**examples[0]))\n",
    "\n",
    "\n",
    "# Finally, create a FewShotPromptTemplate object. \n",
    "# This object takes in the few shot examples and the formatter for the few shot examples.\n",
    "\n",
    "prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples, \n",
    "    example_prompt=example_prompt, \n",
    "    suffix=\"question: {input}\", \n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format(input=\"pink bold\")\n",
    "\n",
    "print(\"\\n=== prompt ===\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"\\n=== answer ===\")\n",
    "print(llm(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2471f9d0-6000-4f48-ad1a-f5e187a8aa54",
   "metadata": {},
   "source": [
    "---\n",
    "## Documents\n",
    "\n",
    "An unstructured object that conaints a pieces of text and metadatas.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO  resource </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648ce2f7-0d40-49ea-b0a8-6e45f9b5f2f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO how to use this concept? \n",
    "make some knowledge available?\n",
    "how to use metadata?\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "097d47df-18e7-4a58-b76c-d7438d3135bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document\n",
      "page_content='\\n\\n        So she swallowed one of the cakes and was delighted to find that she\\n        began shrinking directly. As soon as she was small enough to get through\\n        the door, she ran out of the house and found quite a crowd of little\\n        animals and birds waiting outside. They all made a rush at Alice the\\n        moment she appeared, but she ran off as hard as she could and soon found\\n        herself safe in a thick wood.\\n        ' metadata={'author': 'Lewis Caroll', 'identifier': '1234'}\n",
      "\n",
      "Summary\n",
      " After eating a cake, Alice shrinks and escapes the house. She runs away from a crowd of animals and birds and finds refuge in a thick wood.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "\n",
    "# temperature 0 means no randomness\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "\n",
    "document = Document(\n",
    "    page_content=\"\"\"\n",
    "\n",
    "        So she swallowed one of the cakes and was delighted to find that she\n",
    "        began shrinking directly. As soon as she was small enough to get through\n",
    "        the door, she ran out of the house and found quite a crowd of little\n",
    "        animals and birds waiting outside. They all made a rush at Alice the\n",
    "        moment she appeared, but she ran off as hard as she could and soon found\n",
    "        herself safe in a thick wood.\n",
    "        \"\"\",\n",
    "    metadata={\n",
    "        'author':\"Lewis Caroll\",\n",
    "        'identifier':\"1234\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Document\")\n",
    "print(document)\n",
    "\n",
    "# the attribute stuff instruct the run the chain once\n",
    "chain = load_summarize_chain(\n",
    "    llm, \n",
    "    chain_type=\"stuff\", \n",
    "    verbose=False)\n",
    "\n",
    "# run the chain against the documment\n",
    "summary = chain.run([document])\n",
    "    \n",
    "print(\"\\nSummary\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bf6802aa-fc02-4c31-9d43-93c9f1cbb6d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_sample= \"\"\"\n",
    "<p>Just at this moment her head struck against the roof of the hall; in\n",
    "fact, she was now rather more than nine feet high, and she at once took\n",
    "up the little golden key and hurried off to the garden door.</p>\n",
    "<p>Poor Alice! It was as much as she could do, lying down on one side, to\n",
    "look through into the garden with one eye; but to get through was more\n",
    "hopeless than ever. She sat down and began to cry again.</p>\n",
    "<p>She went on shedding gallons of tears, until there was a large pool all\n",
    "'round her and reaching half down the hall.</p>\n",
    "<p>After a time, she heard a little pattering of feet in the distance and\n",
    "she hastily dried her eyes to see what was coming. It was the White\n",
    "Rabbit returning, splendidly dressed, with a pair of white kid-gloves in\n",
    "one hand and a large fan in the other. He<span class=\"pagenum\"><a id=\"Page_10\">[Pg 10]</a></span> came trotting along in a\n",
    "great hurry, muttering to himself, \"Oh! the Duchess, the Duchess! Oh!\n",
    "<i>won't</i> she be savage if I've kept her waiting!\"</p>\n",
    "<p class=\"figright\"><a href=\"https://www.gutenberg.org/cache/epub/19033/images/i005.jpg\" id=\"id-6474075343490533101\"><img alt=\"Illo5\" src=\"./Alice&#39;s Adventures in Wonderland, by Lewis Carroll_files/i005_th.jpg\" id=\"id-5171882188453704008\"></a></p><p>When the Rabbit came near her, Alice began, in a low, timid voice, \"If\n",
    "you please, sir—\" The Rabbit started violently, dropped the white\n",
    "kid-gloves and the fan and skurried away into the darkness as hard as he\n",
    "could go.</p>\n",
    "<p>Alice took up the fan and gloves and she kept fanning herself all the\n",
    "time she went on talking. \"Dear, dear! How queer everything is to-day!\n",
    "And yesterday things went on just as usual. <i>Was</i> I the same when I got\n",
    "up this morning? But if I'm not the same, the next question is, 'Who in\n",
    "the world am I?' Ah, <i>that's</i> the great puzzle!\"</p>\n",
    "<p>As she said this, she looked down at her hands and was surprised to see\n",
    "that she had put on one of the Rabbit's little white kid-gloves while\n",
    "she was talking. \"How <i>can</i> I have done that?\" she thought. \"I must be\n",
    "growing small again.\" She got up and went to the table to measure\n",
    "herself by it and found that she was now about two feet high and was\n",
    "going on<span class=\"pagenum\"><a id=\"Page_11\">[Pg 11]</a></span> shrinking rapidly. She soon found out that the cause of this\n",
    "was the fan she was holding and she dropped it hastily, just in time to\n",
    "save herself from shrinking away altogether.</p>\n",
    "<p>\"That <i>was</i> a narrow escape!\" said Alice, a good deal frightened at the\n",
    "sudden change, but very glad to find herself still in existence. \"And\n",
    "now for the garden!\" And she ran with all speed back to the little door;\n",
    "but, alas! the little door was shut again and the little golden key was\n",
    "lying on the glass table as before. \"Things are worse than ever,\"\n",
    "thought the poor child, \"for I never was so small as this before,\n",
    "never!\"</p>\n",
    "<p>As she said these words, her foot slipped, and in another moment,\n",
    "splash! she was up to her chin in salt-water. Her first idea was that\n",
    "she had somehow fallen into the sea. However, she soon made out that she\n",
    "was in the pool of tears which she had wept when she was nine feet high.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "53092123-3262-48de-b082-e4bedd51432b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_tokens=901\n",
      "\n",
      "Alice was very tall and bumped her head on the roof of the hall. She tried to get into the garden but it was too hard. She started to cry and made a big pool of tears. Then she heard a noise and saw the White Rabbit. He was wearing fancy clothes and had a fan and gloves. He was in a hurry and said he was late for the Duchess. Alice picked up the fan and gloves and kept talking. She looked down and saw she was wearing one of the Rabbit's gloves. She was shrinking and dropped the fan to stop it. She was very scared but happy to still be alive. She ran back to the door but it was locked. She slipped and fell into the pool of tears she had made.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Note, the default model is already 'text-davinci-003' \n",
    "# temperature 0 means no randomness\n",
    "llm = OpenAI(temperature=0, model_name='text-davinci-003')\n",
    "\n",
    "# check the number of tokens\n",
    "num_tokens = llm.get_num_tokens(text_sample)\n",
    "print(f\"{num_tokens=}\")\n",
    "\n",
    "# build a document reuse text sampke above\n",
    "doc = Document(\n",
    "    page_content=text_sample,\n",
    "    metadata={\n",
    "        'author':\"Lewis Caroll\",\n",
    "        'title':\"Alice in Wonderland\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# chain expect a list of documents\n",
    "docs = [doc]\n",
    "\n",
    "# setup. a custom prompt\n",
    "# a defaukt one is provide: write a concise summary\n",
    "prompt_template = \"\"\"\n",
    "%INSTRUCTIONS:\n",
    "Please summarize the following piece of text.\n",
    "Respond in a manner that a 5 year old would understand.\n",
    "\n",
    "%TEXT:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "# the attribute stuff instruct the run the chain once\n",
    "chain = load_summarize_chain(\n",
    "    llm, \n",
    "    chain_type=\"stuff\", \n",
    "    prompt=prompt, \n",
    "    verbose=False)\n",
    "\n",
    "# run the chain against the documment\n",
    "summary = chain.run(docs)\n",
    "    \n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee98eca-37a3-45c4-90e5-eb2360e1f5e1",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Models\n",
    "LangChain provides interfaces and integrations for two types of models:\n",
    "- LLMs: Models that take a text string as input and return a text string\n",
    "- Chat models: Models that are backed by a language model but take a list of Chat Messages as input and return a Chat Message\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Resources**\n",
    "> - Model Component: https://python.langchain.com/docs/modules/model_io/models/\n",
    "> - List of models: https://platform.openai.com/docs/models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6a1df5-7967-4841-b1eb-1ff58d0a15cb",
   "metadata": {},
   "source": [
    "---\n",
    "## Langage Model \n",
    "LLMs: Models that take a text string as input and return a text string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ddd6cc95-2731-47d9-8624-2392e3a83315",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSaturday'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# additnal parameters to select a mode, pass the API key ...\n",
    "llm = OpenAI(model_name=\"text-ada-001\", temperature=0.7)\n",
    "\n",
    "llm(\"What day comes after Friday?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981bf185-6a93-4eb5-bdc4-779724aab952",
   "metadata": {},
   "source": [
    "---\n",
    "## Chat Model \n",
    "Chat models: Models that are backed by a language model but take a list of Chat Messages as input and return a Chat \n",
    "\n",
    "Also make sense for a unique interaction as Chat API is less expensive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c2099b96-d00a-4aa0-908d-e75a0e4b3a16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "daafb170-e231-4132-bcc2-c88aac2fe475",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here are a few tuna recipes that you might enjoy:\\n\\n1. Tuna Salad: Mix canned tuna with mayonnaise, diced onions, celery, and seasonings like salt, pepper, and lemon juice. Serve it on a sandwich, wrap, or with crackers.\\n\\n2. Tuna Melt: Spread tuna salad on sliced bread, top with cheese, and toast it in a skillet until the cheese is melted and bread is crispy.\\n\\n3. Tuna Pasta Salad: Cook pasta according to package instructions. Drain and cool it. Then, mix it with canned tuna, diced vegetables (like bell peppers, cherry tomatoes, and cucumbers), olives, and a dressing of your choice (such as lemon vinaigrette or creamy ranch).\\n\\n4. Tuna Steaks: Pat dry fresh tuna steaks and season them with salt, pepper, and a squeeze of lemon. Cook them on a hot grill or sear them in a skillet for a few minutes on each side until desired doneness.\\n\\n5. Tuna Poke Bowl: Marinate diced fresh tuna in soy sauce, sesame oil, and a splash of rice vinegar. Serve it over a bed of rice or quinoa, and top with diced avocado, cucumber, seaweed, and sesame seeds.\\n\\n6. Tuna Nicoise Salad: Assemble a salad with boiled potatoes, blanched green beans, hard-boiled eggs, cherry tomatoes, and canned tuna. Drizzle it with a simple vinaigrette dressing.\\n\\nRemember to adjust the recipes according to your preferences and dietary needs. Enjoy your tuna dishes!', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [ \n",
    "    SystemMessage(content=\"You are a nice AI and help users to feature out what to eat.\"),\n",
    "    HumanMessage(content=\"I like tuna, list some recipes.\")\n",
    "]\n",
    "     \n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91de6b77-6cb7-414b-a475-c471c3ca6c5b",
   "metadata": {},
   "source": [
    "---\n",
    "### Text Embedding Model\n",
    "\n",
    "Convert text into a series of numbers (a vector) which holds the meaning of the text.\n",
    "\n",
    "Mainly used for text comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8e7d5813-a505-47d6-a8ef-98881a175c46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding length: 1536\n",
      "5 first values of the vector: [-0.0020272971596568823, -0.016961609944701195, 0.013975410722196102, -0.014824817888438702, 0.001639920868910849]\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "text=\"A leader should know all about truth and honesty, and when to see the difference. (Truck) - Bromeliad Trilogy\"\n",
    "\n",
    "text_embedding = embeddings.embed_query(text)\n",
    "\n",
    "print(f\"embedding length: {len(text_embedding)}\")\n",
    "print(f\"5 first values of the vector: {text_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab0bbcc-adc2-449f-97e4-839a36b3d4f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# 7. prompts\n",
    "A \"prompt\" refers to the input to the model. This input is rarely hard coded, but rather is often constructed from multiple components. A PromptTemplate is responsible for the construction of this input. LangChain provides several classes and functions to make constructing and working with prompts easy.\n",
    "\n",
    "LangChain documentation is split into four sections:\n",
    "- PromptValue: The class representing an input to a model.\n",
    "- Prompt Templates: The class in charge of constructing a PromptValue.\n",
    "- Example Selectors: Often times it is useful to include examples in prompts. These examples can be hardcoded, but it is often more powerful if they are dynamically selected.\n",
    "- Output Parsers: Language models (and Chat Models) output text. But many times you may want to get more structured information than just text back. This is where output parsers come in. Output Parsers are responsible for (1) instructing the model how output should be formatted, (2) parsing output into the desired formatting (including retrying if necessary).\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Resources**\n",
    "> - Prompts Component: https://docs.langchain.com/docs/components/prompts/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc36ebe4-aba6-4cdb-850d-1937839c8608",
   "metadata": {},
   "source": [
    "---\n",
    "## Simple prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d37b8bcc-33ca-4deb-9845-81675d529758",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "It is incorrect; tomorrow is Tuesday.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0.9)\n",
    "\n",
    "# write a simple  prompt. use \"\"\" to allow multiline string.\n",
    "prompt = \"\"\"\n",
    "Today is Monday. Tomorrow is Wednesday.\n",
    "\n",
    "What is wrong with this statement?\n",
    "\"\"\"\n",
    "\n",
    "# query the model\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f8a344-70ab-471d-918e-ee98e7e4fdf1",
   "metadata": {},
   "source": [
    "---\n",
    "## Prompt with template and placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "837116ab-bf8b-49ed-bb3f-a896e14fb14d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt='\\n    Today is Monday. Tomorrow is Wednesday.\\n\\n    What is wrong with this statement?\\n    '\n",
      "\n",
      "The statement is incorrect; tomorrow is Tuesday.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0.9)\n",
    "\n",
    "# setup a prompt. use \"\"\" to allow multiline string.\n",
    "template = PromptTemplate (\n",
    "    input_variables=[\"today\", \"tomorrow\"],\n",
    "    template=\"\"\"\n",
    "    Today is {today}. Tomorrow is {tomorrow}.\n",
    "\n",
    "    What is wrong with this statement?\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "prompt = template.format(today=\"Monday\", tomorrow=\"Wednesday\")\n",
    "print(f\"{prompt=}\")\n",
    "\n",
    "# query the model\n",
    "\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa35aff7-3105-4aed-82ab-7281c517eca3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = template.format(today=\"Thursday\", tomorrow=\"Friday\")\n",
    "print(f\"{prompt=}\")\n",
    "\n",
    "# query the model\n",
    "\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc54ed33-925e-405f-8eaf-a4a57ed78771",
   "metadata": {},
   "source": [
    "---\n",
    "## Example selectors and Few Shot Learning\n",
    "\n",
    "A way to select from a series of examples in few shot learning \n",
    "\n",
    "**Resources**\n",
    "> - Example Selector: https://api.python.langchain.com/en/latest/modules/example_selector.html\n",
    "> - Few shot learning: https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/few_shot_examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e40343-df4e-4b65-8951-8372060ca6d0",
   "metadata": {},
   "source": [
    "### Example selectors and Few Shot Learning with NGram\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> FIXME </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "39afd4e6-4d1d-4348-b871-343e34cb0314",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== exemple prompt ===\n",
      "question: red bold\n",
      "color:red; font-style:bold;\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Select and order examples based on ngram overlap score (sentence_bleu score).\u001b[39;00m\n\u001b[1;32m     35\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpink bold\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 37\u001b[0m example_selector \u001b[38;5;241m=\u001b[39m \u001b[43mNGramOverlapExampleSelector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_examples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquestion\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03mfrom langchain.prompts.example_selector import SemanticSimilarityExampleSelector\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03mfrom langchain.vectorstores import Chroma\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Finally, create a FewShotPromptTemplate object. \u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# This object takes in the few shot examples and the formatter for the few shot examples.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/langchain/prompts/example_selector/ngram_overlap.py:91\u001b[0m, in \u001b[0;36mNGramOverlapExampleSelector.select_examples\u001b[0;34m(self, input_variables)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_examples\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_variables: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m     86\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return list of examples sorted by ngram_overlap_score with input.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    Descending order.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m    Excludes any examples with ngram_overlap_score less than or equal to threshold.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43minput_variables\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m())\n\u001b[1;32m     92\u001b[0m     examples \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     93\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexamples)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.example_selector import NGramOverlapExampleSelector\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# create the example set\n",
    "\n",
    "examples = [\n",
    "    { \"question\": \"red bold\", \"answer\": \"color:red; font-style:bold;\"},\n",
    "    { \"question\": \"green italic\", \"answer\":  \"color:green; font-style:italic;\"},\n",
    "    { \"question\": \"blue bold\", \"answer\":  \"color:blue; font-style:bold;\"},\n",
    "    { \"question\": \"pink\", \"answer\":  \"color:pink;\"},\n",
    "    { \"question\": \"green\", \"answer\":  \"color:green;\"},\n",
    "    { \"question\": \"pink italic\", \"answer\":  \"color:pink; font-style:italic;\"}\n",
    "    \n",
    "]    \n",
    "\n",
    "# Configure a formatter that will format the few shot examples into a string. \n",
    "# This formatter should be a PromptTemplate object.\n",
    "\n",
    "example_prompt = PromptTemplate (\n",
    "    input_variables=[\"question\", \"answer\"], \n",
    "    template=\"question: {question}\\n{answer}\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== exemple prompt ===\")\n",
    "print(example_prompt.format(**examples[0]))\n",
    "\n",
    "\n",
    "# Select and order examples based on ngram overlap score (sentence_bleu score).\n",
    "\n",
    "question = \"pink bold\"\n",
    "\n",
    "example_selector = NGramOverlapExampleSelector.select_examples(\n",
    "    examples,\n",
    "    question\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # This is the list of examples available to select from.\n",
    "    examples,\n",
    "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    OpenAIEmbeddings(),\n",
    "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    Chroma,\n",
    "    # This is the number of examples to produce.\n",
    "    k=1\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Finally, create a FewShotPromptTemplate object. \n",
    "# This object takes in the few shot examples and the formatter for the few shot examples.\n",
    "\n",
    "prompt_template = FewShotPromptTemplate(\n",
    "    #example_selector=example_selector, \n",
    "    examples=selected_examples, \n",
    "    example_prompt=example_prompt, \n",
    "    suffix=\"question: {input}\", \n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format(input=question)\n",
    "\n",
    "print(\"\\n=== prompt ===\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"\\n=== answer ===\")\n",
    "print(llm(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be5012c-2708-417f-a6e8-4a1beb5349df",
   "metadata": {},
   "source": [
    "### Example selectors and Few Shot Learning with similarities\n",
    "\n",
    "requires a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "be6e5756-6345-4b70-844b-f7959fe0d70e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== exemple prompt ===\n",
      "question: red bold\n",
      "color:red; font-style:bold;\n",
      "\n",
      "=== prompt ===\n",
      "question: red bold\n",
      "color:red; font-style:bold;\n",
      "\n",
      "question: pink italic\n",
      "color:pink; font-style:italic;\n",
      "\n",
      "question: pink bold\n",
      "\n",
      "=== answer ===\n",
      "\n",
      "color:pink; font-weight:bold;\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Annoy\n",
    "#from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# create the example set\n",
    "\n",
    "examples = [\n",
    "    { \"question\": \"red bold\", \"answer\": \"color:red; font-style:bold;\"},\n",
    "    { \"question\": \"green italic\", \"answer\":  \"color:green; font-style:italic;\"},\n",
    "    { \"question\": \"blue bold\", \"answer\":  \"color:blue; font-style:bold;\"},\n",
    "    { \"question\": \"pink\", \"answer\":  \"color:pink;\"},\n",
    "    { \"question\": \"green\", \"answer\":  \"color:green;\"},\n",
    "    { \"question\": \"pink italic\", \"answer\":  \"color:pink; font-style:italic;\"}\n",
    "    \n",
    "]    \n",
    "\n",
    "# Configure a formatter that will format the few shot examples into a string. \n",
    "# This formatter should be a PromptTemplate object.\n",
    "\n",
    "example_prompt = PromptTemplate (\n",
    "    input_variables=[\"question\", \"answer\"], \n",
    "    template=\"question: {question}\\n{answer}\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== exemple prompt ===\")\n",
    "print(example_prompt.format(**examples[0]))\n",
    "\n",
    "# Example selector that selects examples based on SemanticSimilarity.\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # This is the list of examples available to select from.\n",
    "    examples,\n",
    "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    OpenAIEmbeddings(),\n",
    "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    #Chroma,\n",
    "    Annoy,\n",
    "    # This is the number of examples to produce.\n",
    "    k=2\n",
    ")\n",
    "\n",
    "# Finally, create a FewShotPromptTemplate object. \n",
    "# This object takes in the few shot examples and the formatter for the few shot examples.\n",
    "\n",
    "prompt_template = FewShotPromptTemplate(\n",
    "    example_selector=example_selector, \n",
    "    example_prompt=example_prompt, \n",
    "    suffix=\"question: {input}\", \n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format(input=\"pink bold\")\n",
    "\n",
    "print(\"\\n=== prompt ===\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"\\n=== answer ===\")\n",
    "print(llm(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2b5aae-d6c0-4ebe-a12e-521b2f807045",
   "metadata": {},
   "source": [
    "---\n",
    "## Output Parser and response format\n",
    "\n",
    "A way to format the outpu\n",
    "- Format nstructions: An autogenerated prompt telling how the result should be formatted\n",
    "- parser: a method which will extract the output int hte desired format. you may prvie a custom parser\n",
    "\n",
    "\n",
    "**Resources**\n",
    "> - OutputParser:https://docs.langchain.com/docs/components/prompts/output-parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "46e0cc93-9499-4ac7-a3b2-669bb3f121e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "format_instructions\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This is a poorly formatted string.\n",
      "\t\"good_string\": string  // This is a your string reformatted.\n",
      "}\n",
      "```\n",
      "\n",
      "prompt\n",
      "\n",
      "You will be given a poorly formatted string from a user. \n",
      "Reformat it and make sure all the words are spelled correctly.\n",
      "\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This is a poorly formatted string.\n",
      "\t\"good_string\": string  // This is a your string reformatted.\n",
      "}\n",
      "```\n",
      "\n",
      "% USER_INPUT:\n",
      "Wellcom to Californya!\n",
      "\n",
      "YOUR RESPONSE:\n",
      "\n",
      "\n",
      "response=\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": \"Wellcom to Californya!\",\n",
      "\t\"good_string\": \"Welcome to California!\"\n",
      "}\n",
      "```\n",
      "\n",
      "parsed output=\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bad_string': 'Wellcom to Californya!',\n",
       " 'good_string': 'Welcome to California!'}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0.9)\n",
    "\n",
    "# how you would like the response to be structured\n",
    "# periods at the send of sentence are required. \n",
    "# If not there description ends up in the json text and break the JSON format\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"bad_string\", description=\"This is a poorly formatted string.\"),\n",
    "    ResponseSchema(name=\"good_string\", description=\"This is a your string reformatted.\")\n",
    "]\n",
    "\n",
    "# How you would like to parse your output\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# check instructions\n",
    "format_instructions =output_parser.get_format_instructions()\n",
    "print(\"\\nformat_instructions\")      \n",
    "print(format_instructions)      \n",
    "\n",
    "template = \"\"\"\n",
    "You will be given a poorly formatted string from a user. \n",
    "Reformat it and make sure all the words are spelled correctly.\n",
    "\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER_INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=['user_input'],\n",
    "    partial_variables={'format_instructions': format_instructions},\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# format the user input as a prompt\n",
    "# for whateveer reason it does not work well with format.\n",
    "# format_promt retruns an object, not a string and should be converted to a string \n",
    "prompt = prompt_template.format_prompt(user_input=\"Wellcom to Californya!\").to_string()\n",
    "print(\"\\nprompt\")\n",
    "print(prompt)\n",
    "\n",
    "# gets the response\n",
    "response = llm(prompt)\n",
    "print(\"\\nresponse=\")      \n",
    "print(response)      \n",
    "\n",
    "# gets the JSON document\n",
    "print(\"\\nparsed output=\")     \n",
    "\n",
    "# comma sometimes missing\n",
    "response.replace('\"good_string\"',',\"good_string\"')\n",
    "\n",
    "output_parser.parse(response)                   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3bc48a-c126-4f79-b8d8-5f65b3387ecd",
   "metadata": {},
   "source": [
    "---\n",
    "# 8. Indexes\n",
    "\n",
    "Indexes refer to ways to structure documents so that LLMs can best interact with them. This module contains utility functions for working with documents, different types of indexes, and then examples for using those indexes in chains.\n",
    "\n",
    "LangChain documentation is split into four sections:\n",
    "\n",
    "- Document Loaders: Classes responsible for loading documents from various sources.\n",
    "- Text Splitters: Classes responsible for splitting text into smaller chunks.\n",
    "- VectorStores: The most common type of index. One that relies on embeddings.\n",
    "- Retrievers: Interface for fetching relevant documents to combine with language models.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Resource**\n",
    "> - Indexes Component: https://docs.langchain.com/docs/components/indexing/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdab962-d2e2-40c9-bcc7-26a07996a493",
   "metadata": {},
   "source": [
    "**Instructions**\n",
    "\n",
    "For the example below, make sure that:\n",
    "- a vector database client is installed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7a68d6-2561-4630-89b0-f543d3880746",
   "metadata": {},
   "source": [
    "---\n",
    "## Document Loaders\n",
    "\n",
    "Easy ways to import documents from other sources \n",
    "and make it available for use in your language models.\n",
    "\n",
    "**Resources**\n",
    "> -  Document Loaders: https://python.langchain.com/docs/modules/data_connection/document_loaders\n",
    "> - List of loaders: https://github.com/hwchase17/langchain/tree/master/langchain/document_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "77127064-7c75-4561-81a7-a3d73f08cb44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76 comments\n",
      "\n",
      "Here's a sample (first 100 chars of the 3 first items)\n",
      "Ozzie_osman 5 months ago  \n",
      "             | next [–] \n",
      "\n",
      "LangChain is awesome. For people not sure what \n",
      "Ozzie_osman 5 months ago  \n",
      "             | parent | next [–] \n",
      "\n",
      "Also, another library to check out is \n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import HNLoader\n",
    " \n",
    "# Setup a Hacker News loader\n",
    "loader = HNLoader(\"https://news.ycombinator.com/item?id=34422627\")\n",
    " \n",
    "data = loader.load()\n",
    " \n",
    "print(f\"Found {len(data)} comments\")\n",
    "\n",
    "\n",
    "sample = '\\n'.join([x.page_content[:100] for x in data[:2]])\n",
    "print(\"\\nHere's a sample (first 100 chars of the 3 first items)\")\n",
    "print(sample)\n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd41d74-2bd1-4d5c-8409-b27c6cdb5d1a",
   "metadata": {},
   "source": [
    "---\n",
    "## Text Splitters\n",
    "\n",
    "allow you to split a document into smaller chunk\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO  resource +FIXME pb with loader </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c32da233-903e-4ae2-8c5e-42cd0c6f3d54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[130], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Setup a HTML loader\u001b[39;00m\n\u001b[1;32m      8\u001b[0m loader \u001b[38;5;241m=\u001b[39m BSHTMLLoader(document_path)\n\u001b[0;32m----> 9\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m document(s)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuument content\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/langchain/document_loaders/html_bs.py:51\u001b[0m, in \u001b[0;36mBSHTMLLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen_encoding) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 51\u001b[0m     soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbs_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m text \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mget_text(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_text_separator)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mtitle:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/bs4/__init__.py:250\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m     builder_class \u001b[38;5;241m=\u001b[39m builder_registry\u001b[38;5;241m.\u001b[39mlookup(\u001b[38;5;241m*\u001b[39mfeatures)\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m builder_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FeatureNotFound(\n\u001b[1;32m    251\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a tree builder with the features you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    252\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequested: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. Do you need to install a parser library?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m             \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(features))\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# with the remaining **kwargs.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m builder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import BSHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    " \n",
    "# This is a long document we can split up.    \n",
    "document_path = \"data/Alice's Adventures in Wonderland, by Lewis Carroll.html\"\n",
    " \n",
    "# Setup a HTML loader\n",
    "loader = BSHTMLLoader(document_path)\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Found {len(documents)} document(s)\")\n",
    "\n",
    "\n",
    "print(\"docuument content\")\n",
    "start = 2200\n",
    "print(documents[0].page_content[start-200:start+30])\n",
    "\n",
    " \n",
    "# The recommended TextSplitter is the RecursiveCharacterTextSplitter. \n",
    "# This will split documents recursively by different characters - starting with \"\\n\\n\", then \"\\n\", then \" \".\n",
    "# This is nice because it will try to keep all the semantically relevant content in the same place \n",
    "# for as long as possible.\n",
    "# Important parameters to know here are chunkSize and chunkOverlap. \n",
    "# chunkSize controls the max size (in terms of number of characters) of the final documents. \n",
    "# chunkOverlap specifies how much overlap there should be between chunks. \n",
    "# in practice they default to 4000 and 200 respectively.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    " \n",
    "texts = text_splitter.create_documents([document[0].page_content])\n",
    " \n",
    "print(f\"\\nSplitted into {len(texts)} parts\")\n",
    " \n",
    "print(\"Preview:\")\n",
    "i = int(start/150)\n",
    "print(texts[i+1].page_content, \"\\n-\")\n",
    "print(texts[i+2].page_content, \"\\n-\")\n",
    "print(texts[i+3].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a947a9d1-e893-4ac8-b854-e5a777363c27",
   "metadata": {},
   "source": [
    "---\n",
    "## Vextor Store and Retrievers \n",
    "A retriever is an interface that returns documents given an unstructured query. \n",
    "\n",
    "A retriever does not need to be able to store documents, only to return (or retrieve) it. \n",
    "\n",
    "It usually relies to a vector store as a document management backbone.\n",
    "\n",
    "A vector store is a particular type of database optimized for storing documents and their embeddings, and then fetching of the most relevant documents for a particular query, ie. those whose embeddings are most similar to the embedding of the query.\n",
    "\n",
    "- local : ChromaDB, FAISS, Annoy\n",
    "- Online: Pinecone, Weaviate\n",
    "\n",
    "However a retriever is more general than a vector store and there are other types of retrievers as well, e.g. Wikipedia or search engines like Elastic Search or Kendra.\n",
    "\n",
    "\n",
    "Question answering over documents consists of four steps:\n",
    "1. Create an index\n",
    "2. Create a Retriever from that index\n",
    "3. Create a question answering chain\n",
    "4. Ask questions\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Resources**\n",
    "> - Lit of retrievers: https://python.langchain.com/docs/modules/data_connection/retrievers/\n",
    "> - LangChain Supported VectorStores: https://api.python.langchain.com/en/latest/modules/vectorstores.html\n",
    "> - Retrievers: https://github.com/hwchase17/langchain/tree/master/langchain/retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3769182e-3197-4cc3-8ab4-d05717fb2922",
   "metadata": {},
   "source": [
    "### Store document in a Vector Store and retrieve information\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> FIXME pb with loader </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fedfae67-fba3-4e5d-ac09-d2d6b465dcb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Setup a HTML loader\u001b[39;00m\n\u001b[1;32m     10\u001b[0m loader \u001b[38;5;241m=\u001b[39m BSHTMLLoader(document_path)\n\u001b[0;32m---> 11\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m document(s)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Get your splitter ready\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Using small chunk for the sake of example. \u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# in practice they default to 4000 and 200 respectively.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/langchain/document_loaders/html_bs.py:51\u001b[0m, in \u001b[0;36mBSHTMLLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen_encoding) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 51\u001b[0m     soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbs_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m text \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mget_text(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_text_separator)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mtitle:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/bs4/__init__.py:250\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m     builder_class \u001b[38;5;241m=\u001b[39m builder_registry\u001b[38;5;241m.\u001b[39mlookup(\u001b[38;5;241m*\u001b[39mfeatures)\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m builder_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FeatureNotFound(\n\u001b[1;32m    251\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a tree builder with the features you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    252\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequested: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. Do you need to install a parser library?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m             \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(features))\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# with the remaining **kwargs.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m builder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import BSHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Annoy\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    " \n",
    "# This is the source document.    \n",
    "document_path = \"data/Alice's Adventures in Wonderland, by Lewis Carroll.html\"\n",
    " \n",
    "# Setup a HTML loader\n",
    "loader = BSHTMLLoader(document_path)\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Found {len(documents)} document(s)\")\n",
    "\n",
    "\n",
    "# Get your splitter ready\n",
    "# Using small chunk for the sake of example. \n",
    "# in practice they default to 4000 and 200 respectively.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=25)\n",
    " \n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"\\nSplitted into {len(texts)} parts\")\n",
    "\n",
    "# Get embedding engine ready\n",
    "embeddings = OpenAIEmbeddings()\n",
    " \n",
    "# Embedd your texts andd store them in the vector database\n",
    "# dtabase is in memory. it might be savecd to a file and loader later on.\n",
    "db = Annoy.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e39c41-6df4-445f-8314-db8d7c49624a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Init a retriever for this db\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# retrieve indexed documents relevant for the query\n",
    "query = \"who is the White Rabbit?\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "print(f\"\\nFound {len(docs)}\")\n",
    "\n",
    "samples = \"\\n\\n\".join([x.page_content[:200] for x in docs[:5]])\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448aa357-9d47-4db2-af62-cd4acd58bd78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Asking theLLM\n",
    "# the response will be based on the retrieved documents \n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97504cf-53c0-4a59-848d-459279b1499f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a99a8f-58db-4ef3-b75e-0f584222f7c1",
   "metadata": {},
   "source": [
    "### Save and load db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fb324b-98dc-449d-8906-ebd536635ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Annoy\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "docstore_file_path = \"alice_docstore\"\n",
    "\n",
    "db.save_local(docstore_file_path)\n",
    "\n",
    "loaded_vector_store = Annoy.load_local(\n",
    "   docstore_file_path, embeddings=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# same document similar to White Red abbit\n",
    "loaded_vector_store.similarity_search_with_score(\"White Rabbit\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ee55b6-6c70-48d0-862b-8a411fc63b57",
   "metadata": {},
   "source": [
    "### One line index creation and information retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c90307a-e8a0-4cb1-a514-38cd52f74685",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import BSHTMLLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Annoy\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "# This is the source document.    \n",
    "document_path = \"data/Alice's Adventures in Wonderland, by Lewis Carroll.html\"\n",
    " \n",
    "# Setup a HTML loader\n",
    "loader = BSHTMLLoader(document_path)\n",
    "\n",
    "# creating an indexer\n",
    "# default to Chroma as a vector database\n",
    "# Use CharacterTextSplitter. May also be RecursiveCharacterTextSplitter.\n",
    "index_creator = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=Annoy,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    text_splitter=CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    ")\n",
    "\n",
    "index = index_creator.from_loaders([loader])\n",
    "\n",
    "# retrieve indexed documents relevant for the query\n",
    "query = \"who is the White Rabbit?\"\n",
    "index.query(query)\n",
    "\n",
    "print(f\"\\nFound {len(docs)}\")\n",
    "\n",
    "samples = \"\\n\\n\".join([x.page_content[:200] for x in docs[:5]])\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27cfddd-ed2d-4892-a521-aa473ccd2c67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Ask the question to the model \n",
    "# the response will be based on the retrieved documents \n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), \n",
    "                                 chain_type=\"stuff\", \n",
    "                                 retriever=index.vectorstore.as_retriever())\n",
    "\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de6f5a1-9765-424c-819c-b79ccb8acc97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0081edf7-2762-4667-895f-5849b15424ea",
   "metadata": {},
   "source": [
    "---\n",
    "## Wikipedia retriever\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO wikipedia retriever </div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> \n",
    "    Move to tools agent_excutor example  <br>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> FIXME pb with loader </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "272625be-5241-4278-9c8f-85a4d81cfed2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should start by searching for a quick summary of Napoleon Bonaparte on Wikipedia. Then, I can search for Serena Williams and compare the two to find any commonalities.\n",
      "Action: Wikipedia\n",
      "Action Input: \"Napoleon Bonaparte\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Napoleon\n",
      "Summary: Napoleon Bonaparte (born Napoleone Buonaparte; 15 August 1769 – 5 May 1821), later known by his regnal name Napoleon I, was a French military commander and political leader who rose to prominence during the French Revolution and led successful campaigns during the Revolutionary Wars. He was the de facto leader of the French Republic as First Consul from 1799 to 1804, then of the French Empire as Emperor of the French from 1804 until 1814 and again in 1815. Napoleon's political and cultural legacy endures to this day, as a highly celebrated and controversial leader. He initiated many liberal reforms that have persisted in society, and is considered one of the greatest military commanders in history. His campaigns are still studied at military academies worldwide. Between three and six million civilians and soldiers died in the Napoleonic Wars.Napoleon was born on the island of Corsica, not long after its annexation by France, to a native family descending from minor Italian nobility. He supported the French Revolution in 1789 while serving in the French army, and tried to spread its ideals to his native Corsica. He rose rapidly in the Army after he saved the governing French Directory by firing on royalist insurgents. In 1796, he began a military campaign against the Austrians and their Italian allies, scoring decisive victories and becoming a national hero. Two years later, he led a military expedition to Egypt that served as a springboard to political power. He engineered a coup in November 1799 and became First Consul of the Republic. In 1804, to expand and consolidate his power, he crowned himself Emperor of the French.\n",
      "Differences with the United Kingdom meant France faced the War of the Third Coalition by 1805. Napoleon shattered this coalition with victories in the Ulm campaign and at the Battle of Austerlitz, which led to the dissolution of the Holy Roman Empire. In 1806, the Fourth Coalition took up arms against him. Napoleon defeated Prussia at the battles of Jena and Auerstedt, marched the Grande Armée into Eastern Europe, and defeated the Russians in June 1807 at Friedland, forcing the defeated nations of the Fourth Coalition to accept the Treaties of Tilsit. Two years later, the Austrians challenged the French again during the War of the Fifth Coalition, but Napoleon solidified his grip over Europe after triumphing at the Battle of Wagram.\n",
      "Hoping to extend the Continental System, his embargo against Britain, Napoleon invaded the Iberian Peninsula and declared his brother Joseph the King of Spain in 1808. The Spanish and the Portuguese revolted in the Peninsular War aided by a British army, culminating in defeat for Napoleon's marshals. Napoleon launched an invasion of Russia in the summer of 1812. The resulting campaign witnessed the catastrophic retreat of Napoleon's Grande Armée. In 1813, Prussia and Austria joined Russian forces in a Sixth Coalition against France, resulting in a large coalition army defeating Napoleon at the Battle of Leipzig. The coalition invaded France and captured Paris, forcing Napoleon to abdicate in April 1814. He was exiled to the island of Elba, between Corsica and Italy. In France, the Bourbons were restored to power.\n",
      "Napoleon escaped in February 1815 and took control of France. The Allies responded by forming a Seventh Coalition, which defeated Napoleon at the Battle of Waterloo in June 1815. The British exiled him to the remote island of Saint Helena in the Atlantic, where he died in 1821 at the age of 51.\n",
      "Napoleon had an extensive impact on the modern world, bringing liberal reforms to the lands he conquered, especially the regions of the Low Countries, Switzerland, and parts of modern Italy and Germany. He implemented many liberal policies in France and Western Europe.\n",
      "\n",
      "Page: Napoleon III\n",
      "Summary: Napoleon III (born Charles Louis Napoléon Bonaparte; 20 April 1808 – 9 January 1873) was the first President of France (as Louis-Napoléon Bonaparte) from 1848 to \u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow that I have a summary of Napoleon Bonaparte, I can search for Serena Williams on Wikipedia and compare the two.\n",
      "Action: Wikipedia\n",
      "Action Input: \"Serena Williams\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Serena Williams\n",
      "Summary: Serena Jameka Williams (born September 26, 1981) is an American former professional tennis player. Widely regarded as one of the greatest tennis players of all time, she was ranked world No. 1 in singles by the Women's Tennis Association (WTA) for 319 weeks, including a joint-record 186 consecutive weeks, and finished as the year-end No. 1 five times. She won 23 Grand Slam women's singles titles, the most in the Open Era, and the second-most of all time. She is the only player to accomplish a career Golden Slam in both singles and doubles.Along with her older sister Venus, Serena Williams was coached by her parents Oracene Price and Richard Williams. Turning professional in 1995, she won her first major singles title at the 1999 US Open. From the 2002 French Open to the 2003 Australian Open, she was dominant, winning all four major singles titles (each time over Venus in the final) to achieve a non-calendar year Grand Slam and the career Grand Slam, known as the 'Serena Slam'. The next few years saw her claim two more singles majors, but suffer from injury and decline in form. Beginning in 2007, however, she gradually returned to form despite continued injuries, retaking the world No. 1 singles ranking. Beginning at the 2012 Wimbledon Championships, Williams returned to dominance, claiming Olympic gold (completing the Career Golden Slam in singles) and winning eight out of thirteen singles majors, including all four in a row from 2014–15 to achieve a second \"Serena Slam\". At the 2017 Australian Open, she won her 23rd major singles title, surpassing Steffi Graf's Open Era record. She then took a break from professional tennis after becoming pregnant and reached four major finals upon returning to play. In August 2022, Williams announced her impending \"evolution\" away from professional tennis and played what was expected to be her final match at the 2022 US Open.Williams also won 14 major women's doubles titles, all with her sister Venus, and the pair was unbeaten in major doubles finals (the best unbeaten record in major finals in any discipline of the sport). The pair achieved a non-calendar year Grand Slam between the 2009 Wimbledon Championships and the 2010 French Open, which granted the sisters the doubles world No. 1 ranking. Serena won four Olympic gold medals, three in women's doubles—an all-time joint record in tennis, shared with her sister. The duo are the only women in the Open Era to win Olympic gold in both singles and doubles. She also won two major mixed doubles titles, both in 1998. She is the only singles player, male or female, to complete three Career Golden Slams – one in women's singles and two in same-sex doubles.The arrival of the Williams sisters has been credited with ushering in a new era of power and athleticism on the women's professional tennis tour. Serena holds a combined 39 major titles: 23 in singles, 14 in women's doubles, and two in mixed doubles. She is joint-third on the all-time list and second in the Open Era for total major titles. She is the most recent woman to simultaneously hold all four major singles titles (2002–03 and 2014–15), and the most recent woman to win the Surface Slam (major titles on hard, clay and grass courts in the same calendar year), doing so in 2015. She is also, with Venus, the most recent player to have simultaneously held all four major women's doubles titles (2009–10).\n",
      "Williams was the world's highest paid woman athlete in 2016, earning almost $29 million. She repeated this feat in 2017 when she was the only woman on Forbes' list of the 100 highest-paid athletes, with $27 million in prize money and endorsements. She won the Laureus Sportswoman of the Year award a record four times (2003, 2010, 2016, 2018), and in December 2015 was named Sportsperson of the Year by Sports Illustrated magazine. She is the highest-earning woman athlete of all time.\n",
      "\n",
      "Page: Williams sisters\n",
      "Summary: The Williams sisters are two professional American te\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "ename": "OutputParserException",
     "evalue": "Could not parse LLM output: `Based on the summaries of Napoleon Bonaparte and Serena Williams, the commonalities between the two are that they both achieved great success in their respective fields. Napoleon Bonaparte was a highly celebrated and controversial leader who is considered one of the greatest military commanders in history. Serena Williams is widely regarded as one of the greatest tennis players of all time, having won 23 Grand Slam women's singles titles, the most in the Open Era. Both Napoleon and Serena have had a significant impact on their respective domains and have left a lasting legacy.`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[131], line 21\u001b[0m\n\u001b[1;32m     11\u001b[0m tools \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     12\u001b[0m     Tool(\n\u001b[1;32m     13\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWikipedia\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     ),\n\u001b[1;32m     17\u001b[0m ]\n\u001b[1;32m     19\u001b[0m agent_executor \u001b[38;5;241m=\u001b[39m initialize_agent(tools, llm, agent\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzero-shot-react-description\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 21\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCan you please provide a quick summary of Napoleon Bonaparte? \u001b[39;49m\u001b[38;5;130;43;01m\\\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;43m                          Then do a separate search and tell me what the commonalities are with Serena Williams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/langchain/chains/base.py:440\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    439\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    441\u001b[0m         _output_key\n\u001b[1;32m    442\u001b[0m     ]\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    446\u001b[0m         _output_key\n\u001b[1;32m    447\u001b[0m     ]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/langchain/chains/base.py:243\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    242\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 243\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    244\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    245\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    246\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    247\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/langchain/chains/base.py:237\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    231\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    232\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    233\u001b[0m     inputs,\n\u001b[1;32m    234\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    236\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 237\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    240\u001b[0m     )\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    242\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/langchain/agents/agent.py:987\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m--> 987\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    994\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m    995\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[1;32m    996\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[1;32m    997\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/langchain/agents/agent.py:803\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    801\u001b[0m     raise_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_error:\n\u001b[0;32m--> 803\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    804\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/langchain/agents/agent.py:792\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Take a single step in the thought-action-observation loop.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m \n\u001b[1;32m    788\u001b[0m \u001b[38;5;124;03mOverride this to take control of how the agent makes and acts on choices.\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m--> 792\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/langchain/agents/agent.py:444\u001b[0m, in \u001b[0;36mAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m full_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_full_inputs(intermediate_steps, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    443\u001b[0m full_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mpredict(callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfull_inputs)\n\u001b[0;32m--> 444\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/langchain/agents/mrkl/output_parser.py:42\u001b[0m, in \u001b[0;36mMRKLOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AgentFinish(\n\u001b[1;32m     38\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: text\u001b[38;5;241m.\u001b[39msplit(FINAL_ANSWER_ACTION)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()}, text\n\u001b[1;32m     39\u001b[0m     )\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*:[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*(.*?)\u001b[39m\u001b[38;5;124m\"\u001b[39m, text, re\u001b[38;5;241m.\u001b[39mDOTALL):\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse LLM output: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     44\u001b[0m         observation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Format: Missing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAction:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m after \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThought:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     45\u001b[0m         llm_output\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m     46\u001b[0m         send_to_llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     47\u001b[0m     )\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*Action\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*Input\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*:[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*(.*)\u001b[39m\u001b[38;5;124m\"\u001b[39m, text, re\u001b[38;5;241m.\u001b[39mDOTALL\n\u001b[1;32m     50\u001b[0m ):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse LLM output: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     53\u001b[0m         observation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Format:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m         send_to_llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     57\u001b[0m     )\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Could not parse LLM output: `Based on the summaries of Napoleon Bonaparte and Serena Williams, the commonalities between the two are that they both achieved great success in their respective fields. Napoleon Bonaparte was a highly celebrated and controversial leader who is considered one of the greatest military commanders in history. Serena Williams is widely regarded as one of the greatest tennis players of all time, having won 23 Grand Slam women's singles titles, the most in the Open Era. Both Napoleon and Serena have had a significant impact on their respective domains and have left a lasting legacy.`"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "\n",
    "# model_name='gpt-4'\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "wikipedia = WikipediaAPIWrapper()\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Wikipedia\",\n",
    "        func=wikipedia.run,\n",
    "        description=\"Useful for when you need to get information from wikipedia about a single topic\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "agent_executor = initialize_agent(tools, llm, agent='zero-shot-react-description', verbose=True)\n",
    "\n",
    "output = agent_executor.run(\"Can you please provide a quick summary of Napoleon Bonaparte? \\\n",
    "                          Then do a separate search and tell me what the commonalities are with Serena Williams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a7a9c8-d0bc-4a5c-9f9d-fff2f870d0ed",
   "metadata": {},
   "source": [
    "---\n",
    "# 9. Memory\n",
    "\n",
    "\n",
    "Memory is the concept of storing and retrieving data in the process of a conversation. \n",
    "\n",
    "There are two main methods:\n",
    "- Based on input, fetch any relevant pieces of data\n",
    "- Based on the input and output, update state accordingly\n",
    "\n",
    "There are two main types of memory: short term and long term.\n",
    "- Short term memory generally refers to how to pass data in the context of a singular conversation (generally is previous ChatMessages or summaries of them).\n",
    "- Long term memory deals with how to fetch and update information between conversations.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Resource**\n",
    "> - Memory Component: https://docs.langchain.com/docs/components/memory/\n",
    "> - Chat Message History: https://docs.langchain.com/docs/components/memory/chat_message_history\n",
    "> - [LangChain: Enhancing Performance with Memory Capacity](https://towardsdatascience.com/langchain-enhancing-performance-with-memory-capacity-c7168e097f81)\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO vs Conversation and buffer memory (check blog)?</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO Long term memory</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ed82a7-7392-4637-85f2-a85b86d40378",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from pprint import pprint\n",
    " \n",
    "chat = ChatOpenAI(temperature=0)\n",
    " \n",
    "history = ChatMessageHistory()\n",
    " \n",
    "history.add_ai_message(\"hi!\")\n",
    " \n",
    "history.add_user_message(\"what is the capital of france?\")\n",
    "\n",
    "#After adding messages to the history, you can pass this history to the language model \n",
    "#to generate context-aware responses:\n",
    "\n",
    "ai_response = chat(history.messages)\n",
    "history.add_ai_message(ai_response.content)\n",
    "\n",
    "print(f\"{ai_response=}\")\n",
    "print(f\"\\nhistory.messages:\")\n",
    "pprint(history.messages, compact=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbd121c-3290-4133-a9c6-c55113fd139f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history.add_user_message(\"what is the population os this city?\")\n",
    "\n",
    "ai_response = chat(history.messages)\n",
    "history.add_ai_message(ai_response.content)\n",
    "\n",
    "print(f\"{ai_response.content=}\")\n",
    "print(f\"\\nhistory.messages:\")\n",
    "pprint(history.messages, compact=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc922de-52de-4c30-8faa-4feded2137d2",
   "metadata": {},
   "source": [
    "---\n",
    "# 10. Chains\n",
    "Chains are sequences of modular components (or other chains) combined in a particular way to accomplish a common use case.\n",
    "\n",
    "\n",
    "Example:\n",
    "- chaining LLM and tool\n",
    "- summarization chain\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Resources**\n",
    "> - Chain Component: https://docs.langchain.com/docs/components/chains/\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO index related chain https://docs.langchain.com/docs/components/chains/index_related_chains  </div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ede334-ad80-49fe-82aa-c9b22c12ac8d",
   "metadata": {},
   "source": [
    "## Simple sequential model\n",
    "\n",
    "A Simple Sequential Chain helps break up tasks to avoid language models getting distracted, confused, or hallucinating when asked to perform too many tasks in a row.\n",
    "\n",
    "In this example, the chain first receives the user location (Rome) and outputs a classic dish from Rome. Then, it provides a simple recipe for that classic dish. The verbose=True parameter ensures that the chain prints statements during its execution, making it easier to debug and understand the chain’s progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1b15c453-5b4a-4857-a232-0ddbfe12067e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain\n",
    " \n",
    "# Cretae a model with high randomness\n",
    "llm = OpenAI(temperature=1)\n",
    " \n",
    "# Step 1 - dish for location\n",
    "\n",
    "template = \"\"\"\n",
    "Your job is to come up with a classic dish from the area that the users suggests. \n",
    "\n",
    "% USER LOCATION {user_location} \n",
    "\n",
    "YOUR RESPONSE: \n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_location\"], template=template)\n",
    "\n",
    "location_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    " \n",
    "\n",
    "# Step 2 - Recipe\n",
    "template = \"\"\"\n",
    "Given a meal, give a short and simple recipe on how to make that dish at home. \n",
    "\n",
    "% MEAL {user_meal} \n",
    "\n",
    "YOUR RESPONSE: \n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
    " \n",
    "meal_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# chain the steps\n",
    "# set verbose to True to check what happes\n",
    "overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=False)\n",
    " \n",
    "review = overall_chain.run(\"Rome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cec1cc-9ce8-4ddc-82c1-69d3a60c977b",
   "metadata": {},
   "source": [
    "## Summarization Chain\n",
    "\n",
    "The Summarization Chain breaks the text into smaller chunks and summarizing each chunk, creating a final summary based on the individual summaries.\n",
    "\n",
    "In this example, the chain first splits the essay into chunks of 700 characters. It then generates summaries for each chunk and creates a final concise summary based on these individual summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f2cfde91-71be-42ed-91d1-d3154f8e4ab3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[133], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Setup a HTML loader\u001b[39;00m\n\u001b[1;32m     13\u001b[0m loader \u001b[38;5;241m=\u001b[39m BSHTMLLoader(document_path)\n\u001b[0;32m---> 14\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Get your splitter ready\u001b[39;00m\n\u001b[1;32m     17\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m700\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/langchain/document_loaders/html_bs.py:51\u001b[0m, in \u001b[0;36mBSHTMLLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen_encoding) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 51\u001b[0m     soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbs_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m text \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mget_text(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_text_separator)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mtitle:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/bs4/__init__.py:250\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m     builder_class \u001b[38;5;241m=\u001b[39m builder_registry\u001b[38;5;241m.\u001b[39mlookup(\u001b[38;5;241m*\u001b[39mfeatures)\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m builder_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FeatureNotFound(\n\u001b[1;32m    251\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a tree builder with the features you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    252\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequested: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. Do you need to install a parser library?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m             \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(features))\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# with the remaining **kwargs.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m builder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import BSHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Cretae a model with low randomness\n",
    "llm = OpenAI(temperature=1)\n",
    "\n",
    "# This is the source document.    \n",
    "document_path = \"data/Alice's Adventures in Wonderland, by Lewis Carroll.html\"\n",
    " \n",
    "# Setup a HTML loader\n",
    "loader = BSHTMLLoader(document_path)\n",
    "documents = loader.load()\n",
    " \n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=50)\n",
    " \n",
    "# Split your docs into texts\n",
    "# only kept first 1 000 characters of the document to save computing\n",
    "texts = text_splitter.split_documents(documents[:1000])\n",
    " \n",
    "# There is a lot of complexity hidden in this one line. \n",
    "# the attribute map_reduce instruct the chain to \n",
    "# - first apply the model to each chunck (map stage) \n",
    "# - then all map results and apply the model (reduce stage)\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)\n",
    "summary = chain.run(texts)\n",
    "    \n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4996a3b2-f9d0-496a-abf4-b28fcb300b5e",
   "metadata": {},
   "source": [
    "**OUTPUT**\n",
    " \n",
    "Some map summaries\n",
    "\n",
    "> Alice hears the White Rabbit muttering to itself, concerned that the Duchess will execute it for losing the fan and pair of white kid-gloves. Alice offers to help the Rabbit search for them, but they are nowhere to be found because everything has changed since Alice's dip in the pool.\n",
    "\n",
    "> Alice meets a Rabbit who accuses her of being his housemaid Mary Ann and orders her to fetch his gloves and fan. She finds a neat little house with the Rabbit's name on a brass plate and goes in without knocking. She is afraid of meeting the real Mary Ann before she can find the fan and gloves.\n",
    "\n",
    "> Alice finds her way into a room with a table in the window, containing a fan and some gloves. She notices a bottle and drinks from it, hoping it will make her grow large again. When she drinks half of the bottle she finds her head pressing against the ceiling, so she hastily puts it down.\n",
    "\n",
    "> A character wishes she wouldn't grow anymore, but sadly she continues to grow rapidly. As a result, she kneels on the floor, puts her arm out the window and her foot up the chimney, and is uncertain of her fate.\n",
    "\n",
    " \n",
    "Final summary\n",
    "\n",
    "> In Lewis Carroll's Alice's Adventures in Wonderland, Alice follows a White Rabbit into a strange world and has to navigate unexpected events and peculiar characters. She eventually meets a Caterpillar who helps her regain control of her changing size. Project Gutenberg is a non-profit organization committed to making electronic books free to the public. Donations up to $5,000 are available, and the full license stipulates amounts and terms of use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80463e50-7e7f-49ed-9596-cdbef0250ced",
   "metadata": {},
   "source": [
    "## Summarize stored documents\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO  make use of the vector db</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706f5599-b76f-45e3-a8fe-566f0ea9b328",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 11. Agents\n",
    "\n",
    "LangChain define agents as decision making engines:\n",
    "> Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an unknown chain that depends on the user's input. In these types of chains, there is a “agent” which has access to a suite of tools. Depending on the user input, the agent can then decide which, if any, of these tools to call.\n",
    "\n",
    "It splits the documentation into the following sections:\n",
    "> - Tools: How language models interact with other resources.\n",
    "> - Agents: The language model that drives decision making.\n",
    "> - Toolkits: Sets of tools that when used together can accomplish a specific task.\n",
    "> - Agent Executor: The logic for running agents with tools.\n",
    "\n",
    "\n",
    "**Resources**\n",
    "> - Agents: https://docs.langchain.com/docs/components/agents/\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c830fd4-b796-4226-b087-1cef2bdd7efb",
   "metadata": {},
   "source": [
    "## Tool\n",
    "Tools are interfaces an agent can call to interact with other services\n",
    "\n",
    "**Resources**\n",
    "> - Tools: https://python.langchain.com/docs/modules/agents/tools/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a158512-085d-4782-bc72-8575f83630a2",
   "metadata": {},
   "source": [
    "**Instructions**\n",
    "\n",
    "For the example below, make sure that:\n",
    "- Google API client is installed\n",
    "- a Custome Search Engine is available (CSE)\n",
    "- the API key has been setup up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce2ddb0-06b1-4703-9b96-093a66b921da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "search = GoogleSearchAPIWrapper()\n",
    "\n",
    "tool = Tool(\n",
    "    name=\"Google Search\",\n",
    "    description=\"Search Google for recent results.\",\n",
    "    func=search.run,\n",
    ")\n",
    "\n",
    "tool.run(\"Who is the French Prime Minister name since May 2022?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e130c84-a1f4-4621-93c9-5e2e1632da08",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Agent leveraging tools\n",
    "\n",
    "Google Search and LLM-math are predefined tools:\n",
    "- LLM-Math is a langage model trained to do math logic.\n",
    "- Google)search tool allow to place queries on Google Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4811ff27-c48a-4c9e-a5a1-161dde97802d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# create a model\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# load some tools\n",
    "tools = load_tools([\"google-search\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# setup an agent\n",
    "agent = initialize_agent(tools, \n",
    "                         llm, \n",
    "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "                         verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a780cbf-a82f-4e8d-95c1-3d89f04e59b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent.run(\"How many Teslas have been sold in 2022. Multiple by 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a201cd59-8deb-43b5-a547-172ed6233ffe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent.run(\"Multiply by 2 the population of the capital of Frannce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016567b4-fb8e-4260-836a-8cd8fd47ecf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent.run(\"\"\"Who is the current prime minister of France. \n",
    "Is he or she younger than the President?\"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0345f8ca-a107-4999-ac88-8286c70044a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    # too complex\n",
    "    # either fails because it tries to add dates and nulber\n",
    "    # or give weird results like\n",
    "    # 'Élisabeth Borne will be 70 in the year 2215.'\n",
    "    agent.run(\"\"\"Who is the current prime minister of France. \n",
    "    When will he or she be 70?\"\"\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d0b205-68f8-4ad4-b0be-c47e3107b836",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"background-color:green;color:black;text-align:center;padding:1rem;font-size:1.5rem;\">LANGCHAIN USE CASES</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a119b54e-5cc1-4069-9175-dda6bda8f422",
   "metadata": {},
   "source": [
    "---\n",
    "# [UC] 1. Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8567af48-3fea-45ba-b915-0dc9c50fddfc",
   "metadata": {},
   "source": [
    "---\n",
    "## Summaries Of Short Text\n",
    "Just write a summarization prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfe6ee3-0eed-4af6-8300-7f28b766ed81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# text to be summarized\n",
    "text_sample = \"\"\"\n",
    "The first thing she heard was a general chorus of \"There goes Bill!\"\n",
    "then the Rabbit's voice alone—\"Catch him, you by the hedge!\" Then\n",
    "silence and then another confusion of voices—\"Hold up his head—Brandy\n",
    "now—Don't choke him—What happened to you?\"\n",
    "\n",
    "Last came a little feeble, squeaking voice, \"Well, I hardly know—No\n",
    "more, thank ye. I'm better now—all I know is, something comes at me\n",
    "like a Jack-in-the-box and up I goes like a sky-rocket!\"\n",
    "\n",
    "After a minute or two of silence, they began moving about again, and\n",
    "Alice heard the Rabbit say, \"A barrowful will do, to begin with.\"\n",
    "\n",
    "\"A barrowful of what?\" thought Alice. But she had not long to doubt,\n",
    "for the next moment a shower of little pebbles came rattling in at the\n",
    "window and some of them hit her in the face. Alice noticed, with some\n",
    "surprise, that the pebbles were all turning into little cakes as they\n",
    "lay on the floor and a bright idea came into her head. \"If I eat one of\n",
    "these cakes,\" she thought, \"it's sure to make some< change in my size.\"\n",
    "\n",
    "So she swallowed one of the cakes and was delighted to find that she\n",
    "began shrinking directly. As soon as she was small enough to get through\n",
    "the door, she ran out of the house and found quite a crowd of little\n",
    "animals and birds waiting outside. They all made a rush at Alice the\n",
    "moment she appeared, but she ran off as hard as she could and soon found\n",
    "herself safe in a thick wood.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9646acb1-8cc3-4665-a938-5e6a5fac1d08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Note, the default model is already 'text-davinci-003' \n",
    "# temperature 0 means no randomness\n",
    "llm = OpenAI(temperature=0, model_name='text-davinci-003')\n",
    "\n",
    "# check the number of tokens\n",
    "num_tokens = llm.get_num_tokens(text_sample)\n",
    "print(f\"{num_tokens=}\")\n",
    "\n",
    "# Summarization prompt template\n",
    "template = \"\"\"\n",
    "%INSTRUCTIONS:\n",
    "Please summarize the following piece of text.\n",
    "Respond in a manner that a 5 year old would understand.\n",
    "\n",
    "%TEXT:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "# Create a LangChain prompt template that we can insert values to later\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format(text=text_sample)\n",
    "\n",
    "#print(\"\\nPrompt\")\n",
    "#print(prompt)\n",
    "\n",
    "# run the model\n",
    "output = llm(prompt)\n",
    "\n",
    "print(\"\\nOutput\")\n",
    "print (output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481c1b47-ce62-4d6f-a282-a669747fca0f",
   "metadata": {},
   "source": [
    "---\n",
    "## Summaries of Short text leveraging Summarization Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d45981-0b9c-4ec3-842f-c2a05713441b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Note, the default model is already 'text-davinci-003' \n",
    "# temperature 0 means no randomness\n",
    "llm = OpenAI(temperature=0, model_name='text-davinci-003')\n",
    "\n",
    "# check the number of tokens\n",
    "num_tokens = llm.get_num_tokens(text_sample)\n",
    "print(f\"{num_tokens=}\")\n",
    "\n",
    "# build a document reuse text sampke above\n",
    "doc = Document(\n",
    "    page_content=text_sample,\n",
    "    metadata={\n",
    "        'author':\"Lewis Caroll\",\n",
    "        'title':\"Alice in Wonderland\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# chain expect a list of documents\n",
    "docs = [doc]\n",
    "\n",
    "# the attribute stuff instruct the run the chain once\n",
    "chain = load_summarize_chain(\n",
    "    llm, \n",
    "    chain_type=\"stuff\", \n",
    "    verbose=False)\n",
    "\n",
    "# run the chain against the documment\n",
    "summary = chain.run(docs)\n",
    "    \n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec956f86-893a-495c-afc6-6a37143fd6d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Summaries of Short text leveraging Summarization Chain and custom prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6c1edb-423a-4e89-9551-42eaccfcbdd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Note, the default model is already 'text-davinci-003' \n",
    "# temperature 0 means no randomness\n",
    "llm = OpenAI(temperature=0, model_name='text-davinci-003')\n",
    "\n",
    "# check the number of tokens\n",
    "num_tokens = llm.get_num_tokens(text_sample)\n",
    "print(f\"{num_tokens=}\")\n",
    "\n",
    "# build a document reuse text sampke above\n",
    "doc = Document(\n",
    "    page_content=text_sample,\n",
    "    metadata={\n",
    "        'author':\"Lewis Caroll\",\n",
    "        'title':\"Alice in Wonderland\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# chain expect a list of documents\n",
    "docs = [doc]\n",
    "\n",
    "# setup. a custom prompt\n",
    "# a defaukt one is provide: write a concise summary\n",
    "prompt_template = \"\"\"\n",
    "%INSTRUCTIONS:\n",
    "Please summarize the following piece of text.\n",
    "Respond in a manner that a 5 year old would understand.\n",
    "\n",
    "%TEXT:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "# the attribute stuff instruct the run the chain once\n",
    "chain = load_summarize_chain(\n",
    "    llm, \n",
    "    chain_type=\"stuff\", \n",
    "    prompt=prompt, \n",
    "    verbose=False)\n",
    "\n",
    "# run the chain against the documment\n",
    "summary = chain.run(docs)\n",
    "    \n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee66787c-12f5-4c15-a9fa-78fea9bc7c2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Summaries Of longer Text\n",
    "If the text is longer than the limit in tokens, the text must be splitted in chunks. \n",
    "Langchain components will take care of splitting and chaining the summarization tasks.\n",
    "\n",
    "The Summarization Chain breaks the text into smaller chunks and summarizing each chunk, creating a final summary based on the individual summaries.\n",
    "\n",
    "Check notebook tests-large-dpcuments in the same folder as this notebook.\n",
    "\n",
    "In this example, the chain first splits the essay into chunks of 2000 characters. It then generates summaries for each chunk and creates a final concise summary based on these individual summaries.\n",
    "\n",
    "<br/>\n",
    "**Resources**\n",
    "\n",
    "> - Qummarization quickstart: https://python.langchain.com/docs/modules/chains/popular/summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7349ff71-de9d-46a2-a6fb-22d29458d4fc",
   "metadata": {},
   "source": [
    "---\n",
    "# [UC] 2.  Question & Answering Using Documents As Context\n",
    "Question answering in this context refers to question answering over your document data. F\n",
    "\n",
    "It is basically the example in Indexes.\n",
    "\n",
    "In order to use LLMs for question and answer we must:\n",
    "- Pass the LLM relevant context it needs to answer a question\n",
    "- Pass it our question that we want answered\n",
    "\n",
    "<br/>\n",
    "\n",
    "++Resources**\n",
    "> - [QA] LangChain Question & Answer Docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae034b5-2238-436b-8fa4-33618e0d8f6b",
   "metadata": {},
   "source": [
    "---\n",
    "# [UC] 2.  Question & Answering Using Documents As Context\n",
    "It is basically the example in Indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b21e6ae-1cbb-40b0-baab-95d9dae12a6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import BSHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Annoy\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    " \n",
    "# This is the source document.    \n",
    "document_path = \"data/Alice's Adventures in Wonderland, by Lewis Carroll.html\"\n",
    " \n",
    "# Setup a HTML loader\n",
    "loader = BSHTMLLoader(document_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# Get your splitter ready\n",
    "# in practice they default to 4000 and 200 respectively.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    " \n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(f\"Generated {len(texts)} parts\")\n",
    "\n",
    "# Get embedding engine ready\n",
    "embeddings = OpenAIEmbeddings()\n",
    " \n",
    "# Embedd your texts andd store them in the vector database\n",
    "# dtabase is in memory. it might be savecd to a file and loader later on.\n",
    "db = Annoy.from_documents(texts, embeddings)\n",
    "\n",
    "# Init a retriever for this db\n",
    "#retriever = db.as_retriever()\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":4})\n",
    "\n",
    "# ra query\n",
    "query = \"who is the White Rabbit?\"\n",
    "\n",
    "# retrieve and count indexed documents relevant for the query\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "print(f\"\\nFound {len(docs)} relevant documen(s)\")\n",
    "\n",
    "#samples = \"\\n\\n\".join([x.page_content[:200] for x in docs[:5]])\n",
    "#print(samples)\n",
    "\n",
    "# create a chain to answer questions \n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(), \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    return_source_documents=True)\n",
    "\n",
    "response = qa({\"query\": query})\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f12348f-d1b6-46c4-84d8-8bf320034547",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# using instructions to get a more interesting reponse\n",
    "instructions = \". Give a funny answer 30 words long.\"\n",
    "response = qa({\"query\": query + instructions})\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46668964-0f30-4221-8447-5ef99c56f10f",
   "metadata": {},
   "source": [
    "---\n",
    "### Questions and Answer using a loaded vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c491b0-75c1-4d73-abc1-fb46b5e2fc19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# saving the database for future use\n",
    "\n",
    "docstore_file_path = \"alice_docstore_2\"\n",
    "\n",
    "db.save_local(docstore_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e2276-1c2e-4025-905b-09d2c1532689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loading the database \n",
    "\n",
    "docstore_file_path = \"alice_docstore_2\"\n",
    "\n",
    "loaded_vector_store = Annoy.load_local(\n",
    "   docstore_file_path, embeddings=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# expose this index in a retriever interface\n",
    "retriever = loaded_vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":4})\n",
    "\n",
    "# ra query\n",
    "query = \"who is the White Rabbit?\"\n",
    "\n",
    "# create a chain to answer questions \n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(), \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    return_source_documents=True)\n",
    "\n",
    "instructions = \". Give a pedantic answer 50 words long.\"\n",
    "\n",
    "response = qa({\"query\": query + instructions})\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f0cc91-36bc-4b05-888e-78be46d97415",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# [UC] 3. Extraction\n",
    "\n",
    "Extraction is the process of parsing data from a piece of text. This is commonly used with output parsing in order to structure our data.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**Resources**\n",
    "> - https://python.langchain.com/en/latest/use_cases/extraction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "058884b5-0b5f-436d-a0a5-a855413ebf14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To help construct our Chat Messages\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# We will be using a chat model, defaults to gpt-3.5-turbo\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# To parse outputs and get structured data back\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edc7654-a4d8-451c-bccb-d517d58f88a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## Vanilla Extraction\n",
    "\n",
    "Let's start off withan easy example. Here I simply supply a prompt with instructions with the type of output I want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fbaf178e-3449-4e75-9df5-a570e941e862",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Apple\": \"🍎\",\n",
      "  \"Pear\": \"🍐\",\n",
      "  \"kiwi\": \"🥝\"\n",
      "}\n",
      "<class 'str'>\n",
      "{'Apple': '🍎', 'Pear': '🍐', 'kiwi': '🥝'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "instructions = \"\"\"\n",
    "You will be given a sentence with fruit names, extract those fruit names and assign an emoji to them\n",
    "Return the fruit name and emojis in a python dictionary\n",
    "\"\"\"\n",
    "\n",
    "fruit_names = \"\"\"\n",
    "Apple, Pear, this is an kiwi\n",
    "\"\"\"\n",
    "\n",
    "# Make your prompt which combines the instructions w/ the fruit names\n",
    "prompt = (instructions + fruit_names)\n",
    "\n",
    "# Call the LLM\n",
    "output = chat_model([HumanMessage(content=prompt)])\n",
    "\n",
    "print (output.content)\n",
    "print (type(output.content))\n",
    "\n",
    "#Let's turn this into a proper python dictionary\n",
    "\n",
    "output_dict = eval(output.content)\n",
    "\n",
    "print (output_dict)\n",
    "print (type(output_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f056ff8-c389-4ccd-b17a-4b36ea9e6f09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "---\n",
    "## Using LangChain's Response Schema\n",
    "\n",
    "LangChain's response schema will does two things for us:\n",
    "- Autogenerate the a prompt with bonafide format instructions. This is great because I don't need to worry about the prompt engineering side, I'll leave that up to LangChain!\n",
    "- Read the output from the LLM and turn it into a proper python object for me\n",
    "\n",
    "Here I define the schema I want. I'm going to pull out the song and artist that a user wants to play from a pseudo chat message.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09bdc39e-10cd-49a5-b49f-0beba24c33af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**FORMAT**\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"artist\": string  // The name of the musical artist\n",
      "\t\"song\": string  // The name of the song that the artist plays\n",
      "}\n",
      "```\n",
      "*ı*QUERY**\n",
      "\n",
      "Given a command from the user, extract the artist and song names\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"artist\": string  // The name of the musical artist\n",
      "\t\"song\": string  // The name of the song that the artist plays\n",
      "}\n",
      "```\n",
      "\n",
      "I really like So Young by Portugal. The Man\n",
      "\n",
      "\n",
      "**RESPONSE*\n",
      "{'artist': 'Portugal. The Man', 'song': 'So Young'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# The schema I want out\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"artist\", description=\"The name of the musical artist\"),\n",
    "    ResponseSchema(name=\"song\", description=\"The name of the song that the artist plays\")\n",
    "]\n",
    "\n",
    "# The parser that will look for the LLM output in my schema and return it back to me\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "# The format instructions that LangChain makes. Let's look at them\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(\"\\n**FORMAT**\")\n",
    "print(format_instructions)\n",
    "\n",
    "#The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
    "# rxample\n",
    "#{\n",
    "#\t\"artist\": string  // The name of the musical artist\n",
    "#\t\"song\": string  // The name of the song that the artist plays\n",
    "#}\n",
    "\n",
    "# The prompt template that brings it all together\n",
    "# Note: This is a different prompt template than before because we are using a Chat Model\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Given a command from the user, extract the artist and song names\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "{user_prompt}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate.from_template(prompt_template)  \n",
    "    ],\n",
    "    input_variables=[\"user_prompt\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "chat_query = prompt.format_prompt(\n",
    "    user_prompt=\"I really like So Young by Portugal. The Man\"\n",
    ")\n",
    "print(\"*ı*QUERY**\")\n",
    "print (chat_query.messages[0].content)\n",
    "\n",
    "# Given a command from the user, extract the artist and song names \n",
    "# The output should be a markdown code snippet formatted in the following schema, \n",
    "#including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
    "## ```json\n",
    "# {\n",
    "# \t\"artist\": string  // The name of the musical artist\n",
    "# \t\"song\": string  // The name of the song that the artist plays\n",
    "#}\n",
    "#```\n",
    "\n",
    "chat_output = chat_model(chat_query.to_messages())\n",
    "response = output_parser.parse(chat_output.content)\n",
    "\n",
    "print(\"\\n**RESPONSE*\")\n",
    "print (response)\n",
    "print (type(response))\n",
    "\n",
    "# example\n",
    "#{'artist': 'Portugal. The Man', 'song': 'So Young'}\n",
    "\n",
    "#Warning: The parser looks for an output from the LLM in a specific format. \n",
    "##our model may not output the same format every time. \n",
    "#Make sure to handle errors with this one. GPT4 and future iterations will be more reliable.\n",
    "\n",
    "# For more advanced parsing check out Kor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c460f2d-e64e-4aab-8da7-0e265676719f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'artist': 'Porcupine Tree', 'song': 'Sound of Muzak'}\n"
     ]
    }
   ],
   "source": [
    "chat_query = prompt.format_prompt(\n",
    "    user_prompt=\"I would like to listen Sound of Muzak by Porcupine Tree\"\n",
    ")\n",
    "chat_output = chat_model(chat_query.to_messages())\n",
    "response = output_parser.parse(chat_output.content)\n",
    "\n",
    "print (response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9a24ae-02a0-440f-9b22-4f2fac69a7ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9de8da-1014-4a22-8e6b-f39bec5c020c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO Kor\n",
    "</div>\n",
    "\n",
    "Kor\n",
    "\n",
    "This is a half-baked prototype that “helps” you extract structured data from text using LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b134a99a-0fee-4dc4-a367-785287b44f04",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# [UC] 4. Evaluation\n",
    "\n",
    "Evaluation is the process of doing quality checks on the output of your applications. \n",
    "\n",
    "Normal, deterministic, code has tests we can run, but judging the output of LLMs is more difficult \n",
    "because of the unpredictableness and variability of natural language. \n",
    "\n",
    "LangChain provides tools that aid us in this journey.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Resources**\n",
    "> - https://python.langchain.com/en/latest/use_cases/evaluation.html\n",
    "> - https://docs.langchain.com/docs/use-cases/evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "154bc185-fe2d-4728-a59c-7c128fc47a26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 document\n",
      "You have 72216 characters in that document\n"
     ]
    }
   ],
   "source": [
    "# Embeddings, store, and retrieval\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Model and doc loader\n",
    "from langchain import OpenAI\n",
    "from langchain.document_loaders import BSHTMLLoader\n",
    "\n",
    "# Eval!\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Our long essay from before\n",
    "# This is the source document.    \n",
    "document_path = \"data/Alice's Adventures in Wonderland, by Lewis Carroll.html\"\n",
    "\n",
    "loader = BSHTMLLoader(document_path)\n",
    "doc = loader.load()\n",
    "\n",
    "print (f\"You have {len(doc)} document\")\n",
    "print (f\"You have {len(doc[0].page_content)} characters in that document\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c145e7d2-f958-49cf-b3f3-3bb88da0f6e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 35 documents that have an average of 2,179 characters\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=400)\n",
    "docs = text_splitter.split_documents(doc)\n",
    "\n",
    "# Get the total number of characters so we can see the average later\n",
    "num_total_characters = sum([len(x.page_content) for x in docs])\n",
    "\n",
    "print (f\"Now you have {len(docs)} documents that have an average of {num_total_characters / len(docs):,.0f} characters\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fe1316f-ecea-41dd-96ea-3d0aebc7744b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Annoy\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Get embedding engine ready\n",
    "embeddings = OpenAIEmbeddings()\n",
    " \n",
    "# Embedd your texts andd store them in the vector database\n",
    "# dtabase is in memory. it might be savecd to a file and loader later on.\n",
    "db = Annoy.from_documents(docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c78d03f-2f44-474f-b67a-a04ea97d5a0f",
   "metadata": {},
   "source": [
    "Make your retrieval chain. Notice how I have an input_key parameter now. This tells the chain which key from a dictionary I supply has my prompt/query in it. I specify question to match the question in the dict below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da8aaecf-7b6c-4f65-9dfe-3baef261d540",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", \n",
    "                                    retriever=db.as_retriever(), \n",
    "                                    input_key=\"question\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d38d1c1-7c75-4702-8075-51f69b2957b2",
   "metadata": {},
   "source": [
    "Now pass a list of questions and ground truth answers to the LLM that I know are correct \n",
    "(I validated them as a human)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4ed47c1-0ea4-4db3-ba15-527a9332ce77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question_answers = [\n",
    "    {'question' : \"Who is the protagonist?\", \n",
    "     'answer' : 'Alice'},\n",
    "    {'question' : \"Who is Gutenberg in the book?\",\n",
    "     'answer' : 'The name of the project'}, \n",
    "    {'question' : \"How many characters are they?\",\n",
    "     'answer' : '50'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896f746f-e544-4959-8f78-5b40c045941f",
   "metadata": {},
   "source": [
    "chain.apply runs questions one by one separately.\n",
    "\n",
    "It gets back another key in the dictionary result which will be the output from the LLM.\n",
    "\n",
    "Note:  3rd question is ambigious and tough to answer in one pass so the LLM would get it incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "564bec6b-64f6-4611-bdae-fa5bac5b9104",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'answer': 'Alice',\n",
      "  'question': 'Who is the protagonist?',\n",
      "  'result': ' Alice is the protagonist.'},\n",
      " {'answer': 'The name of the project',\n",
      "  'question': 'Who is Gutenberg in the book?',\n",
      "  'result': \" Gutenberg is not a character in the book Alice's Adventures in \"\n",
      "            'Wonderland. It is the name of the organization that produced the '\n",
      "            'ebook version of the book.'},\n",
      " {'answer': '50',\n",
      "  'question': 'How many characters are they?',\n",
      "  'result': ' There are six characters mentioned in the context: Alice, the '\n",
      "            'Caterpillar, the Queen, the Cat, the King, and the White Rabbit.'}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "predictions = chain.apply(question_answers)\n",
    "pprint(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060c6380-8a58-45d5-84da-b8d09afb51a7",
   "metadata": {},
   "source": [
    "**OUTPUT%**\n",
    "\n",
    "[{'answer': 'Alice',\n",
    "  'question': 'Who is the protagonist?',\n",
    "  'result': ' Alice is the protagonist.'},\n",
    " {'answer': 'The name of the project',\n",
    "  'question': 'Who is Gutenberg in the book?',\n",
    "  'result': \" Gutenberg is not a character in the book Alice's Adventures in \"\n",
    "            'Wonderland. It is the name of the organization that produced the '\n",
    "            'ebook version of the book.'},\n",
    " {'answer': '50',\n",
    "  'question': 'How many characters are they?',\n",
    "  'result': ' There are six characters mentioned in the context: Alice, the '\n",
    "            'Caterpillar, the Queen, the Cat, the King, and the White Rabbit.'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "609f2021-09f5-41bb-ba37-c905075714b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': ' CORRECT'}, {'text': ' CORRECT'}, {'text': ' INCORRECT'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start your eval chain\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "\n",
    "# Have it grade itself. The code below helps the eval_chain know where the different parts are\n",
    "graded_outputs = eval_chain.evaluate(question_answers,\n",
    "                                     predictions,\n",
    "                                     question_key=\"question\",\n",
    "                                     prediction_key=\"result\",\n",
    "                                     answer_key='answer')\n",
    "graded_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb1aaa7-17bc-4759-9caa-662e29a93733",
   "metadata": {},
   "source": [
    "**OUTPUT**\n",
    "\n",
    "[{'text': ' CORRECT'}, {'text': ' CORRECT'}, {'text': ' INCORRECT'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd21fcf4-3db9-4ebd-875e-2177ea6ebdc6",
   "metadata": {},
   "source": [
    "---\n",
    "# [UC] 5. Querying Tabular Data\n",
    "\n",
    "The most common type of data in the world sits in tabular form (ok, ok, besides unstructured data). It is super powerful to be able to query this data with LangChain and pass it through to an LLM\n",
    "\n",
    "Steps:\n",
    "\n",
    "- Find which table to use\n",
    "- Find which column to use\n",
    "- Construct the correct sql query\n",
    "- Execute that query\n",
    "- Get the result\n",
    "- Return a natural language reponse back\n",
    "\n",
    "For futher reading check out \"Agents + Tabular Data\" (Pandas, SQL, CSV)\n",
    "\n",
    "**Resources**\n",
    "> - https://python.langchain.com/en/latest/use_cases/tabular.html\n",
    "> - https://python.langchain.com/docs/modules/chains/popular/sqlite.html\n",
    " \n",
    "<div class=\"alert alert-block alert-warning\"> TODO move csv and db to subfolders\n",
    "    \n",
    "SQL Agent, \n",
    "Pandas Agent, \n",
    "CSV Agent\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575f6524-1c1a-4669-b5f9-12a444805ab7",
   "metadata": {},
   "source": [
    "Sample datasets\n",
    "\n",
    "https://scikit-learn.org/stable/datasets/toy_dataset.html\n",
    "\n",
    "https://github.com/mwaskom/seaborn-data\n",
    "\n",
    "https://www.kaggle.com/datasets/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83294479-b0cd-4aac-9771-7c6e55a7de2c",
   "metadata": {},
   "source": [
    "## The movies dataset\n",
    "\n",
    "IMDB Movies dataset from Kaggle\n",
    "> - https://www.kaggle.com/datasets/harshitshankhdhar/imdb-dataset-of-top-1000-movies-and-tv-shows\n",
    "\n",
    "**Resources**\n",
    "> - https://www.kaggle.com/docs/datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d10224-bff4-48fa-bc0c-c2b9adfea97c",
   "metadata": {},
   "source": [
    "## Download the dataset\n",
    "\n",
    "Requires kaggle is installed and api keys are setup. Check the first part of the notebbok if need be.\n",
    "\n",
    "**Resources**\n",
    "> - https://lindevs.com/set-up-kaggle-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "18306183-6737-4686-b9a0-251262dc3607",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading imdb-dataset-of-top-1000-movies-and-tv-shows.zip to data\n",
      "100%|█████████████████████████████████████████| 175k/175k [00:00<00:00, 995kB/s]\n",
      "100%|█████████████████████████████████████████| 175k/175k [00:00<00:00, 861kB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d harshitshankhdhar/imdb-dataset-of-top-1000-movies-and-tv-shows -p data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f2a6c291-d189-4cca-8941-df942663feea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data/imdb-dataset-of-top-1000-movies-and-tv-shows.zip\n",
      "  inflating: data/imdb_top_1000.csv  \n"
     ]
    }
   ],
   "source": [
    "!unzip data/imdb-dataset-of-top-1000-movies-and-tv-shows.zip -d data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e09705c-c70c-4730-9033-c286e8d8f029",
   "metadata": {},
   "source": [
    "## SQL QUerying \n",
    "\n",
    "check the dedicated notebook tests-sql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763a4831-795f-4e44-a9a3-26ca58b778e9",
   "metadata": {},
   "source": [
    "---\n",
    "# [UC] 6. Code Understanding\n",
    "\n",
    "A big part of this is having a LLM that can understand code and help you with a particular task.\n",
    "\n",
    "**Resources**\n",
    "> - https://python.langchain.com/en/latest/use_cases/code.html\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f745d57-b34d-4f9d-9fd7-ac36ede8f660",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper to read local files\n",
    "import os\n",
    "\n",
    "# Vector Support\n",
    "from langchain.vectorstores import Annoy\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# Model and chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Text splitters\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo')\n",
    "\n",
    "# create the vector store\n",
    "embeddings = OpenAIEmbeddings(disallowed_special=())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c7c067-a671-4ee0-83e4-a4c99cecd0c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!curl https://github.com/seatgeek/thefuzz/archive/refs/heads/master.zip -o data/thefuzz.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4d76c0-c749-424d-bdce-48ac28e59b6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!unzip data/thefuzz.zip -d data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5a85b2-e8b3-4b90-8595-ff301b02e75a",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO download file\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4057712f-c077-4c39-af51-aa6a939007c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!unzip data/thefuzz-master.zip -d data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9096e6d-2bb4-4f87-826c-3d5c3579528f",
   "metadata": {},
   "source": [
    "## Load all files into a document store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "56b53a8a-65be-48ce-a72b-730db8aead3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_dir = 'data/thefuzz-master'\n",
    "docs = []\n",
    "\n",
    "# Go through each folder\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    \n",
    "    # Go through each file\n",
    "    for file in filenames:\n",
    "        try: \n",
    "            # Load up the file as a doc and split\n",
    "            loader = TextLoader(os.path.join(dirpath, file), encoding='utf-8')\n",
    "            docs.extend(loader.load_and_split())\n",
    "        except Exception as e: \n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f34f8f31-599c-4dc1-b1b4-e8e3756bd435",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 175 documents\n",
      "\n",
      "------ Start Document ------\n",
      "Changelog\n",
      "=========\n",
      "\n",
      "0.17.0 (2018-08-20)\n",
      "-------------------\n",
      "\n",
      "- Make benchmarks script Py3 compatible. [Stefan Behnel]\n",
      "\n",
      "- Add Go lang port. [iddober]\n",
      "\n",
      "- Add reference to C# port. [ericcoleman]\n",
      "\n",
      "- Chore: remove license header from files. [Jose Diaz-Gonzalez]\n",
      "\n",
      "  The files should all inherit the projec\n"
     ]
    }
   ],
   "source": [
    "print (f\"You have {len(docs)} documents\\n\")\n",
    "print (\"------ Start Document ------\")\n",
    "print (docs[0].page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b568e65e-2b62-4455-a804-5fa7d24ce790",
   "metadata": {},
   "source": [
    "Embed and store them in a docstore. This will make an API call to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "19376480-7c7c-44e8-9845-2d793c432950",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "docsearch = Annoy.from_documents(docs, embeddings)\n",
    "\n",
    "# Get our retriever ready\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374d8f78-8c07-4ad8-8a4d-f50e0a2164bb",
   "metadata": {},
   "source": [
    "## Query the doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "920643ce-86f3-4445-b837-1dd1cb40e585",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can use the `process.extractOne()` function from the `thefuzz` library to find the most similar item in a list of items. It takes a query string and a list of choices, and returns the best match along with its similarity score. Here's an example:\n",
      "\n",
      "```python\n",
      "from thefuzz import process\n",
      "\n",
      "choices = [\"apple\", \"banana\", \"cherry\", \"durian\"]\n",
      "query = \"berry\"\n",
      "\n",
      "best_match = process.extractOne(query, choices)\n",
      "print(best_match)\n",
      "```\n",
      "\n",
      "Output:\n",
      "```\n",
      "('cherry', 62)\n",
      "```\n",
      "\n",
      "In this example, the best match for the query \"berry\" in the list of choices is \"cherry\" with a similarity score of 62.\n"
     ]
    }
   ],
   "source": [
    "query = \"What function do I use if I want to find the most similar item in a list of items?\"\n",
    "output = qa.run(query)\n",
    "print (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e73deb27-d786-4c26-b709-aded5b09a304",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query = \"new york mets at chicago cubs\"\n",
      "choices = [\n",
      "    None,\n",
      "    \"new york mets vs chicago cubs\",\n",
      "    \"new york yankees vs boston red sox\",\n",
      "    None,\n",
      "    None\n",
      "]\n",
      "\n",
      "best = process.extractOne(query, choices)\n",
      "print(best[0])\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you write the code to use the process.extractOne() function? Only respond with code. No other text or explanation\"\n",
    "output = qa.run(query)\n",
    "print (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80097a3-7975-46e2-b789-0a25428fb48e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0fd5334-83a9-4eff-ba2f-46dbe1a2022c",
   "metadata": {},
   "source": [
    "---\n",
    "# [UC] 7. Interacting with APIs\n",
    "\n",
    "Very simple example to demonstrate how ot works.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Resources**\n",
    ">- https://python.langchain.com/en/latest/use_cases/apis.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "104f3fc9-a9d2-465c-b052-83f77581d74f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import APIChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741b05ef-a35f-4d7e-a6ce-bad34054032e",
   "metadata": {},
   "source": [
    "LangChain's APIChain has the ability to read API documentation and understand which endpoint it needs to call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "580e645d-8b40-4fc1-ae6b-b56eb93939a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# API documentation to be used\n",
    "\n",
    "api_docs = \"\"\"\n",
    "\n",
    "BASE URL: https://restcountries.com/\n",
    "\n",
    "API Documentation:\n",
    "\n",
    "The API endpoint /v3.1/name/{name} Used to find informatin about a country. All URL parameters are listed below:\n",
    "    - name: Name of country - Ex: italy, france\n",
    "    \n",
    "The API endpoint /v3.1/currency/{currency} Uesd to find information about a region. All URL parameters are listed below:\n",
    "    - currency: 3 letter currency. Example: USD, COP\n",
    "    \n",
    "Woo! This is my documentation\n",
    "\"\"\"\n",
    "\n",
    "chain_new = APIChain.from_llm_and_api_docs(llm, api_docs, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "463eba9b-152b-446d-a1d9-d08b7952f99f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m https://restcountries.com/v3.1/name/france\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m[{\"name\":{\"common\":\"France\",\"official\":\"French Republic\",\"nativeName\":{\"fra\":{\"official\":\"République française\",\"common\":\"France\"}}},\"tld\":[\".fr\"],\"cca2\":\"FR\",\"ccn3\":\"250\",\"cca3\":\"FRA\",\"cioc\":\"FRA\",\"independent\":true,\"status\":\"officially-assigned\",\"unMember\":true,\"currencies\":{\"EUR\":{\"name\":\"Euro\",\"symbol\":\"€\"}},\"idd\":{\"root\":\"+3\",\"suffixes\":[\"3\"]},\"capital\":[\"Paris\"],\"altSpellings\":[\"FR\",\"French Republic\",\"République française\"],\"region\":\"Europe\",\"subregion\":\"Western Europe\",\"languages\":{\"fra\":\"French\"},\"translations\":{\"ara\":{\"official\":\"الجمهورية الفرنسية\",\"common\":\"فرنسا\"},\"bre\":{\"official\":\"Republik Frañs\",\"common\":\"Frañs\"},\"ces\":{\"official\":\"Francouzská republika\",\"common\":\"Francie\"},\"cym\":{\"official\":\"French Republic\",\"common\":\"France\"},\"deu\":{\"official\":\"Französische Republik\",\"common\":\"Frankreich\"},\"est\":{\"official\":\"Prantsuse Vabariik\",\"common\":\"Prantsusmaa\"},\"fin\":{\"official\":\"Ranskan tasavalta\",\"common\":\"Ranska\"},\"fra\":{\"official\":\"République française\",\"common\":\"France\"},\"hrv\":{\"official\":\"Francuska Republika\",\"common\":\"Francuska\"},\"hun\":{\"official\":\"Francia Köztársaság\",\"common\":\"Franciaország\"},\"ita\":{\"official\":\"Repubblica francese\",\"common\":\"Francia\"},\"jpn\":{\"official\":\"フランス共和国\",\"common\":\"フランス\"},\"kor\":{\"official\":\"프랑스 공화국\",\"common\":\"프랑스\"},\"nld\":{\"official\":\"Franse Republiek\",\"common\":\"Frankrijk\"},\"per\":{\"official\":\"جمهوری فرانسه\",\"common\":\"فرانسه\"},\"pol\":{\"official\":\"Republika Francuska\",\"common\":\"Francja\"},\"por\":{\"official\":\"República Francesa\",\"common\":\"França\"},\"rus\":{\"official\":\"Французская Республика\",\"common\":\"Франция\"},\"slk\":{\"official\":\"Francúzska republika\",\"common\":\"Francúzsko\"},\"spa\":{\"official\":\"República francés\",\"common\":\"Francia\"},\"srp\":{\"official\":\"Француска Република\",\"common\":\"Француска\"},\"swe\":{\"official\":\"Republiken Frankrike\",\"common\":\"Frankrike\"},\"tur\":{\"official\":\"Fransa Cumhuriyeti\",\"common\":\"Fransa\"},\"urd\":{\"official\":\"جمہوریہ فرانس\",\"common\":\"فرانس\"},\"zho\":{\"official\":\"法兰西共和国\",\"common\":\"法国\"}},\"latlng\":[46.0,2.0],\"landlocked\":false,\"borders\":[\"AND\",\"BEL\",\"DEU\",\"ITA\",\"LUX\",\"MCO\",\"ESP\",\"CHE\"],\"area\":551695.0,\"demonyms\":{\"eng\":{\"f\":\"French\",\"m\":\"French\"},\"fra\":{\"f\":\"Française\",\"m\":\"Français\"}},\"flag\":\"\\uD83C\\uDDEB\\uD83C\\uDDF7\",\"maps\":{\"googleMaps\":\"https://goo.gl/maps/g7QxxSFsWyTPKuzd7\",\"openStreetMaps\":\"https://www.openstreetmap.org/relation/1403916\"},\"population\":67391582,\"gini\":{\"2018\":32.4},\"fifa\":\"FRA\",\"car\":{\"signs\":[\"F\"],\"side\":\"right\"},\"timezones\":[\"UTC-10:00\",\"UTC-09:30\",\"UTC-09:00\",\"UTC-08:00\",\"UTC-04:00\",\"UTC-03:00\",\"UTC+01:00\",\"UTC+02:00\",\"UTC+03:00\",\"UTC+04:00\",\"UTC+05:00\",\"UTC+10:00\",\"UTC+11:00\",\"UTC+12:00\"],\"continents\":[\"Europe\"],\"flags\":{\"png\":\"https://flagcdn.com/w320/fr.png\",\"svg\":\"https://flagcdn.com/fr.svg\",\"alt\":\"The flag of France is composed of three equal vertical bands of blue, white and red.\"},\"coatOfArms\":{\"png\":\"https://mainfacts.com/media/images/coats_of_arms/fr.png\",\"svg\":\"https://mainfacts.com/media/images/coats_of_arms/fr.svg\"},\"startOfWeek\":\"monday\",\"capitalInfo\":{\"latlng\":[48.87,2.33]},\"postalCode\":{\"format\":\"#####\",\"regex\":\"^(\\\\d{5})$\"}}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' France is an officially-assigned, independent country located in Western Europe. Its capital is Paris and its official language is French. Its currency is the Euro (€). It has a population of 67,391,582 and its borders are shared with Andorra, Belgium, Germany, Italy, Luxembourg, Monaco, Spain, and Switzerland.'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try to use the API meant for the country endpoint\n",
    "\n",
    "chain_new.run('Can you tell me information about France?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cda5e946-6890-412b-9bfa-6f44976956b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m https://restcountries.com/v3.1/currency/COP\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m[{\"name\":{\"common\":\"Colombia\",\"official\":\"Republic of Colombia\",\"nativeName\":{\"spa\":{\"official\":\"República de Colombia\",\"common\":\"Colombia\"}}},\"tld\":[\".co\"],\"cca2\":\"CO\",\"ccn3\":\"170\",\"cca3\":\"COL\",\"cioc\":\"COL\",\"independent\":true,\"status\":\"officially-assigned\",\"unMember\":true,\"currencies\":{\"COP\":{\"name\":\"Colombian peso\",\"symbol\":\"$\"}},\"idd\":{\"root\":\"+5\",\"suffixes\":[\"7\"]},\"capital\":[\"Bogotá\"],\"altSpellings\":[\"CO\",\"Republic of Colombia\",\"República de Colombia\"],\"region\":\"Americas\",\"subregion\":\"South America\",\"languages\":{\"spa\":\"Spanish\"},\"translations\":{\"ara\":{\"official\":\"جمهورية كولومبيا\",\"common\":\"كولومبيا\"},\"bre\":{\"official\":\"Republik Kolombia\",\"common\":\"Kolombia\"},\"ces\":{\"official\":\"Kolumbijská republika\",\"common\":\"Kolumbie\"},\"cym\":{\"official\":\"Gweriniaeth Colombia\",\"common\":\"Colombia\"},\"deu\":{\"official\":\"Republik Kolumbien\",\"common\":\"Kolumbien\"},\"est\":{\"official\":\"Colombia Vabariik\",\"common\":\"Colombia\"},\"fin\":{\"official\":\"Kolumbian tasavalta\",\"common\":\"Kolumbia\"},\"fra\":{\"official\":\"République de Colombie\",\"common\":\"Colombie\"},\"hrv\":{\"official\":\"Republika Kolumbija\",\"common\":\"Kolumbija\"},\"hun\":{\"official\":\"Kolumbiai Köztársaság\",\"common\":\"Kolumbia\"},\"ita\":{\"official\":\"Repubblica di Colombia\",\"common\":\"Colombia\"},\"jpn\":{\"official\":\"コロンビア共和国\",\"common\":\"コロンビア\"},\"kor\":{\"official\":\"콜롬비아 공화국\",\"common\":\"콜롬비아\"},\"nld\":{\"official\":\"Republiek Colombia\",\"common\":\"Colombia\"},\"per\":{\"official\":\"جمهوری کلمبیا\",\"common\":\"کلمبیا\"},\"pol\":{\"official\":\"Republika Kolumbii\",\"common\":\"Kolumbia\"},\"por\":{\"official\":\"República da Colômbia\",\"common\":\"Colômbia\"},\"rus\":{\"official\":\"Республика Колумбия\",\"common\":\"Колумбия\"},\"slk\":{\"official\":\"Kolumbijská republika\",\"common\":\"Kolumbia\"},\"spa\":{\"official\":\"República de Colombia\",\"common\":\"Colombia\"},\"srp\":{\"official\":\"Република Колумбија\",\"common\":\"Колумбија\"},\"swe\":{\"official\":\"Republiken Colombia\",\"common\":\"Colombia\"},\"tur\":{\"official\":\"Kolombiya Cumhuriyeti\",\"common\":\"Kolombiya\"},\"urd\":{\"official\":\"جمہوریہ کولمبیا\",\"common\":\"کولمبیا\"},\"zho\":{\"official\":\"哥伦比亚共和国\",\"common\":\"哥伦比亚\"}},\"latlng\":[4.0,-72.0],\"landlocked\":false,\"borders\":[\"BRA\",\"ECU\",\"PAN\",\"PER\",\"VEN\"],\"area\":1141748.0,\"demonyms\":{\"eng\":{\"f\":\"Colombian\",\"m\":\"Colombian\"},\"fra\":{\"f\":\"Colombienne\",\"m\":\"Colombien\"}},\"flag\":\"\\uD83C\\uDDE8\\uD83C\\uDDF4\",\"maps\":{\"googleMaps\":\"https://goo.gl/maps/RdwTG8e7gPwS62oR6\",\"openStreetMaps\":\"https://www.openstreetmap.org/relation/120027\"},\"population\":50882884,\"gini\":{\"2019\":51.3},\"fifa\":\"COL\",\"car\":{\"signs\":[\"CO\"],\"side\":\"right\"},\"timezones\":[\"UTC-05:00\"],\"continents\":[\"South America\"],\"flags\":{\"png\":\"https://flagcdn.com/w320/co.png\",\"svg\":\"https://flagcdn.com/co.svg\",\"alt\":\"The flag of Colombia is composed of three horizontal bands of yellow, blue and red, with the yellow band twice the height of the other two bands.\"},\"coatOfArms\":{\"png\":\"https://mainfacts.com/media/images/coats_of_arms/co.png\",\"svg\":\"https://mainfacts.com/media/images/coats_of_arms/co.svg\"},\"startOfWeek\":\"monday\",\"capitalInfo\":{\"latlng\":[4.71,-74.07]}}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The currency of Colombia is the Colombian peso (COP), symbolized by the \"$\" sign.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_new.run('Can you tell me about the currency COP?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b5c8473-d6c4-4530-92df-970e3c902d2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m https://restcountries.com/v3.1/name/norway\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m[{\"name\":{\"common\":\"Norway\",\"official\":\"Kingdom of Norway\",\"nativeName\":{\"nno\":{\"official\":\"Kongeriket Noreg\",\"common\":\"Noreg\"},\"nob\":{\"official\":\"Kongeriket Norge\",\"common\":\"Norge\"},\"smi\":{\"official\":\"Norgga gonagasriika\",\"common\":\"Norgga\"}}},\"tld\":[\".no\"],\"cca2\":\"NO\",\"ccn3\":\"578\",\"cca3\":\"NOR\",\"cioc\":\"NOR\",\"independent\":true,\"status\":\"officially-assigned\",\"unMember\":true,\"currencies\":{\"NOK\":{\"name\":\"Norwegian krone\",\"symbol\":\"kr\"}},\"idd\":{\"root\":\"+4\",\"suffixes\":[\"7\"]},\"capital\":[\"Oslo\"],\"altSpellings\":[\"NO\",\"Norge\",\"Noreg\",\"Kingdom of Norway\",\"Kongeriket Norge\",\"Kongeriket Noreg\"],\"region\":\"Europe\",\"subregion\":\"Northern Europe\",\"languages\":{\"nno\":\"Norwegian Nynorsk\",\"nob\":\"Norwegian Bokmål\",\"smi\":\"Sami\"},\"translations\":{\"ara\":{\"official\":\"مملكة النرويج\",\"common\":\"النرويج\"},\"bre\":{\"official\":\"Rouantelezh Norvegia\",\"common\":\"Norvegia\"},\"ces\":{\"official\":\"Norské království\",\"common\":\"Norsko\"},\"cym\":{\"official\":\"Kingdom of Norway\",\"common\":\"Norway\"},\"deu\":{\"official\":\"Königreich Norwegen\",\"common\":\"Norwegen\"},\"est\":{\"official\":\"Norra Kuningriik\",\"common\":\"Norra\"},\"fin\":{\"official\":\"Norjan kuningaskunta\",\"common\":\"Norja\"},\"fra\":{\"official\":\"Royaume de Norvège\",\"common\":\"Norvège\"},\"hrv\":{\"official\":\"Kraljevina Norveška\",\"common\":\"Norveška\"},\"hun\":{\"official\":\"Norvég Királyság\",\"common\":\"Norvégia\"},\"ita\":{\"official\":\"Regno di Norvegia\",\"common\":\"Norvegia\"},\"jpn\":{\"official\":\"ノルウェー王国\",\"common\":\"ノルウェー\"},\"kor\":{\"official\":\"노르웨이 왕국\",\"common\":\"노르웨이\"},\"nld\":{\"official\":\"Koninkrijk Noorwegen\",\"common\":\"Noorwegen\"},\"per\":{\"official\":\"پادشاهی نروژ\",\"common\":\"نروژ\"},\"pol\":{\"official\":\"Królestwo Norwegii\",\"common\":\"Norwegia\"},\"por\":{\"official\":\"Reino da Noruega\",\"common\":\"Noruega\"},\"rus\":{\"official\":\"Королевство Норвегия\",\"common\":\"Норвегия\"},\"slk\":{\"official\":\"Nórske kráľovstvo\",\"common\":\"Nórsko\"},\"spa\":{\"official\":\"Reino de Noruega\",\"common\":\"Noruega\"},\"srp\":{\"official\":\"Краљевина Норвешка\",\"common\":\"Норвешка\"},\"swe\":{\"official\":\"Konungariket Norge\",\"common\":\"Norge\"},\"tur\":{\"official\":\"Norveç Krallığı\",\"common\":\"Norveç\"},\"urd\":{\"official\":\"مملکتِ ناروے\",\"common\":\"ناروے\"},\"zho\":{\"official\":\"挪威王国\",\"common\":\"挪威\"}},\"latlng\":[62.0,10.0],\"landlocked\":false,\"borders\":[\"FIN\",\"SWE\",\"RUS\"],\"area\":323802.0,\"demonyms\":{\"eng\":{\"f\":\"Norwegian\",\"m\":\"Norwegian\"},\"fra\":{\"f\":\"Norvégienne\",\"m\":\"Norvégien\"}},\"flag\":\"\\uD83C\\uDDF3\\uD83C\\uDDF4\",\"maps\":{\"googleMaps\":\"https://goo.gl/maps/htWRrphA7vNgQNdSA\",\"openStreetMaps\":\"https://www.openstreetmap.org/relation/2978650\"},\"population\":5379475,\"gini\":{\"2018\":27.6},\"fifa\":\"NOR\",\"car\":{\"signs\":[\"N\"],\"side\":\"right\"},\"timezones\":[\"UTC+01:00\"],\"continents\":[\"Europe\"],\"flags\":{\"png\":\"https://flagcdn.com/w320/no.png\",\"svg\":\"https://flagcdn.com/no.svg\",\"alt\":\"The flag of Norway has a red field with a large white-edged navy blue cross that extends to the edges of the field. The vertical part of this cross is offset towards the hoist side.\"},\"coatOfArms\":{\"png\":\"https://mainfacts.com/media/images/coats_of_arms/no.png\",\"svg\":\"https://mainfacts.com/media/images/coats_of_arms/no.svg\"},\"startOfWeek\":\"monday\",\"capitalInfo\":{\"latlng\":[59.92,10.75]},\"postalCode\":{\"format\":\"####\",\"regex\":\"^(\\\\d{4})$\"}}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Norway is an officially-assigned, independent country located in Northern Europe. Its capital is Oslo and its official currency is the Norwegian krone (NOK). It has a population of 5,379,475 and its official languages are Norwegian Nynorsk, Norwegian Bokmål, and Sami.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test a country not listed in the documentation\n",
    "\n",
    "chain_new.run('Can you tell me information about Norway?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53cff33e-5f6e-4264-9ee2-484a51c535c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m https://restcountries.com/v3.1/name/france?fields=name;translations\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m[{\"name\":{\"common\":\"France\",\"official\":\"French Republic\",\"nativeName\":{\"fra\":{\"official\":\"République française\",\"common\":\"France\"}}}}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The name of France in Swedish is \"Frankrike\".'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test tretrieval of some specifc attribute\n",
    "# seems that it just translates afterwards\n",
    "\n",
    "chain_new.run('Can you tell me the name of France in swedish?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25843465-50e5-480b-82e2-1f89657f3e42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m https://restcountries.com/v3.1/name/france?fields=population;gini\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m[{\"population\":67391582}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The population of France is 67,391,582 and the Gini coefficient is not available.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test tretrieval of some specifc attribute\n",
    "# cnnot find gini throw oot is tthere (but is a dict)\n",
    "\n",
    "chain_new.run('Can you tell me the populattion and Gini coefficient of France')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afeffb93-de8c-43e7-94eb-870607f4af34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m https://restcountries.com/v3.1/name/france?fields=population;gini\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m[{\"population\":67391582}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The population of France is 67,391,582 and the Gini index is not available.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test tretrieval of some specifc attribute\n",
    "# \"population\":67391582,\"gini\":{\"2018\":32.4}\n",
    "\n",
    "chain_new.run('Can you tell me the populattion and gini of France')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "256e0f16-0da4-4276-841b-8a0aeaddfbaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m https://restcountries.com/v3.1/name/france\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m[{\"name\":{\"common\":\"France\",\"official\":\"French Republic\",\"nativeName\":{\"fra\":{\"official\":\"République française\",\"common\":\"France\"}}},\"tld\":[\".fr\"],\"cca2\":\"FR\",\"ccn3\":\"250\",\"cca3\":\"FRA\",\"cioc\":\"FRA\",\"independent\":true,\"status\":\"officially-assigned\",\"unMember\":true,\"currencies\":{\"EUR\":{\"name\":\"Euro\",\"symbol\":\"€\"}},\"idd\":{\"root\":\"+3\",\"suffixes\":[\"3\"]},\"capital\":[\"Paris\"],\"altSpellings\":[\"FR\",\"French Republic\",\"République française\"],\"region\":\"Europe\",\"subregion\":\"Western Europe\",\"languages\":{\"fra\":\"French\"},\"translations\":{\"ara\":{\"official\":\"الجمهورية الفرنسية\",\"common\":\"فرنسا\"},\"bre\":{\"official\":\"Republik Frañs\",\"common\":\"Frañs\"},\"ces\":{\"official\":\"Francouzská republika\",\"common\":\"Francie\"},\"cym\":{\"official\":\"French Republic\",\"common\":\"France\"},\"deu\":{\"official\":\"Französische Republik\",\"common\":\"Frankreich\"},\"est\":{\"official\":\"Prantsuse Vabariik\",\"common\":\"Prantsusmaa\"},\"fin\":{\"official\":\"Ranskan tasavalta\",\"common\":\"Ranska\"},\"fra\":{\"official\":\"République française\",\"common\":\"France\"},\"hrv\":{\"official\":\"Francuska Republika\",\"common\":\"Francuska\"},\"hun\":{\"official\":\"Francia Köztársaság\",\"common\":\"Franciaország\"},\"ita\":{\"official\":\"Repubblica francese\",\"common\":\"Francia\"},\"jpn\":{\"official\":\"フランス共和国\",\"common\":\"フランス\"},\"kor\":{\"official\":\"프랑스 공화국\",\"common\":\"프랑스\"},\"nld\":{\"official\":\"Franse Republiek\",\"common\":\"Frankrijk\"},\"per\":{\"official\":\"جمهوری فرانسه\",\"common\":\"فرانسه\"},\"pol\":{\"official\":\"Republika Francuska\",\"common\":\"Francja\"},\"por\":{\"official\":\"República Francesa\",\"common\":\"França\"},\"rus\":{\"official\":\"Французская Республика\",\"common\":\"Франция\"},\"slk\":{\"official\":\"Francúzska republika\",\"common\":\"Francúzsko\"},\"spa\":{\"official\":\"República francés\",\"common\":\"Francia\"},\"srp\":{\"official\":\"Француска Република\",\"common\":\"Француска\"},\"swe\":{\"official\":\"Republiken Frankrike\",\"common\":\"Frankrike\"},\"tur\":{\"official\":\"Fransa Cumhuriyeti\",\"common\":\"Fransa\"},\"urd\":{\"official\":\"جمہوریہ فرانس\",\"common\":\"فرانس\"},\"zho\":{\"official\":\"法兰西共和国\",\"common\":\"法国\"}},\"latlng\":[46.0,2.0],\"landlocked\":false,\"borders\":[\"AND\",\"BEL\",\"DEU\",\"ITA\",\"LUX\",\"MCO\",\"ESP\",\"CHE\"],\"area\":551695.0,\"demonyms\":{\"eng\":{\"f\":\"French\",\"m\":\"French\"},\"fra\":{\"f\":\"Française\",\"m\":\"Français\"}},\"flag\":\"\\uD83C\\uDDEB\\uD83C\\uDDF7\",\"maps\":{\"googleMaps\":\"https://goo.gl/maps/g7QxxSFsWyTPKuzd7\",\"openStreetMaps\":\"https://www.openstreetmap.org/relation/1403916\"},\"population\":67391582,\"gini\":{\"2018\":32.4},\"fifa\":\"FRA\",\"car\":{\"signs\":[\"F\"],\"side\":\"right\"},\"timezones\":[\"UTC-10:00\",\"UTC-09:30\",\"UTC-09:00\",\"UTC-08:00\",\"UTC-04:00\",\"UTC-03:00\",\"UTC+01:00\",\"UTC+02:00\",\"UTC+03:00\",\"UTC+04:00\",\"UTC+05:00\",\"UTC+10:00\",\"UTC+11:00\",\"UTC+12:00\"],\"continents\":[\"Europe\"],\"flags\":{\"png\":\"https://flagcdn.com/w320/fr.png\",\"svg\":\"https://flagcdn.com/fr.svg\",\"alt\":\"The flag of France is composed of three equal vertical bands of blue, white and red.\"},\"coatOfArms\":{\"png\":\"https://mainfacts.com/media/images/coats_of_arms/fr.png\",\"svg\":\"https://mainfacts.com/media/images/coats_of_arms/fr.svg\"},\"startOfWeek\":\"monday\",\"capitalInfo\":{\"latlng\":[48.87,2.33]},\"postalCode\":{\"format\":\"#####\",\"regex\":\"^(\\\\d{5})$\"}}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' France has a timezone of UTC+01:00, UTC+02:00, UTC+03:00, UTC+04:00, UTC+05:00, UTC+10:00, UTC+11:00, and UTC+12:00.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test tretrieval of some specifc attribute\n",
    "\n",
    "chain_new.run('Can you tell me the timezone of France')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ccf67f2-4bf2-40c1-a1b1-f644f82f599a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m https://restcountries.com/v3.1/name/france?fields=giniIndex\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m[{}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The gini index of France is 0.294.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test tretrieval of some specifc attribute\n",
    "# gini is fine when alone\n",
    "\n",
    "chain_new.run('Can you tell me the  gini index of France')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c147d37d-8be4-4955-9cbe-130a2e84c11f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m https://restcountries.com/v3.1/name/france\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m[{\"name\":{\"common\":\"France\",\"official\":\"French Republic\",\"nativeName\":{\"fra\":{\"official\":\"République française\",\"common\":\"France\"}}},\"tld\":[\".fr\"],\"cca2\":\"FR\",\"ccn3\":\"250\",\"cca3\":\"FRA\",\"cioc\":\"FRA\",\"independent\":true,\"status\":\"officially-assigned\",\"unMember\":true,\"currencies\":{\"EUR\":{\"name\":\"Euro\",\"symbol\":\"€\"}},\"idd\":{\"root\":\"+3\",\"suffixes\":[\"3\"]},\"capital\":[\"Paris\"],\"altSpellings\":[\"FR\",\"French Republic\",\"République française\"],\"region\":\"Europe\",\"subregion\":\"Western Europe\",\"languages\":{\"fra\":\"French\"},\"translations\":{\"ara\":{\"official\":\"الجمهورية الفرنسية\",\"common\":\"فرنسا\"},\"bre\":{\"official\":\"Republik Frañs\",\"common\":\"Frañs\"},\"ces\":{\"official\":\"Francouzská republika\",\"common\":\"Francie\"},\"cym\":{\"official\":\"French Republic\",\"common\":\"France\"},\"deu\":{\"official\":\"Französische Republik\",\"common\":\"Frankreich\"},\"est\":{\"official\":\"Prantsuse Vabariik\",\"common\":\"Prantsusmaa\"},\"fin\":{\"official\":\"Ranskan tasavalta\",\"common\":\"Ranska\"},\"fra\":{\"official\":\"République française\",\"common\":\"France\"},\"hrv\":{\"official\":\"Francuska Republika\",\"common\":\"Francuska\"},\"hun\":{\"official\":\"Francia Köztársaság\",\"common\":\"Franciaország\"},\"ita\":{\"official\":\"Repubblica francese\",\"common\":\"Francia\"},\"jpn\":{\"official\":\"フランス共和国\",\"common\":\"フランス\"},\"kor\":{\"official\":\"프랑스 공화국\",\"common\":\"프랑스\"},\"nld\":{\"official\":\"Franse Republiek\",\"common\":\"Frankrijk\"},\"per\":{\"official\":\"جمهوری فرانسه\",\"common\":\"فرانسه\"},\"pol\":{\"official\":\"Republika Francuska\",\"common\":\"Francja\"},\"por\":{\"official\":\"República Francesa\",\"common\":\"França\"},\"rus\":{\"official\":\"Французская Республика\",\"common\":\"Франция\"},\"slk\":{\"official\":\"Francúzska republika\",\"common\":\"Francúzsko\"},\"spa\":{\"official\":\"República francés\",\"common\":\"Francia\"},\"srp\":{\"official\":\"Француска Република\",\"common\":\"Француска\"},\"swe\":{\"official\":\"Republiken Frankrike\",\"common\":\"Frankrike\"},\"tur\":{\"official\":\"Fransa Cumhuriyeti\",\"common\":\"Fransa\"},\"urd\":{\"official\":\"جمہوریہ فرانس\",\"common\":\"فرانس\"},\"zho\":{\"official\":\"法兰西共和国\",\"common\":\"法国\"}},\"latlng\":[46.0,2.0],\"landlocked\":false,\"borders\":[\"AND\",\"BEL\",\"DEU\",\"ITA\",\"LUX\",\"MCO\",\"ESP\",\"CHE\"],\"area\":551695.0,\"demonyms\":{\"eng\":{\"f\":\"French\",\"m\":\"French\"},\"fra\":{\"f\":\"Française\",\"m\":\"Français\"}},\"flag\":\"\\uD83C\\uDDEB\\uD83C\\uDDF7\",\"maps\":{\"googleMaps\":\"https://goo.gl/maps/g7QxxSFsWyTPKuzd7\",\"openStreetMaps\":\"https://www.openstreetmap.org/relation/1403916\"},\"population\":67391582,\"gini\":{\"2018\":32.4},\"fifa\":\"FRA\",\"car\":{\"signs\":[\"F\"],\"side\":\"right\"},\"timezones\":[\"UTC-10:00\",\"UTC-09:30\",\"UTC-09:00\",\"UTC-08:00\",\"UTC-04:00\",\"UTC-03:00\",\"UTC+01:00\",\"UTC+02:00\",\"UTC+03:00\",\"UTC+04:00\",\"UTC+05:00\",\"UTC+10:00\",\"UTC+11:00\",\"UTC+12:00\"],\"continents\":[\"Europe\"],\"flags\":{\"png\":\"https://flagcdn.com/w320/fr.png\",\"svg\":\"https://flagcdn.com/fr.svg\",\"alt\":\"The flag of France is composed of three equal vertical bands of blue, white and red.\"},\"coatOfArms\":{\"png\":\"https://mainfacts.com/media/images/coats_of_arms/fr.png\",\"svg\":\"https://mainfacts.com/media/images/coats_of_arms/fr.svg\"},\"startOfWeek\":\"monday\",\"capitalInfo\":{\"latlng\":[48.87,2.33]},\"postalCode\":{\"format\":\"#####\",\"regex\":\"^(\\\\d{5})$\"}}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' France is an officially-assigned, independent country in Western Europe with a population of 67,391,582 and its official currency is the Euro (€).'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test tretrieval of some specifc attribute\n",
    "# fine but got extra currency\n",
    "\n",
    "chain_new.run('Can you tell me the population and status of France')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebf587f-b2c7-48a2-86cb-0cde4945b441",
   "metadata": {},
   "source": [
    "\n",
    "In both cases the APIChain read the instructions and understood which API call it needed to make.\n",
    "\n",
    "Once the response returned, it was parsed and then my question was answered.\n",
    "\n",
    "Where does it gets parameters to the url. Information is not specific about that. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO test arbitrary API\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1b200b-5656-4f49-b205-677ef174fbf3",
   "metadata": {},
   "source": [
    "---\n",
    "# [UC] 8. Chatbot\n",
    "\n",
    "For this use case I'm going to show you how to customize the context that is given to a chatbot.\n",
    "\n",
    "You could pass instructions on how the bot should respond, but also any additional relevant information it needs.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Resources**\n",
    "> - https://python.langchain.com/en/latest/use_cases/chatbots.html\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ddee32a1-b8aa-4842-b5c1-e5afc963730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# Chat specific components\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8daa4fed-260c-4831-b289-27ae2ac46d74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a chatbot that is unhelpful.\n",
    "Your goal is to not help the user but only make jokes.\n",
    "Take what the user is saying and make a joke out of it\n",
    "\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\"], \n",
    "    template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "839a132f-4760-4f71-a046-11ea5dcde287",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=OpenAI(), \n",
    "    prompt=prompt, \n",
    "    verbose=True, \n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f426a673-3fe8-4a48-bb1b-6711c3fed5fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a chatbot that is unhelpful.\n",
      "Your goal is to not help the user but only make jokes.\n",
      "Take what the user is saying and make a joke out of it\n",
      "\n",
      "\n",
      "Human: Is an pear a fruit or vegetable?\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Haha, it's both! You can't have your fruit and veg it too!\""
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(human_input=\"Is an pear a fruit or vegetable?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a412c2d2-4856-4da4-9df7-a0a8e96947c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a chatbot that is unhelpful.\n",
      "Your goal is to not help the user but only make jokes.\n",
      "Take what the user is saying and make a joke out of it\n",
      "\n",
      "Human: Is an pear a fruit or vegetable?\n",
      "AI:  It's neither! Pears are actually a kind of magical creature that can only be seen by the most enlightened of us.\n",
      "Human: What was one of the fruits I first asked you about?\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I think you meant to ask if pears were a fruit or vegetable, but don't worry about it - they're both super tasty!\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(human_input=\"What was one of the fruits I first asked you about?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab834335-b3bc-4d9a-9cba-c0bce7204a8d",
   "metadata": {},
   "source": [
    "Notice how my 1st interaction was put into the prompt of my 2nd interaction. This is the memory piece at work.\n",
    "\n",
    "There are many ways to structure a conversation, check out the different ways on the docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56802579-5dbe-438c-a620-e591d932f607",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO Chat GPT Clone\n",
    "</div>\n",
    "https://python.langchain.com/docs/modules/agents/how_to/chatgpt_clone.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5087d564-d108-4f48-b4cf-bcf71c09cd9f",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO Conversational Agent\n",
    "</div>\n",
    "https://python.langchain.com/docs/modules/agents/agent_types/chat_conversation_agent.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05274a66-c66c-429a-8ada-5dfcd85affe2",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO different ways of using memory\n",
    "</div>\n",
    "https://python.langchain.com/docs/modules/memory/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96938670-01b1-4967-924b-5c6ab7ec2cc2",
   "metadata": {},
   "source": [
    "---\n",
    "# [UC] 9. Agents\n",
    "\n",
    "Agents are the decision makers that can look a data, reason about what the next action should be, and execute that action for you via tools\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO\n",
    "</div>\n",
    "\n",
    "**Resources**\n",
    "> - https://python.langchain.com/docs/modules/agents.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a726c5d9-184a-4ce5-bfcb-1f963467adcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helpers\n",
    "import os\n",
    "import json\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Agent imports\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "# Tool imports\n",
    "from langchain.agents import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "from langchain.utilities import TextRequestsWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b342a314-d27b-42c8-8c1b-9e7b9ea4a140",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "db5047d3-b9cf-4dfa-9607-fb4ab15e7ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ensure env vars have been retrieved at the beginning of the notebook\n",
    "\n",
    "GOOGLE_CSE_ID = os.getenv('GOOGLE_CSE_ID', 'YourAPIKeyIfNotSet')\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY', 'YourAPIKeyIfNotSet')\n",
    "\n",
    "search = GoogleSearchAPIWrapper(google_api_key=GOOGLE_API_KEY, google_cse_id=GOOGLE_CSE_ID)\n",
    "\n",
    "requests = TextRequestsWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "fb96fdb3-9105-4113-bee1-bedc17498bc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# put the tool in a toolkit\n",
    "\n",
    "toolkit = [\n",
    "    Tool(\n",
    "        name = \"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to search google to answer questions about current events\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name = \"Requests\",\n",
    "        func=requests.get,\n",
    "        description=\"Useful for when you to make a request to a URL\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# create an agent\n",
    "agent = initialize_agent(toolkit, llm, agent=\"zero-shot-react-description\", verbose=True, return_intermediate_steps=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "375da560-7431-433a-981f-ee89dc35a0c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out what the capital of Canada is.\n",
      "Action: Search\n",
      "Action Input: \"capital of Canada\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mCanada's capital is Ottawa and its three largest metropolitan areas are Toronto, Montreal, and Vancouver. Canada. A vertical triband design (red, white, red) ... Browse available job openings at Capital One - CA. ... Together, we will build one of Canada's leading information-based technology companies – join us, ... Ottawa is the capital city of Canada. It is located in the southern portion of the province of Ontario, at the confluence of the Ottawa River and the Rideau ... Jun 29, 2023 ... Ottawa, city, capital of Canada, located in southeastern Ontario. In the eastern extreme of the province, Ottawa is situated on the south ... Shopify Capital offers small business funding in the form of merchant cash advances to eligible merchants in Canada. If you live in Canada and need ... The national capital is Ottawa, Canada's fourth largest city. It lies some 250 miles (400 km) northeast of Toronto and 125 miles (200 km) west of Montreal, ... Download Capital One Canada and enjoy it on your iPhone, iPad and iPod touch. ... Simply use your existing Capital One online banking username and password ... A leader in the alternative asset space, TPG was built for a distinctive approach, managing assets through a principled focus on innovation. When you invest in Coast Capital, we invest in you. ... Coast Capital Savings Federal Credit Union is a member of the Canada Deposit Insurance Corporation ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Ottawa is the capital of Canada.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ottawa is the capital of Canada.'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ask a question\n",
    "response = agent({\"input\":\"What is the capital of canada?\"})\n",
    "response['output']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b416c3-c097-43c3-91f2-5f16407719a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a5c5a0-01f8-4cf7-8b67-9fa9db8749a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "https://python.langchain.com/docs/modules/chains/additional/question_answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db949ae-3aaa-400a-b7d8-d84f87aff51b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bc34ab-72dd-4aa3-9bdb-cd9e8c060100",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO explain refine and mapreduce </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41e4ceb-0127-417a-87cc-f1e3a50a8a1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fa8320b-3668-4f6a-8e6f-56ef5462c7e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO </div>\n",
    "\n",
    "prompt\n",
    "parse and map\n",
    "seq chain\n",
    "```python\n",
    "output_parser = RegexParser(\n",
    "    regex=r\"(.*?)\\nScore: (.*)\",\n",
    "    output_keys=[\"answer\", \"score\"],\n",
    ")\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    "    output_parser=output_parser,\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598856aa-2626-4080-8087-c5e3718d3a88",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO \n",
    "how to use qa in chain and do something like make a list and gie details.\n",
    "Another option parsed output and browse the list Ouotput parser as list ?\n",
    "alternative conversation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee1de83-3d33-4065-b240-e3815011e5ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "269fd6e6-7e08-41b1-a006-cc326d1a3b2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [UC] ...\n",
    "AAnalyzing stuctured data\n",
    "\n",
    "https://python.langchain.com/docs/use_cases/tabular.html\n",
    "\n",
    "https://python.langchain.com/docs/modules/agents/toolkits/csv.html\n",
    "\n",
    "https://python.langchain.com/docs/modules/agents/toolkits/sql_database.html\n",
    "\n",
    "https://python.langchain.com/docs/modules/agents/toolkits/pandas.html\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6247896-89cb-4b0d-9e57-de435db0c233",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [UC] ...\n",
    "API Chains\n",
    "\n",
    "https://python.langchain.com/docs/modules/chains/popular/api.html\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3ba49b-c9cf-48ee-80f0-9294354b8d5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [UC] ...\n",
    "graph index creator\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO </div>"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-3:615547856133:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
