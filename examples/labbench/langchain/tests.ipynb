{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00afbf75-6012-4c57-9dad-4080e36c5687",
   "metadata": {},
   "source": [
    "# Langchain Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dc41cd-690a-4c2d-8253-dd71b609560a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c1be20e-ba74-42e4-a144-3d05841998bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## What is LangChain?\n",
    "\n",
    "LangChain is an open source framework that allows AI developers to combine Large Language Models (LLMs) with external data. \n",
    "\n",
    "**Resources**\n",
    "\n",
    "> LangChain resources\n",
    "> - Landpage: https://readthedocs.org/projects/langchain/db2d\n",
    "> - Comonents: https://docs.langchain.com/docs/category/components\n",
    "> - git: https://github.com/hwchase17/langchain.git\n",
    "> - API Reference: https://api.python.langchain.com/en/latest/\n",
    "\n",
    "> LangChain applications\n",
    "> - [LangChain Awesome](https://github.com/kyrolabs/awesome-langchain)\n",
    "\n",
    "> This notebook is largely based on Greg Kamradt's videos and cookbooks\n",
    "> - [Langchain tuorial suite](https://www.youtube.com/playlist?list=PLqZXAkvF1bPNQER9mLmDbntNfSpzdDIU5)\n",
    "> - [LangChain cookbooks](https://github.com/gkamradt/langchain-tutorials)\n",
    "\n",
    "> Additonal resources and tutorial\n",
    "> - [Cookbook Comprehensive Guide](https://nathankjer.com/introduction-to-langchain/)\n",
    "> - [A Gentle Intro to Chaining LLMs, Agents, and utils via LangChain](https://towardsdatascience.com/a-gentle-intro-to-chaining-llms-agents-and-utils-via-langchain-16cd385fca81)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ab8418-6cca-4c79-9f4c-95604c87214c",
   "metadata": {},
   "source": [
    "## This notebook\n",
    "\n",
    "This notebook collects Python examples. The chapters are based oo the LangChain compoents documented here https://docs.langchain.com/docs/category/components.\n",
    "\n",
    "Some changes though:\n",
    "- use Annoy instead of FAISS as a vector database\n",
    "- use Google Search API instead of SerpAPI\n",
    "- change in examples and additional examples \n",
    "- change in API keys setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff9bffb-d536-4f22-b5dc-2153c49d8340",
   "metadata": {},
   "source": [
    "This notebook has been tested in June 2023 on AWS SageMaker using DataScience 3.0 image.\n",
    "\n",
    "Test environment:\n",
    "> - AWS SageMaker Studio's notebook \n",
    ">> - Kernel image Data Science 3.0\n",
    ">> - t3.medium 2CPU - 4GB\n",
    ">> - Python 3.9.15\n",
    ">> - Linux default 4.14.304-226.531.amzn2.x86_64\n",
    "> - installed packages:\n",
    ">> - langchain 0.0.218\n",
    ">> - openai 0.27.8\n",
    ">> - google_api_python_client 2.90.0\n",
    ">> - tikitoken 0.4.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562017b1-fd67-475a-bc77-2019581bf2d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "<div style=\"background-color:green;color:black;text-align:center;padding:1rem;font-size:1.5rem;\">NOTEBOOK SETUP</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf59e64-4e2f-4277-a3af-485c2bf7a8e8",
   "metadata": {},
   "source": [
    "**Instructions**\n",
    "\n",
    "All setups are at the top of the notebook so that you can run all this section initialize the notebook.\n",
    "\n",
    "Notebook chapters are not dependant on each other and may be run in isolation.\n",
    "\n",
    "Before running the setup you may need to create the following resources\n",
    "- request an OpenAI API keys. OpenAI APIs are not free.\n",
    "- create a Custom Search Engine in Google Search. it is free.\n",
    "- request an API key for the Google Search service. It is free.\n",
    "\n",
    "Confer to the setup sections for instruction on how to create those resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614b5320-9d11-4e53-9f04-4c3dba3b769d",
   "metadata": {},
   "source": [
    "---\n",
    "## API keys and environment\n",
    "\n",
    "Langchain will get the API keys from environment variables or function parameters.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Never show the keys in shared notebooks, whether it part of the code or a log. A simple way to avoid key leakage, is to use environement variables.  You set the environment variable in the terminal or some local configuration. If so you do not have to set the key here.\n",
    "\n",
    "- If it is easier for you to set the key here by assigning the value, do not forget to empty the string right after you run this block. The environment will be kept in memory as long as the kernel runs.\n",
    "\n",
    "- Be careful when printing the keys. Ensure that you remove the outputs. \n",
    "\n",
    "- Before sharing check that the keys are not printed out by some features of the libraries. Avoid to print libraries' objects. They often hold the API keys as a property and may disclose the key value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4397c800-3ee5-40aa-b0a7-5021985e334a",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "I Store API keys and configuration information in AWS Secrets Manager. The code below retrieves the secret holding the keys. The secret is a JSON string consisting in key/value pairs. It will be used later to set various environnement variables.\n",
    "\n",
    "When using Notebooks an SageMaker do not forget to give permissions to read this secret to SageMaker execution role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6671d484-9c5d-4818-8449-6df0189fd307",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash --out secrets \n",
    "# using AWS's Secret Manager to store keys\n",
    "# garb the keys and store it into a Pytthon variable\n",
    "export RESPONSE=$(aws secretsmanager get-secret-value --secret-id 'salvia/labbench/tests' )\n",
    "export SECRETS=$( echo $RESPONSE | jq '.SecretString | fromjson')\n",
    "\n",
    "echo $SECRETS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7188bf6d-17d8-41c0-8c24-6db9ff6e07bb",
   "metadata": {},
   "source": [
    "---\n",
    "## LangChain Setup\n",
    "\n",
    "**Resources**\n",
    "> - [LangChain GetStarted](https://python.langchain.com/docs/get_started/quickstart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8713a09-0a7a-416c-807a-e2560796f7d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain==0.0.218\n",
      "  Using cached langchain-0.0.218-py3-none-any.whl (1.2 MB)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.218) (5.4.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.218) (1.4.39)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain==0.0.218)\n",
      "  Using cached aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain==0.0.218)\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.218)\n",
      "  Using cached dataclasses_json-0.5.9-py3-none-any.whl (26 kB)\n",
      "Collecting langchainplus-sdk>=0.0.17 (from langchain==0.0.218)\n",
      "  Using cached langchainplus_sdk-0.0.19-py3-none-any.whl (25 kB)\n",
      "Collecting numexpr<3.0.0,>=2.8.4 (from langchain==0.0.218)\n",
      "  Using cached numexpr-2.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (381 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.218) (1.24.3)\n",
      "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain==0.0.218)\n",
      "  Using cached openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
      "Collecting pydantic<2,>=1 (from langchain==0.0.218)\n",
      "  Using cached pydantic-1.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.218) (2.31.0)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain==0.0.218)\n",
      "  Using cached tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.218) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.218) (2.0.4)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.218)\n",
      "  Using cached multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.218)\n",
      "  Using cached yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.218)\n",
      "  Using cached frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.218)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.218)\n",
      "  Using cached marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
      "Collecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.218)\n",
      "  Using cached marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.218)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<2,>=1->langchain==0.0.218) (4.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.218) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.218) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.218) (2023.5.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.218) (1.1.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.218) (21.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.218) (0.4.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.218) (3.0.9)\n",
      "Installing collected packages: typing-inspect, tenacity, pydantic, numexpr, multidict, frozenlist, async-timeout, yarl, openapi-schema-pydantic, marshmallow, langchainplus-sdk, aiosignal, marshmallow-enum, aiohttp, dataclasses-json, langchain\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.0.1\n",
      "    Uninstalling tenacity-8.0.1:\n",
      "      Successfully uninstalled tenacity-8.0.1\n",
      "  Attempting uninstall: numexpr\n",
      "    Found existing installation: numexpr 2.8.3\n",
      "    Uninstalling numexpr-2.8.3:\n",
      "      Successfully uninstalled numexpr-2.8.3\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 dataclasses-json-0.5.9 frozenlist-1.3.3 langchain-0.0.218 langchainplus-sdk-0.0.19 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 numexpr-2.8.4 openapi-schema-pydantic-1.2.4 pydantic-1.10.10 tenacity-8.2.2 typing-inspect-0.9.0 yarl-1.9.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain==0.0.218\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51de5b76-5e76-4c8f-bd11-9445f98c6941",
   "metadata": {},
   "source": [
    "---\n",
    "## OpenAI Setup\n",
    "\n",
    "**Resources**\n",
    "> - [OpenAI tutorial on API keys](https://platform.openai.com/docs/quickstart)\n",
    "> - [OpenAI package on Pypi](https://pypi.org/project/openai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be008522-bd17-47d4-bd6d-58da90214050",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = eval(secrets)[\"OPENAI_API_KEY\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bc38381-5ee0-4107-9b3a-40dd9cb9302e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==0.27.8\n",
      "  Using cached openai-0.27.8-py3-none-any.whl (73 kB)\n",
      "Requirement already satisfied: requests>=2.20 in /opt/conda/lib/python3.10/site-packages (from openai==0.27.8) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from openai==0.27.8) (4.64.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from openai==0.27.8) (3.8.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai==0.27.8) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai==0.27.8) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai==0.27.8) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai==0.27.8) (2023.5.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai==0.27.8) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai==0.27.8) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai==0.27.8) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai==0.27.8) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai==0.27.8) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai==0.27.8) (1.3.1)\n",
      "Installing collected packages: openai\n",
      "Successfully installed openai-0.27.8\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai==0.27.8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31520149-8e0f-4b08-b4a5-08c118fb638f",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Google Search setup\n",
    "\n",
    "**Resources**\n",
    "\n",
    "> How to configure the Google search in LangChain \n",
    "> - https://python.langchain.com/docs/ecosystem/integrations/google_search\n",
    "\n",
    "> Custom Search Engine configuration \n",
    "> - https://stackoverflow.com/questions/37083058/programmatically-searching-google-in-python-using-custom-search\n",
    "\n",
    "> CSE API \n",
    "> - repo: https://github.com/google/google-api-python-client\n",
    "> - more info: https://developers.google.com/api-client-library/python/apis/customsearch/v1\n",
    "> - complete docs: https://api-python-client-doc.appspot.com/\n",
    "\n",
    "> Get an API key\n",
    "> - https://developers.google.com/custom-search/v1/introduction\n",
    "\n",
    "> Package information\n",
    "> - [Google API client package on Pypi](https://pypi.org/project/google-api-python-client/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09c65606-95d2-4b96-813f-db44b9ffcbb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Unlock the API and get a key \n",
    "os.environ[\"GOOGLE_API_KEY\"] = eval(secrets)[\"GOOGLE_API_KEY\"]\n",
    "# Create or use an existing Custom Search Engine\n",
    "# on the CSE page under Searcg Engone ID\n",
    "os.environ[\"GOOGLE_CSE_ID\"] = eval(secrets)[\"GOOGLE_CSE_ID\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e169ba1-9d2d-4bfa-843c-afe61e1228e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-api-python-client==2.90.0\n",
      "  Using cached google_api_python_client-2.90.0-py2.py3-none-any.whl (11.4 MB)\n",
      "Collecting httplib2<1.dev0,>=0.15.0 (from google-api-python-client==2.90.0)\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Collecting google-auth<3.0.0.dev0,>=1.19.0 (from google-api-python-client==2.90.0)\n",
      "  Using cached google_auth-2.21.0-py2.py3-none-any.whl (182 kB)\n",
      "Collecting google-auth-httplib2>=0.1.0 (from google-api-python-client==2.90.0)\n",
      "  Using cached google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 (from google-api-python-client==2.90.0)\n",
      "  Using cached google_api_core-2.11.1-py3-none-any.whl (120 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client==2.90.0)\n",
      "  Using cached uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client==2.90.0)\n",
      "  Using cached googleapis_common_protos-1.59.1-py2.py3-none-any.whl (224 kB)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client==2.90.0) (3.20.3)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client==2.90.0) (2.31.0)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client==2.90.0)\n",
      "  Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client==2.90.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client==2.90.0) (4.7.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client==2.90.0) (1.16.0)\n",
      "Collecting urllib3<2.0 (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client==2.90.0)\n",
      "  Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.10/site-packages (from httplib2<1.dev0,>=0.15.0->google-api-python-client==2.90.0) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client==2.90.0) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client==2.90.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client==2.90.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client==2.90.0) (2023.5.7)\n",
      "Installing collected packages: urllib3, uritemplate, httplib2, googleapis-common-protos, cachetools, google-auth, google-auth-httplib2, google-api-core, google-api-python-client\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.3\n",
      "    Uninstalling urllib3-2.0.3:\n",
      "      Successfully uninstalled urllib3-2.0.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed cachetools-5.3.1 google-api-core-2.11.1 google-api-python-client-2.90.0 google-auth-2.21.0 google-auth-httplib2-0.1.0 googleapis-common-protos-1.59.1 httplib2-0.22.0 uritemplate-4.1.1 urllib3-1.26.16\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-api-python-client==2.90.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eac58b-33e7-49cb-981a-a8c2a3dd573b",
   "metadata": {},
   "source": [
    "## Setup Annoy as a vector database \n",
    "\n",
    "Some examples requires a Vector Database (document selector, document retrieval).\n",
    "\n",
    "LangChain use ChromaDB by default. For whatever reason it failed to install. Used Annoy instead. An alterntive is FAIIS. You may also want to use online Vector database like Pinecone or Weaviate. \n",
    "\n",
    "Most of these packages include c++ code and requires GCC at the install time. It is not included in SageMaker DataScience 3 image. So the first step is installing GCC. \n",
    "\n",
    "NOTE: Annoy is read-only - once the index is built you cannot add any more emebddings.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Resources**\n",
    "> - [Annoy package on Pypi](https://pypi.org/project/annoy/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43d6d859-637f-4187-8270-8063f2c4abd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://deb.debian.org/debian bullseye InRelease [116 kB]\n",
      "Get:2 http://security.debian.org/debian-security bullseye-security InRelease [48.4 kB]\n",
      "Get:3 http://deb.debian.org/debian bullseye-updates InRelease [44.1 kB]\n",
      "Get:4 http://security.debian.org/debian-security bullseye-security/main amd64 Packages [252 kB]\n",
      "Get:5 http://deb.debian.org/debian bullseye/main amd64 Packages [8183 kB]\n",
      "Get:6 http://deb.debian.org/debian bullseye-updates/main amd64 Packages [14.8 kB]\n",
      "Fetched 8658 kB in 2s (5078 kB/s)\n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  dirmngr dpkg-dev fakeroot g++ g++-10 gnupg gnupg-l10n gnupg-utils gpg\n",
      "  gpg-agent gpg-wks-client gpg-wks-server gpgconf gpgsm gpgv\n",
      "  libalgorithm-diff-perl libalgorithm-diff-xs-perl libalgorithm-merge-perl\n",
      "  libassuan0 libdpkg-perl libfakeroot libfile-fcntllock-perl libksba8\n",
      "  liblocale-gettext-perl libnpth0 libstdc++-10-dev make patch pinentry-curses\n",
      "  xz-utils\n",
      "Suggested packages:\n",
      "  dbus-user-session libpam-systemd pinentry-gnome3 tor debian-keyring\n",
      "  g++-multilib g++-10-multilib gcc-10-doc parcimonie xloadimage scdaemon bzr\n",
      "  libstdc++-10-doc make-doc ed diffutils-doc pinentry-doc\n",
      "The following NEW packages will be installed:\n",
      "  build-essential dirmngr dpkg-dev fakeroot g++ g++-10 gnupg gnupg-l10n\n",
      "  gnupg-utils gpg gpg-agent gpg-wks-client gpg-wks-server gpgconf gpgsm\n",
      "  libalgorithm-diff-perl libalgorithm-diff-xs-perl libalgorithm-merge-perl\n",
      "  libassuan0 libdpkg-perl libfakeroot libfile-fcntllock-perl libksba8\n",
      "  liblocale-gettext-perl libnpth0 libstdc++-10-dev make patch pinentry-curses\n",
      "  xz-utils\n",
      "The following packages will be upgraded:\n",
      "  gpgv\n",
      "1 upgraded, 30 newly installed, 0 to remove and 32 not upgraded.\n",
      "Need to get 24.3 MB of archives.\n",
      "After this operation, 70.6 MB of additional disk space will be used.\n",
      "Get:1 http://deb.debian.org/debian bullseye/main amd64 liblocale-gettext-perl amd64 1.07-4+b1 [19.0 kB]\n",
      "Get:2 http://deb.debian.org/debian bullseye/main amd64 gpgv amd64 2.2.27-2+deb11u2 [626 kB]\n",
      "Get:3 http://deb.debian.org/debian bullseye/main amd64 xz-utils amd64 5.2.5-2.1~deb11u1 [220 kB]\n",
      "Get:4 http://deb.debian.org/debian bullseye/main amd64 libstdc++-10-dev amd64 10.2.1-6 [1741 kB]\n",
      "Get:5 http://deb.debian.org/debian bullseye/main amd64 g++-10 amd64 10.2.1-6 [9380 kB]\n",
      "Get:6 http://deb.debian.org/debian bullseye/main amd64 g++ amd64 4:10.2.1-1 [1644 B]\n",
      "Get:7 http://deb.debian.org/debian bullseye/main amd64 make amd64 4.3-4.1 [396 kB]\n",
      "Get:8 http://deb.debian.org/debian bullseye/main amd64 libdpkg-perl all 1.20.12 [1551 kB]\n",
      "Get:9 http://deb.debian.org/debian bullseye/main amd64 patch amd64 2.7.6-7 [128 kB]\n",
      "Get:10 http://deb.debian.org/debian bullseye/main amd64 dpkg-dev all 1.20.12 [2312 kB]\n",
      "Get:11 http://deb.debian.org/debian bullseye/main amd64 build-essential amd64 12.9 [7704 B]\n",
      "Get:12 http://deb.debian.org/debian bullseye/main amd64 libassuan0 amd64 2.5.3-7.1 [50.5 kB]\n",
      "Get:13 http://deb.debian.org/debian bullseye/main amd64 gpgconf amd64 2.2.27-2+deb11u2 [548 kB]\n",
      "Get:14 http://deb.debian.org/debian bullseye/main amd64 libksba8 amd64 1.5.0-3+deb11u2 [123 kB]\n",
      "Get:15 http://deb.debian.org/debian bullseye/main amd64 libnpth0 amd64 1.6-3 [19.0 kB]\n",
      "Get:16 http://deb.debian.org/debian bullseye/main amd64 dirmngr amd64 2.2.27-2+deb11u2 [763 kB]\n",
      "Get:17 http://deb.debian.org/debian bullseye/main amd64 libfakeroot amd64 1.25.3-1.1 [47.0 kB]\n",
      "Get:18 http://deb.debian.org/debian bullseye/main amd64 fakeroot amd64 1.25.3-1.1 [87.0 kB]\n",
      "Get:19 http://deb.debian.org/debian bullseye/main amd64 gnupg-l10n all 2.2.27-2+deb11u2 [1086 kB]\n",
      "Get:20 http://deb.debian.org/debian bullseye/main amd64 gnupg-utils amd64 2.2.27-2+deb11u2 [905 kB]\n",
      "Get:21 http://deb.debian.org/debian bullseye/main amd64 gpg amd64 2.2.27-2+deb11u2 [928 kB]\n",
      "Get:22 http://deb.debian.org/debian bullseye/main amd64 pinentry-curses amd64 1.1.0-4 [64.9 kB]\n",
      "Get:23 http://deb.debian.org/debian bullseye/main amd64 gpg-agent amd64 2.2.27-2+deb11u2 [669 kB]\n",
      "Get:24 http://deb.debian.org/debian bullseye/main amd64 gpg-wks-client amd64 2.2.27-2+deb11u2 [524 kB]\n",
      "Get:25 http://deb.debian.org/debian bullseye/main amd64 gpg-wks-server amd64 2.2.27-2+deb11u2 [516 kB]\n",
      "Get:26 http://deb.debian.org/debian bullseye/main amd64 gpgsm amd64 2.2.27-2+deb11u2 [645 kB]\n",
      "Get:27 http://deb.debian.org/debian bullseye/main amd64 gnupg all 2.2.27-2+deb11u2 [825 kB]\n",
      "Get:28 http://deb.debian.org/debian bullseye/main amd64 libalgorithm-diff-perl all 1.201-1 [43.3 kB]\n",
      "Get:29 http://deb.debian.org/debian bullseye/main amd64 libalgorithm-diff-xs-perl amd64 0.04-6+b1 [12.0 kB]\n",
      "Get:30 http://deb.debian.org/debian bullseye/main amd64 libalgorithm-merge-perl all 0.08-3 [12.7 kB]\n",
      "Get:31 http://deb.debian.org/debian bullseye/main amd64 libfile-fcntllock-perl amd64 0.22-3+b7 [35.5 kB]\n",
      "Fetched 24.3 MB in 0s (131 MB/s)                    \n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package liblocale-gettext-perl.\n",
      "(Reading database ... 21534 files and directories currently installed.)\n",
      "Preparing to unpack .../liblocale-gettext-perl_1.07-4+b1_amd64.deb ...\n",
      "Unpacking liblocale-gettext-perl (1.07-4+b1) ...\n",
      "Preparing to unpack .../gpgv_2.2.27-2+deb11u2_amd64.deb ...\n",
      "Unpacking gpgv (2.2.27-2+deb11u2) over (2.2.27-2+deb11u1) ...\n",
      "Setting up gpgv (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package xz-utils.\n",
      "(Reading database ... 21549 files and directories currently installed.)\n",
      "Preparing to unpack .../00-xz-utils_5.2.5-2.1~deb11u1_amd64.deb ...\n",
      "Unpacking xz-utils (5.2.5-2.1~deb11u1) ...\n",
      "Selecting previously unselected package libstdc++-10-dev:amd64.\n",
      "Preparing to unpack .../01-libstdc++-10-dev_10.2.1-6_amd64.deb ...\n",
      "Unpacking libstdc++-10-dev:amd64 (10.2.1-6) ...\n",
      "Selecting previously unselected package g++-10.\n",
      "Preparing to unpack .../02-g++-10_10.2.1-6_amd64.deb ...\n",
      "Unpacking g++-10 (10.2.1-6) ...\n",
      "Selecting previously unselected package g++.\n",
      "Preparing to unpack .../03-g++_4%3a10.2.1-1_amd64.deb ...\n",
      "Unpacking g++ (4:10.2.1-1) ...\n",
      "Selecting previously unselected package make.\n",
      "Preparing to unpack .../04-make_4.3-4.1_amd64.deb ...\n",
      "Unpacking make (4.3-4.1) ...\n",
      "Selecting previously unselected package libdpkg-perl.\n",
      "Preparing to unpack .../05-libdpkg-perl_1.20.12_all.deb ...\n",
      "Unpacking libdpkg-perl (1.20.12) ...\n",
      "Selecting previously unselected package patch.\n",
      "Preparing to unpack .../06-patch_2.7.6-7_amd64.deb ...\n",
      "Unpacking patch (2.7.6-7) ...\n",
      "Selecting previously unselected package dpkg-dev.\n",
      "Preparing to unpack .../07-dpkg-dev_1.20.12_all.deb ...\n",
      "Unpacking dpkg-dev (1.20.12) ...\n",
      "Selecting previously unselected package build-essential.\n",
      "Preparing to unpack .../08-build-essential_12.9_amd64.deb ...\n",
      "Unpacking build-essential (12.9) ...\n",
      "Selecting previously unselected package libassuan0:amd64.\n",
      "Preparing to unpack .../09-libassuan0_2.5.3-7.1_amd64.deb ...\n",
      "Unpacking libassuan0:amd64 (2.5.3-7.1) ...\n",
      "Selecting previously unselected package gpgconf.\n",
      "Preparing to unpack .../10-gpgconf_2.2.27-2+deb11u2_amd64.deb ...\n",
      "Unpacking gpgconf (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package libksba8:amd64.\n",
      "Preparing to unpack .../11-libksba8_1.5.0-3+deb11u2_amd64.deb ...\n",
      "Unpacking libksba8:amd64 (1.5.0-3+deb11u2) ...\n",
      "Selecting previously unselected package libnpth0:amd64.\n",
      "Preparing to unpack .../12-libnpth0_1.6-3_amd64.deb ...\n",
      "Unpacking libnpth0:amd64 (1.6-3) ...\n",
      "Selecting previously unselected package dirmngr.\n",
      "Preparing to unpack .../13-dirmngr_2.2.27-2+deb11u2_amd64.deb ...\n",
      "Unpacking dirmngr (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package libfakeroot:amd64.\n",
      "Preparing to unpack .../14-libfakeroot_1.25.3-1.1_amd64.deb ...\n",
      "Unpacking libfakeroot:amd64 (1.25.3-1.1) ...\n",
      "Selecting previously unselected package fakeroot.\n",
      "Preparing to unpack .../15-fakeroot_1.25.3-1.1_amd64.deb ...\n",
      "Unpacking fakeroot (1.25.3-1.1) ...\n",
      "Selecting previously unselected package gnupg-l10n.\n",
      "Preparing to unpack .../16-gnupg-l10n_2.2.27-2+deb11u2_all.deb ...\n",
      "Unpacking gnupg-l10n (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package gnupg-utils.\n",
      "Preparing to unpack .../17-gnupg-utils_2.2.27-2+deb11u2_amd64.deb ...\n",
      "Unpacking gnupg-utils (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package gpg.\n",
      "Preparing to unpack .../18-gpg_2.2.27-2+deb11u2_amd64.deb ...\n",
      "Unpacking gpg (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package pinentry-curses.\n",
      "Preparing to unpack .../19-pinentry-curses_1.1.0-4_amd64.deb ...\n",
      "Unpacking pinentry-curses (1.1.0-4) ...\n",
      "Selecting previously unselected package gpg-agent.\n",
      "Preparing to unpack .../20-gpg-agent_2.2.27-2+deb11u2_amd64.deb ...\n",
      "Unpacking gpg-agent (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package gpg-wks-client.\n",
      "Preparing to unpack .../21-gpg-wks-client_2.2.27-2+deb11u2_amd64.deb ...\n",
      "Unpacking gpg-wks-client (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package gpg-wks-server.\n",
      "Preparing to unpack .../22-gpg-wks-server_2.2.27-2+deb11u2_amd64.deb ...\n",
      "Unpacking gpg-wks-server (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package gpgsm.\n",
      "Preparing to unpack .../23-gpgsm_2.2.27-2+deb11u2_amd64.deb ...\n",
      "Unpacking gpgsm (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package gnupg.\n",
      "Preparing to unpack .../24-gnupg_2.2.27-2+deb11u2_all.deb ...\n",
      "Unpacking gnupg (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package libalgorithm-diff-perl.\n",
      "Preparing to unpack .../25-libalgorithm-diff-perl_1.201-1_all.deb ...\n",
      "Unpacking libalgorithm-diff-perl (1.201-1) ...\n",
      "Selecting previously unselected package libalgorithm-diff-xs-perl.\n",
      "Preparing to unpack .../26-libalgorithm-diff-xs-perl_0.04-6+b1_amd64.deb ...\n",
      "Unpacking libalgorithm-diff-xs-perl (0.04-6+b1) ...\n",
      "Selecting previously unselected package libalgorithm-merge-perl.\n",
      "Preparing to unpack .../27-libalgorithm-merge-perl_0.08-3_all.deb ...\n",
      "Unpacking libalgorithm-merge-perl (0.08-3) ...\n",
      "Selecting previously unselected package libfile-fcntllock-perl.\n",
      "Preparing to unpack .../28-libfile-fcntllock-perl_0.22-3+b7_amd64.deb ...\n",
      "Unpacking libfile-fcntllock-perl (0.22-3+b7) ...\n",
      "Setting up libksba8:amd64 (1.5.0-3+deb11u2) ...\n",
      "Setting up libstdc++-10-dev:amd64 (10.2.1-6) ...\n",
      "Setting up g++-10 (10.2.1-6) ...\n",
      "Setting up libfile-fcntllock-perl (0.22-3+b7) ...\n",
      "Setting up libalgorithm-diff-perl (1.201-1) ...\n",
      "Setting up libnpth0:amd64 (1.6-3) ...\n",
      "Setting up libassuan0:amd64 (2.5.3-7.1) ...\n",
      "Setting up libfakeroot:amd64 (1.25.3-1.1) ...\n",
      "Setting up fakeroot (1.25.3-1.1) ...\n",
      "update-alternatives: using /usr/bin/fakeroot-sysv to provide /usr/bin/fakeroot (fakeroot) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/fakeroot.1.gz because associated file /usr/share/man/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/faked.1.gz because associated file /usr/share/man/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/es/man1/fakeroot.1.gz because associated file /usr/share/man/es/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/es/man1/faked.1.gz because associated file /usr/share/man/es/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/fr/man1/fakeroot.1.gz because associated file /usr/share/man/fr/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/fr/man1/faked.1.gz because associated file /usr/share/man/fr/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/sv/man1/fakeroot.1.gz because associated file /usr/share/man/sv/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/sv/man1/faked.1.gz because associated file /usr/share/man/sv/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "Setting up make (4.3-4.1) ...\n",
      "Setting up gnupg-l10n (2.2.27-2+deb11u2) ...\n",
      "Setting up xz-utils (5.2.5-2.1~deb11u1) ...\n",
      "update-alternatives: using /usr/bin/xz to provide /usr/bin/lzma (lzma) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzma.1.gz because associated file /usr/share/man/man1/xz.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/unlzma.1.gz because associated file /usr/share/man/man1/unxz.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzcat.1.gz because associated file /usr/share/man/man1/xzcat.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzmore.1.gz because associated file /usr/share/man/man1/xzmore.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzless.1.gz because associated file /usr/share/man/man1/xzless.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzdiff.1.gz because associated file /usr/share/man/man1/xzdiff.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzcmp.1.gz because associated file /usr/share/man/man1/xzcmp.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzgrep.1.gz because associated file /usr/share/man/man1/xzgrep.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzegrep.1.gz because associated file /usr/share/man/man1/xzegrep.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzfgrep.1.gz because associated file /usr/share/man/man1/xzfgrep.1.gz (of link group lzma) doesn't exist\n",
      "Setting up patch (2.7.6-7) ...\n",
      "Setting up libdpkg-perl (1.20.12) ...\n",
      "Setting up g++ (4:10.2.1-1) ...\n",
      "update-alternatives: using /usr/bin/g++ to provide /usr/bin/c++ (c++) in auto mode\n",
      "Setting up gpgconf (2.2.27-2+deb11u2) ...\n",
      "Setting up libalgorithm-diff-xs-perl (0.04-6+b1) ...\n",
      "Setting up liblocale-gettext-perl (1.07-4+b1) ...\n",
      "Setting up gpg (2.2.27-2+deb11u2) ...\n",
      "Setting up libalgorithm-merge-perl (0.08-3) ...\n",
      "Setting up gnupg-utils (2.2.27-2+deb11u2) ...\n",
      "Setting up pinentry-curses (1.1.0-4) ...\n",
      "Setting up gpg-agent (2.2.27-2+deb11u2) ...\n",
      "Setting up gpgsm (2.2.27-2+deb11u2) ...\n",
      "Setting up dpkg-dev (1.20.12) ...\n",
      "Setting up dirmngr (2.2.27-2+deb11u2) ...\n",
      "Setting up gpg-wks-server (2.2.27-2+deb11u2) ...\n",
      "Setting up build-essential (12.9) ...\n",
      "Setting up gpg-wks-client (2.2.27-2+deb11u2) ...\n",
      "Setting up gnupg (2.2.27-2+deb11u2) ...\n",
      "Processing triggers for libc-bin (2.31-13+deb11u3) ...\n"
     ]
    }
   ],
   "source": [
    "!apt-get update && apt-get install -y build-essential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb5d265a-34ef-4c53-940b-2b762c09859d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting annoy==1.17.3\n",
      "  Using cached annoy-1.17.3-cp310-cp310-linux_x86_64.whl\n",
      "Installing collected packages: annoy\n",
      "Successfully installed annoy-1.17.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install annoy==1.17.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465dcd67-f5de-4e37-a0c8-f61e8b3c6356",
   "metadata": {},
   "source": [
    "# Setup additionalm API tools\n",
    "<div class=\"alert alert-block alert-warning\"> \n",
    "    TODO <br>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1abcaa39-8611-4564-a26a-fd39f2e5300d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Using cached wikipedia-1.4.0-py3-none-any.whl\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from wikipedia) (4.11.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.5.7)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->wikipedia) (2.3.1)\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04436cf8-3983-487f-9688-a929ba1ab86b",
   "metadata": {},
   "source": [
    "## Setup additional tools for embeddings\n",
    "\n",
    "When working with embeddings additonal packages are required.\n",
    "\n",
    "- tiktoken, as a encoder and tokenizer\n",
    "\n",
    "**Resources**\n",
    "> - [Tiktoken package on Pypi](https://pypi.org/project/tiktoken/)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "189b27d4-8afc-481d-b5bb-610a28171566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken==0.4.0\n",
      "  Using cached tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken==0.4.0) (2022.7.9)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken==0.4.0) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken==0.4.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken==0.4.0) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken==0.4.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken==0.4.0) (2023.5.7)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken==0.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6986800-5d16-496f-aaa2-3e48587a492f",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"background-color:green;color:black;text-align:center;padding:1rem;font-size:1.5rem;\">LANGCHAIN OVERVIEW</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6bd9f8-efef-4797-ba5d-9eb135e6b862",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Basic features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da068c2f-26b2-4c5e-b72c-23a1d50c8543",
   "metadata": {},
   "source": [
    "---\n",
    "## Get prediction from a langage model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e8dd21-8ada-4cd4-a532-43c2e1c3fc63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "# OPENAI_API_KEY is requested. Get it from the OpenAI site.\n",
    "# a paid account and available units are requested to be able to place a request.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "text = \"what are the 5 best countries in Europe\"\n",
    "\n",
    "# Actual API call - may tale a while.\n",
    "print(llm(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17250fa0-fe5d-47c1-9406-9822dd38fc9e",
   "metadata": {},
   "source": [
    "---\n",
    "## Manage prompts with templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdf0d54-abaa-4aa6-aeb5-275bebb5246b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# setup a prompt\n",
    "prompt = PromptTemplate (\n",
    "    input_variables=[\"interest\"],\n",
    "    template=\"what are the 5 best countries in Europe ranked by {interest}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09393d7-6674-42f3-97c0-08cd12439d3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = prompt.format(interest=\"food\")\n",
    "print(f\"{text=}\")\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e3e1e6-871b-42e3-9106-96249b3f726e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = prompt.format(interest=\"siteseeing\")\n",
    "print(f\"{text=}\")\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db89d7fa-10d1-495d-8e89-24ab1dce738f",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ae61a5-823a-4585-b308-8e83ea8b6a7a",
   "metadata": {},
   "source": [
    "Chains are sequences of modular components (or other chains) combined in a particular way to accomplish a common use case.\n",
    "\n",
    "\n",
    "Example:\n",
    "- chaining LLM and tool\n",
    "- summarization chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d607ac-9d7f-40f3-86f4-243670f42fe2",
   "metadata": {},
   "source": [
    "---\n",
    "## Built-in chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d68f8d0-4ee4-48c5-a6b6-5f515c581eea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import PALChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "palchain = PALChain.from_math_prompt(llm=llm, verbose=True)\n",
    "\n",
    "\n",
    "text = \"\"\"If my age is half of my dad's age \n",
    "and he is going to be 60 next year, \n",
    "what is my current age?\"\"\"\n",
    "#palchain.run(\"If my age is half of my dad's age and he is going to be 60 next year, what is my current age?\")\n",
    "palchain.run(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ed407-348c-4c00-a405-c8233fe07bb4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "    TODO <br>\n",
    "    - different result each run <br>\n",
    "    - and should be 29.5\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8337725d-b79a-41b9-99f8-d7d55339c149",
   "metadata": {},
   "source": [
    "> Entering new  chain...\n",
    "def solution():\n",
    "    \"\"\"If my age is half of my dad's age and he is going to be 60 next year, what is my current age?\"\"\"\n",
    "    dad_age_next_year = 60\n",
    "    my_age_fraction = 0.5\n",
    "    my_age_now = dad_age_next_year * my_age_fraction\n",
    "    result = my_age_now\n",
    "    return result\n",
    "\n",
    "> Finished chain.\n",
    "'30.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8acdd9d-e9d5-4ca1-b3db-6e8b2a47d81b",
   "metadata": {},
   "source": [
    "> Entering new  chain...\n",
    "def solution():\n",
    "    \"\"\"If my age is half of my dad's age and he is going to be 60 next year, what is my current age?\"\"\"\n",
    "    dad_age_current = 59\n",
    "    my_age_current = dad_age_current / 2\n",
    "    result = my_age_current\n",
    "    return result\n",
    "\n",
    "> Finished chain.\n",
    "'29.5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b09463-2f2e-4840-9602-1a22478bdd5c",
   "metadata": {},
   "source": [
    "---\n",
    "## Multi-step workflow to feed prompt into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe1a4f0-4c08-47aa-9304-0b6a7002968b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# setup a prompt\n",
    "prompt = PromptTemplate (\n",
    "    input_variables=[\"interest\"],\n",
    "    template=\"what are the 5 best countries in Europe ranked on {interest}\"\n",
    ")\n",
    "\n",
    "# chain feeds the prompt into the langage mmodel.\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97622a8b-eb22-4218-803d-34d1a76c1af2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain.run(\"science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f73c33b-634d-48fb-9552-7329658c4e23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chain.run(\"tv shows\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c72f8b-7623-4e34-a2b2-c8bf785fa719",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Using OpenAI Chat API (less expensive)\n",
    "requires a chain to feed the prompt into the chat \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO  move to components + desribe resource </div>\n",
    "\n",
    "**Resources**\n",
    "> - Other Chat APIs: https://api.python.langchain.com/en/latest/modules/chat_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18f9ea2-c494-462a-aa3e-5e7ca5952d31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chatopenai = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "prompt = PromptTemplate (\n",
    "    input_variables=[\"interest\"],\n",
    "    template=\"what are the 5 best countries in Europe ranked on {interest}\"\n",
    ")\n",
    "\n",
    "llmchain_chat = LLMChain(llm=chatopenai, prompt=prompt)\n",
    "print(llmchain_chat.run(\"food\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3204392e-b1de-47f0-983e-ae05901179ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Leverage LLM Math\n",
    "\n",
    "Evaluating chains that know how to do math.\n",
    "\n",
    "**Resources**\n",
    "> - Langchain module LLM_Math: ttps://python.langchain.com/docs/guides/evaluation/llm_math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ed6528-f540-4b2f-9617-e5ae8ffe6d26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "from langchain.chains import LLMMathChain\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "prompt = load_prompt('lc://prompts/llm_math/prompt.json')\n",
    "\n",
    "# deprecated\n",
    "##chain = LLMMathChain(llm=llm, prompt=prompt)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "print(chain.run(\"what is the largest prime number lower than 20\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb52e51-3733-4ff5-a8cb-1094a391b98f",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Agent\n",
    "\n",
    "LangChain define agents as decision making engines:\n",
    "> Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an unknown chain that depends on the user's input. In these types of chains, there is a agent which has access to a suite of tools. Depending on the user input, the agent can then decide which, if any, of these tools to call.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61edc5bd-e7fa-49ab-85b1-cf08351ea471",
   "metadata": {},
   "source": [
    "---\n",
    "## Test with LLM model only \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb08bb0-a363-40d2-b119-0a2bd0805272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "# OPENAI_API_KEY is requested. Get it from the OpenAI site.\n",
    "# a paid account and available units are requested to be able to place a request.\n",
    "# low temperature to avoid randomness\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "text = \"Who is the prime minister of France since may 2022\"\n",
    "\n",
    "# Actual API call - may tale a while.\n",
    "print(llm(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af68d10d-eaf3-4f27-b102-7b28e62a9b20",
   "metadata": {},
   "source": [
    "**OUTPUT**\n",
    "\n",
    "'The Prime Minister of France since May 2022 is Jean Castex.'\n",
    "\n",
    "This answer is wrong. Since the model has been trained mid 2021, it is not up-to-date. Elisabeth Borne is Prime Minister since may 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c68946-559e-497d-a971-45c473ea5770",
   "metadata": {},
   "source": [
    "---\n",
    "## Agent leveraging Google Search\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "Make sure:\n",
    "- Google API client is installed\n",
    "- a Custome Search Engine is available (CSE)\n",
    "- the API key has been setup up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d462acdb-fff1-449f-98f6-4b93916dbaff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64311c2-8e52-4278-a4ac-42a22bceec08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# create a model\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# load some tools\n",
    "tools = load_tools([\"google-search\"], llm=llm)\n",
    "\n",
    "# setup an agent\n",
    "agent = initialize_agent(tools, \n",
    "                         llm, \n",
    "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "                         verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff155cf5-99f9-4ba4-8dbc-9d9366825475",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent.run(\"Who is the prime minister of France since may 2022\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb7aeec-170c-4491-8344-1191a1102a80",
   "metadata": {},
   "source": [
    "**OUTPUT**\n",
    "\n",
    "'lisabeth Borne is the prime minister of France since May 16, 2022.'\n",
    "\n",
    "This is true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28631b0b-d1ec-4d14-bf44-091f8abd1584",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Memory - Conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bde2f8-060f-4da3-a69f-24df8df6aa63",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO  what is a conversation </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13541ec0-684d-48da-a205-ef1c6e3e7937",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import OpenAI, ConversationChain\n",
    "\n",
    "# create a model\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "conversation.predict(input=\"Hi There\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7271c38f-0a3e-44fb-9d8a-1014667c3467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What is the first thing that I said to you?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2613f8b-beb3-47b3-b842-e5f2b4746143",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What is an alternative for the first thing that I said to you?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3912d8-050d-4c4a-89e8-1d9ce0a1f4cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "<div style=\"background-color:green;color:black;text-align:center;padding:1rem;font-size:1.5rem;\">LANGCHAIN COMPONENTS</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684914b4-7b16-4f0b-bde2-6ef6c7df5079",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Schemas\n",
    "\n",
    "Basic data types and schemas that are used throughout the codebase.\n",
    "\n",
    "There are 3 types of schemas\n",
    "- Text (see above)\n",
    "- Prompts\n",
    "- Messages \n",
    "- Document\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Resources**\n",
    "> - Schhemas component:  https://docs.langchain.com/docs/components/schema/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7bb2c3-a6cd-4fee-b6a8-964a5f811433",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8843ce9-1f71-4b43-a704-d456dc5653e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "# OPENAI_API_KEY is requested. Get it from the OpenAI site.\n",
    "# a paid account and available units are requested to be able to place a request.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "text = \"what are the 5 best countries in Europe\"\n",
    "\n",
    "# Actual API call - may tale a while.\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf610897-44b3-4d72-8e38-3dd50b0776bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Chat messages\n",
    "Chat messages are like text with a type\n",
    "\n",
    "There are 3 types\n",
    "- System: background context that tells the AI what to do\n",
    "- Human: inputs sent by the user\n",
    "- AI : response of the AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8d5a96-f105-45f2-ba15-0f0b108cd9ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7a0269-e45c-445b-b44f-b19fc4be5e53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [ SystemMessage(content=\"You are a nice AI and help users to feature out what to eat.\")]\n",
    "     \n",
    "messages.append( HumanMessage(content=\"I like tuna, list some recipes.\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e203228-09cd-4405-9679-c4eebfbd7c1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = chat(messages)\n",
    "messages.append( AIMessage(content=response.content) )\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d140f70f-a86d-4634-899e-12401f0315cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages.append( HumanMessage(content=\"show the first one.\") )\n",
    "\n",
    "response = chat(messages)\n",
    "messages.append( AIMessage(content=response.content) )\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af92d18-1d2b-4c55-aa71-e5a91fde59bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Examples\n",
    "An list of input output pairs thet represent the input and expected output.\n",
    "\n",
    "Used to fine tune a model or do in-context learning.\n",
    "\n",
    "**Resources**\n",
    "> - Prompt Template:  https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/few_shot_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca97e03b-8d6c-4156-8358-a9504901d733",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# create the example set\n",
    "\n",
    "examples = [\n",
    "    { \"question\": \"red bold\", \"answer\": \"color:red; font-style:bold;\"},\n",
    "    { \"question\": \"green italic\", \"answer\":  \"color:green; font-style:italic;\"},\n",
    "    { \"question\": \"blue bold\", \"answer\":  \"color:blue; font-style:bold;\"},\n",
    "    { \"question\": \"pink\", \"answer\":  \"color:pink;\"},\n",
    "    { \"question\": \"green\", \"answer\":  \"color:green;\"},\n",
    "    { \"question\": \"pink italic\", \"answer\":  \"color:pink; font-style:italic;\"}\n",
    "    \n",
    "]    \n",
    "\n",
    "# Configure a formatter that will format the few shot examples into a string. \n",
    "# This formatter should be a PromptTemplate object.\n",
    "\n",
    "example_prompt = PromptTemplate (\n",
    "    input_variables=[\"question\", \"answer\"], \n",
    "    template=\"question: {question}\\n{answer}\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== exemple prompt ===\")\n",
    "print(example_prompt.format(**examples[0]))\n",
    "\n",
    "\n",
    "# Finally, create a FewShotPromptTemplate object. \n",
    "# This object takes in the few shot examples and the formatter for the few shot examples.\n",
    "\n",
    "prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples, \n",
    "    example_prompt=example_prompt, \n",
    "    suffix=\"question: {input}\", \n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format(input=\"pink bold\")\n",
    "\n",
    "print(\"\\n=== prompt ===\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"\\n=== answer ===\")\n",
    "print(llm(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2471f9d0-6000-4f48-ad1a-f5e187a8aa54",
   "metadata": {},
   "source": [
    "---\n",
    "## Documents\n",
    "\n",
    "An unstructured object that conaints a pieces of text and metadatas.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO  resource </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648ce2f7-0d40-49ea-b0a8-6e45f9b5f2f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO how to use this concept? \n",
    "make some knowledge available?\n",
    "how to use metadata?\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097d47df-18e7-4a58-b76c-d7438d3135bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# temperature 0 means no randomness\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "\n",
    "document = Document(\n",
    "    page_content=\"\"\"\n",
    "\n",
    "        So she swallowed one of the cakes and was delighted to find that she\n",
    "        began shrinking directly. As soon as she was small enough to get through\n",
    "        the door, she ran out of the house and found quite a crowd of little\n",
    "        animals and birds waiting outside. They all made a rush at Alice the\n",
    "        moment she appeared, but she ran off as hard as she could and soon found\n",
    "        herself safe in a thick wood.\n",
    "        \"\"\",\n",
    "    metadata={\n",
    "        'author':\"Lewis Caroll\",\n",
    "        'identifier':\"1234\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Document\")\n",
    "print(document)\n",
    "\n",
    "# the attribute stuff instruct the run the chain once\n",
    "chain = load_summarize_chain(\n",
    "    llm, \n",
    "    chain_type=\"stuff\", \n",
    "    verbose=False)\n",
    "\n",
    "# run the chain against the documment\n",
    "summary = chain.run([document])\n",
    "    \n",
    "print(\"\\nSummary\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53092123-3262-48de-b082-e4bedd51432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Note, the default model is already 'text-davinci-003' \n",
    "# temperature 0 means no randomness\n",
    "llm = OpenAI(temperature=0, model_name='text-davinci-003')\n",
    "\n",
    "# check the number of tokens\n",
    "num_tokens = llm.get_num_tokens(text_sample)\n",
    "print(f\"{num_tokens=}\")\n",
    "\n",
    "# build a document reuse text sampke above\n",
    "doc = Document(\n",
    "    page_content=text_sample,\n",
    "    metadata={\n",
    "        'author':\"Lewis Caroll\",\n",
    "        'title':\"Alice in Wonderland\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# chain expect a list of documents\n",
    "docs = [doc]\n",
    "\n",
    "# setup. a custom prompt\n",
    "# a defaukt one is provide: write a concise summary\n",
    "prompt_template = \"\"\"\n",
    "%INSTRUCTIONS:\n",
    "Please summarize the following piece of text.\n",
    "Respond in a manner that a 5 year old would understand.\n",
    "\n",
    "%TEXT:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "# the attribute stuff instruct the run the chain once\n",
    "chain = load_summarize_chain(\n",
    "    llm, \n",
    "    chain_type=\"stuff\", \n",
    "    prompt=prompt, \n",
    "    verbose=False)\n",
    "\n",
    "# run the chain against the documment\n",
    "summary = chain.run(docs)\n",
    "    \n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee98eca-37a3-45c4-90e5-eb2360e1f5e1",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Models\n",
    "LangChain provides interfaces and integrations for two types of models:\n",
    "- LLMs: Models that take a text string as input and return a text string\n",
    "- Chat models: Models that are backed by a language model but take a list of Chat Messages as input and return a Chat Message\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Resources**\n",
    "> - Model Component: https://python.langchain.com/docs/modules/model_io/models/\n",
    "> - List of models: https://platform.openai.com/docs/models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6a1df5-7967-4841-b1eb-1ff58d0a15cb",
   "metadata": {},
   "source": [
    "---\n",
    "## Langage Model \n",
    "LLMs: Models that take a text string as input and return a text string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd6cc95-2731-47d9-8624-2392e3a83315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# additnal parameters to select a mode, pass the API key ...\n",
    "llm = OpenAI(model_name=\"text-ada-001\", temperature=0.7)\n",
    "\n",
    "llm(\"What day comes after Friday?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981bf185-6a93-4eb5-bdc4-779724aab952",
   "metadata": {},
   "source": [
    "---\n",
    "## Chat Model \n",
    "Chat models: Models that are backed by a language model but take a list of Chat Messages as input and return a Chat \n",
    "\n",
    "Also make sense for a unique interaction as Chat API is less expensive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2099b96-d00a-4aa0-908d-e75a0e4b3a16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daafb170-e231-4132-bcc2-c88aac2fe475",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [ \n",
    "    SystemMessage(content=\"You are a nice AI and help users to feature out what to eat.\"),\n",
    "    HumanMessage(content=\"I like tuna, list some recipes.\")\n",
    "]\n",
    "     \n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91de6b77-6cb7-414b-a475-c471c3ca6c5b",
   "metadata": {},
   "source": [
    "---\n",
    "### Text Embedding Model\n",
    "\n",
    "Convert text into a series of numbers (a vector) which holds the meaning of the text.\n",
    "\n",
    "Mainly used for text comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7d5813-a505-47d6-a8ef-98881a175c46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "text=\"A leader should know all about truth and honesty, and when to see the difference. (Truck) - Bromeliad Trilogy\"\n",
    "\n",
    "text_embedding = embeddings.embed_query(text)\n",
    "\n",
    "print(f\"embedding length: {len(text_embedding)}\")\n",
    "print(f\"5 first values of the vector: {text_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab0bbcc-adc2-449f-97e4-839a36b3d4f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# 7. prompts\n",
    "A \"prompt\" refers to the input to the model. This input is rarely hard coded, but rather is often constructed from multiple components. A PromptTemplate is responsible for the construction of this input. LangChain provides several classes and functions to make constructing and working with prompts easy.\n",
    "\n",
    "LangChain documentation is split into four sections:\n",
    "- PromptValue: The class representing an input to a model.\n",
    "- Prompt Templates: The class in charge of constructing a PromptValue.\n",
    "- Example Selectors: Often times it is useful to include examples in prompts. These examples can be hardcoded, but it is often more powerful if they are dynamically selected.\n",
    "- Output Parsers: Language models (and Chat Models) output text. But many times you may want to get more structured information than just text back. This is where output parsers come in. Output Parsers are responsible for (1) instructing the model how output should be formatted, (2) parsing output into the desired formatting (including retrying if necessary).\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Resources**\n",
    "> - Prompts Component: https://docs.langchain.com/docs/components/prompts/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc36ebe4-aba6-4cdb-850d-1937839c8608",
   "metadata": {},
   "source": [
    "---\n",
    "## Simple prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37b8bcc-33ca-4deb-9845-81675d529758",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0.9)\n",
    "\n",
    "# write a simple  prompt. use \"\"\" to allow multiline string.\n",
    "prompt = \"\"\"\n",
    "Today is Monday. Tomorrow is Wednesday.\n",
    "\n",
    "What is wrong with this statement?\n",
    "\"\"\"\n",
    "\n",
    "# query the model\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f8a344-70ab-471d-918e-ee98e7e4fdf1",
   "metadata": {},
   "source": [
    "---\n",
    "## Prompt with template and placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837116ab-bf8b-49ed-bb3f-a896e14fb14d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0.9)\n",
    "\n",
    "# setup a prompt. use \"\"\" to allow multiline string.\n",
    "template = PromptTemplate (\n",
    "    input_variables=[\"today\", \"tomorrow\"],\n",
    "    template=\"\"\"\n",
    "    Today is {today}. Tomorrow is {tomorrow}.\n",
    "\n",
    "    What is wrong with this statement?\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "prompt = template.format(today=\"Monday\", tomorrow=\"Wednesday\")\n",
    "print(f\"{prompt=}\")\n",
    "\n",
    "# query the model\n",
    "\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa35aff7-3105-4aed-82ab-7281c517eca3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = template.format(today=\"Thursday\", tomorrow=\"Friday\")\n",
    "print(f\"{prompt=}\")\n",
    "\n",
    "# query the model\n",
    "\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc54ed33-925e-405f-8eaf-a4a57ed78771",
   "metadata": {},
   "source": [
    "---\n",
    "## Example selectors and Few Shot Learning\n",
    "\n",
    "A way to select from a series of examples in few shot learning \n",
    "\n",
    "**Resources**\n",
    "> - Example Selector: https://api.python.langchain.com/en/latest/modules/example_selector.html\n",
    "> - Few shot learning: https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/few_shot_examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e40343-df4e-4b65-8951-8372060ca6d0",
   "metadata": {},
   "source": [
    "### Example selectors and Few Shot Learning with NGram\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> FIXME </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39afd4e6-4d1d-4348-b871-343e34cb0314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.example_selector import NGramOverlapExampleSelector\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# create the example set\n",
    "\n",
    "examples = [\n",
    "    { \"question\": \"red bold\", \"answer\": \"color:red; font-style:bold;\"},\n",
    "    { \"question\": \"green italic\", \"answer\":  \"color:green; font-style:italic;\"},\n",
    "    { \"question\": \"blue bold\", \"answer\":  \"color:blue; font-style:bold;\"},\n",
    "    { \"question\": \"pink\", \"answer\":  \"color:pink;\"},\n",
    "    { \"question\": \"green\", \"answer\":  \"color:green;\"},\n",
    "    { \"question\": \"pink italic\", \"answer\":  \"color:pink; font-style:italic;\"}\n",
    "    \n",
    "]    \n",
    "\n",
    "# Configure a formatter that will format the few shot examples into a string. \n",
    "# This formatter should be a PromptTemplate object.\n",
    "\n",
    "example_prompt = PromptTemplate (\n",
    "    input_variables=[\"question\", \"answer\"], \n",
    "    template=\"question: {question}\\n{answer}\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== exemple prompt ===\")\n",
    "print(example_prompt.format(**examples[0]))\n",
    "\n",
    "\n",
    "# Select and order examples based on ngram overlap score (sentence_bleu score).\n",
    "\n",
    "question = \"pink bold\"\n",
    "\n",
    "example_selector = NGramOverlapExampleSelector.select_examples(\n",
    "    examples,\n",
    "    question\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # This is the list of examples available to select from.\n",
    "    examples,\n",
    "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    OpenAIEmbeddings(),\n",
    "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    Chroma,\n",
    "    # This is the number of examples to produce.\n",
    "    k=1\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Finally, create a FewShotPromptTemplate object. \n",
    "# This object takes in the few shot examples and the formatter for the few shot examples.\n",
    "\n",
    "prompt_template = FewShotPromptTemplate(\n",
    "    #example_selector=example_selector, \n",
    "    examples=selected_examples, \n",
    "    example_prompt=example_prompt, \n",
    "    suffix=\"question: {input}\", \n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format(input=question)\n",
    "\n",
    "print(\"\\n=== prompt ===\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"\\n=== answer ===\")\n",
    "print(llm(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be5012c-2708-417f-a6e8-4a1beb5349df",
   "metadata": {},
   "source": [
    "### Example selectors and Few Shot Learning with similarities\n",
    "\n",
    "requires a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6e5756-6345-4b70-844b-f7959fe0d70e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Annoy\n",
    "#from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# create the example set\n",
    "\n",
    "examples = [\n",
    "    { \"question\": \"red bold\", \"answer\": \"color:red; font-style:bold;\"},\n",
    "    { \"question\": \"green italic\", \"answer\":  \"color:green; font-style:italic;\"},\n",
    "    { \"question\": \"blue bold\", \"answer\":  \"color:blue; font-style:bold;\"},\n",
    "    { \"question\": \"pink\", \"answer\":  \"color:pink;\"},\n",
    "    { \"question\": \"green\", \"answer\":  \"color:green;\"},\n",
    "    { \"question\": \"pink italic\", \"answer\":  \"color:pink; font-style:italic;\"}\n",
    "    \n",
    "]    \n",
    "\n",
    "# Configure a formatter that will format the few shot examples into a string. \n",
    "# This formatter should be a PromptTemplate object.\n",
    "\n",
    "example_prompt = PromptTemplate (\n",
    "    input_variables=[\"question\", \"answer\"], \n",
    "    template=\"question: {question}\\n{answer}\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== exemple prompt ===\")\n",
    "print(example_prompt.format(**examples[0]))\n",
    "\n",
    "# Example selector that selects examples based on SemanticSimilarity.\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # This is the list of examples available to select from.\n",
    "    examples,\n",
    "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    OpenAIEmbeddings(),\n",
    "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    #Chroma,\n",
    "    Annoy,\n",
    "    # This is the number of examples to produce.\n",
    "    k=2\n",
    ")\n",
    "\n",
    "# Finally, create a FewShotPromptTemplate object. \n",
    "# This object takes in the few shot examples and the formatter for the few shot examples.\n",
    "\n",
    "prompt_template = FewShotPromptTemplate(\n",
    "    example_selector=example_selector, \n",
    "    example_prompt=example_prompt, \n",
    "    suffix=\"question: {input}\", \n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format(input=\"pink bold\")\n",
    "\n",
    "print(\"\\n=== prompt ===\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"\\n=== answer ===\")\n",
    "print(llm(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2b5aae-d6c0-4ebe-a12e-521b2f807045",
   "metadata": {},
   "source": [
    "---\n",
    "## Output Parser and response format\n",
    "\n",
    "A way to format the outpu\n",
    "- Format nstructions: An autogenerated prompt telling how the result should be formatted\n",
    "- parser: a method which will extract the output int hte desired format. you may prvie a custom parser\n",
    "\n",
    "\n",
    "**Resources**\n",
    "> - OutputParser:https://docs.langchain.com/docs/components/prompts/output-parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "46e0cc93-9499-4ac7-a3b2-669bb3f121e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "format_instructions\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This is a poorly formatted string.\n",
      "\t\"good_string\": string  // This is a your string reformatted.\n",
      "}\n",
      "```\n",
      "\n",
      "prompt\n",
      "\n",
      "You will be given a poorly formatted string from a user. \n",
      "Reformat it and make sure all the words are spelled correctly.\n",
      "\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This is a poorly formatted string.\n",
      "\t\"good_string\": string  // This is a your string reformatted.\n",
      "}\n",
      "```\n",
      "\n",
      "% USER_INPUT:\n",
      "Wellcom to Californya!\n",
      "\n",
      "YOUR RESPONSE:\n",
      "\n",
      "\n",
      "response=\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": \"Wellcom to Californya!\",\n",
      "\t\"good_string\": \"Welcome to California!\"\n",
      "}\n",
      "```\n",
      "\n",
      "parsed output=\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bad_string': 'Wellcom to Californya!',\n",
       " 'good_string': 'Welcome to California!'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0.9)\n",
    "\n",
    "# how you would like the response to be structured\n",
    "# periods at the send of sentence are required. \n",
    "# If not there description ends up in the json text and break the JSON format\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"bad_string\", description=\"This is a poorly formatted string.\"),\n",
    "    ResponseSchema(name=\"good_string\", description=\"This is a your string reformatted.\")\n",
    "]\n",
    "\n",
    "# How you would like to parse your output\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# check instructions\n",
    "format_instructions =output_parser.get_format_instructions()\n",
    "print(\"\\nformat_instructions\")      \n",
    "print(format_instructions)      \n",
    "\n",
    "template = \"\"\"\n",
    "You will be given a poorly formatted string from a user. \n",
    "Reformat it and make sure all the words are spelled correctly.\n",
    "\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER_INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=['user_input'],\n",
    "    partial_variables={'format_instructions': format_instructions},\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# format the user input as a prompt\n",
    "# for whateveer reason it does not work well with format.\n",
    "# format_promt retruns an object, not a string and should be converted to a string \n",
    "prompt = prompt_template.format_prompt(user_input=\"Wellcom to Californya!\").to_string()\n",
    "print(\"\\nprompt\")\n",
    "print(prompt)\n",
    "\n",
    "# gets the response\n",
    "response = llm(prompt)\n",
    "print(\"\\nresponse=\")      \n",
    "print(response)      \n",
    "\n",
    "# gets the JSON document\n",
    "print(\"\\nparsed output=\")     \n",
    "\n",
    "# comma sometimes missing\n",
    "response.replace('\"good_string\"',',\"good_string\"')\n",
    "\n",
    "output_parser.parse(response)                   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3bc48a-c126-4f79-b8d8-5f65b3387ecd",
   "metadata": {},
   "source": [
    "---\n",
    "# 8. Indexes\n",
    "\n",
    "Indexes refer to ways to structure documents so that LLMs can best interact with them. This module contains utility functions for working with documents, different types of indexes, and then examples for using those indexes in chains.\n",
    "\n",
    "LangChain documentation is split into four sections:\n",
    "\n",
    "- Document Loaders: Classes responsible for loading documents from various sources.\n",
    "- Text Splitters: Classes responsible for splitting text into smaller chunks.\n",
    "- VectorStores: The most common type of index. One that relies on embeddings.\n",
    "- Retrievers: Interface for fetching relevant documents to combine with language models.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Resource**\n",
    "> - Indexes Component: https://docs.langchain.com/docs/components/indexing/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdab962-d2e2-40c9-bcc7-26a07996a493",
   "metadata": {},
   "source": [
    "**Instructions**\n",
    "\n",
    "For the example below, make sure that:\n",
    "- a vector database client is installed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7a68d6-2561-4630-89b0-f543d3880746",
   "metadata": {},
   "source": [
    "---\n",
    "## Document Loaders\n",
    "\n",
    "Easy ways to import documents from other sources \n",
    "and make it available for use in your language models.\n",
    "\n",
    "**Resources**\n",
    "> -  Document Loaders: https://python.langchain.com/docs/modules/data_connection/document_loaders\n",
    "> - List of loaders: https://github.com/hwchase17/langchain/tree/master/langchain/document_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77127064-7c75-4561-81a7-a3d73f08cb44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import HNLoader\n",
    " \n",
    "# Setup a Hacker News loader\n",
    "loader = HNLoader(\"https://news.ycombinator.com/item?id=34422627\")\n",
    " \n",
    "data = loader.load()\n",
    " \n",
    "print(f\"Found {len(data)} comments\")\n",
    "\n",
    "\n",
    "sample = '\\n'.join([x.page_content[:100] for x in data[:2]])\n",
    "print(\"\\nHere's a sample (first 100 chars of the 3 first items)\")\n",
    "print(sample)\n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd41d74-2bd1-4d5c-8409-b27c6cdb5d1a",
   "metadata": {},
   "source": [
    "---\n",
    "## Text Splitters\n",
    "\n",
    "allow you to split a document into smaller chunk\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO  resource </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32da233-903e-4ae2-8c5e-42cd0c6f3d54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import BSHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    " \n",
    "# This is a long document we can split up.    \n",
    "document_path = \"data/Alice's Adventures in Wonderland, by Lewis Carroll.html\"\n",
    " \n",
    "# Setup a HTML loader\n",
    "loader = BSHTMLLoader(document_path)\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Found {len(documents)} document(s)\")\n",
    "\n",
    "\n",
    "print(\"docuument content\")\n",
    "start = 2200\n",
    "print(documents[0].page_content[start-200:start+300])\n",
    "\n",
    " \n",
    "# The recommended TextSplitter is the RecursiveCharacterTextSplitter. \n",
    "# This will split documents recursively by different characters - starting with \"\\n\\n\", then \"\\n\", then \" \".\n",
    "# This is nice because it will try to keep all the semantically relevant content in the same place \n",
    "# for as long as possible.\n",
    "# Important parameters to know here are chunkSize and chunkOverlap. \n",
    "# chunkSize controls the max size (in terms of number of characters) of the final documents. \n",
    "# chunkOverlap specifies how much overlap there should be between chunks. \n",
    "# in practice they default to 4000 and 200 respectively.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    " \n",
    "texts = text_splitter.create_documents([document[0].page_content])\n",
    " \n",
    "print(f\"\\nSplitted into {len(texts)} parts\")\n",
    " \n",
    "print(\"Preview:\")\n",
    "i = int(start/150)\n",
    "print(texts[i+1].page_content, \"\\n-\")\n",
    "print(texts[i+2].page_content, \"\\n-\")\n",
    "print(texts[i+3].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a947a9d1-e893-4ac8-b854-e5a777363c27",
   "metadata": {},
   "source": [
    "---\n",
    "## Vextor Store and Retrievers \n",
    "A retriever is an interface that returns documents given an unstructured query. \n",
    "\n",
    "A retriever does not need to be able to store documents, only to return (or retrieve) it. \n",
    "\n",
    "It usually relies to a vector store as a document management backbone.\n",
    "\n",
    "A vector store is a particular type of database optimized for storing documents and their embeddings, and then fetching of the most relevant documents for a particular query, ie. those whose embeddings are most similar to the embedding of the query.\n",
    "\n",
    "- local : ChromaDB, FAISS, Annoy\n",
    "- Online: Pinecone, Weaviate\n",
    "\n",
    "However a retriever is more general than a vector store and there are other types of retrievers as well, e.g. Wikipedia or search engines like Elastic Search or Kendra.\n",
    "\n",
    "\n",
    "Question answering over documents consists of four steps:\n",
    "1. Create an index\n",
    "2. Create a Retriever from that index\n",
    "3. Create a question answering chain\n",
    "4. Ask questions\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Resources**\n",
    "> - Lit of retrievers: https://python.langchain.com/docs/modules/data_connection/retrievers/\n",
    "> - LangChain Supported VectorStores: https://api.python.langchain.com/en/latest/modules/vectorstores.html\n",
    "> - Retrievers: https://github.com/hwchase17/langchain/tree/master/langchain/retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3769182e-3197-4cc3-8ab4-d05717fb2922",
   "metadata": {},
   "source": [
    "### Store document in a Vector Store and retrieve information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedfae67-fba3-4e5d-ac09-d2d6b465dcb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import BSHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Annoy\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    " \n",
    "# This is the source document.    \n",
    "document_path = \"data/Alice's Adventures in Wonderland, by Lewis Carroll.html\"\n",
    " \n",
    "# Setup a HTML loader\n",
    "loader = BSHTMLLoader(document_path)\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Found {len(documents)} document(s)\")\n",
    "\n",
    "\n",
    "# Get your splitter ready\n",
    "# Using small chunk for the sake of example. \n",
    "# in practice they default to 4000 and 200 respectively.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=25)\n",
    " \n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"\\nSplitted into {len(texts)} parts\")\n",
    "\n",
    "# Get embedding engine ready\n",
    "embeddings = OpenAIEmbeddings()\n",
    " \n",
    "# Embedd your texts andd store them in the vector database\n",
    "# dtabase is in memory. it might be savecd to a file and loader later on.\n",
    "db = Annoy.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e39c41-6df4-445f-8314-db8d7c49624a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Init a retriever for this db\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# retrieve indexed documents relevant for the query\n",
    "query = \"who is the White Rabbit?\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "print(f\"\\nFound {len(docs)}\")\n",
    "\n",
    "samples = \"\\n\\n\".join([x.page_content[:200] for x in docs[:5]])\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448aa357-9d47-4db2-af62-cd4acd58bd78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Asking theLLM\n",
    "# the response will be based on the retrieved documents \n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97504cf-53c0-4a59-848d-459279b1499f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a99a8f-58db-4ef3-b75e-0f584222f7c1",
   "metadata": {},
   "source": [
    "### Save and load db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fb324b-98dc-449d-8906-ebd536635ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Annoy\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "docstore_file_path = \"alice_docstore\"\n",
    "\n",
    "db.save_local(docstore_file_path)\n",
    "\n",
    "loaded_vector_store = Annoy.load_local(\n",
    "   docstore_file_path, embeddings=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# same document similar to White Red abbit\n",
    "loaded_vector_store.similarity_search_with_score(\"White Rabbit\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ee55b6-6c70-48d0-862b-8a411fc63b57",
   "metadata": {},
   "source": [
    "### One line index creation and information retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c90307a-e8a0-4cb1-a514-38cd52f74685",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import BSHTMLLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Annoy\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "# This is the source document.    \n",
    "document_path = \"data/Alice's Adventures in Wonderland, by Lewis Carroll.html\"\n",
    " \n",
    "# Setup a HTML loader\n",
    "loader = BSHTMLLoader(document_path)\n",
    "\n",
    "# creating an indexer\n",
    "# default to Chroma as a vector database\n",
    "# Use CharacterTextSplitter. May also be RecursiveCharacterTextSplitter.\n",
    "index_creator = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=Annoy,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    text_splitter=CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    ")\n",
    "\n",
    "index = index_creator.from_loaders([loader])\n",
    "\n",
    "# retrieve indexed documents relevant for the query\n",
    "query = \"who is the White Rabbit?\"\n",
    "index.query(query)\n",
    "\n",
    "print(f\"\\nFound {len(docs)}\")\n",
    "\n",
    "samples = \"\\n\\n\".join([x.page_content[:200] for x in docs[:5]])\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27cfddd-ed2d-4892-a521-aa473ccd2c67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Ask the question to the model \n",
    "# the response will be based on the retrieved documents \n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), \n",
    "                                 chain_type=\"stuff\", \n",
    "                                 retriever=index.vectorstore.as_retriever())\n",
    "\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de6f5a1-9765-424c-819c-b79ccb8acc97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0081edf7-2762-4667-895f-5849b15424ea",
   "metadata": {},
   "source": [
    "---\n",
    "## Wikipedia retriever\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO wikipedia retriever </div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> \n",
    "    Move to tools agent_excutor example  <br>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272625be-5241-4278-9c8f-85a4d81cfed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "\n",
    "# model_name='gpt-4'\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "wikipedia = WikipediaAPIWrapper()\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Wikipedia\",\n",
    "        func=wikipedia.run,\n",
    "        description=\"Useful for when you need to get information from wikipedia about a single topic\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "agent_executor = initialize_agent(tools, llm, agent='zero-shot-react-description', verbose=True)\n",
    "\n",
    "output = agent_executor.run(\"Can you please provide a quick summary of Napoleon Bonaparte? \\\n",
    "                          Then do a separate search and tell me what the commonalities are with Serena Williams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a7a9c8-d0bc-4a5c-9f9d-fff2f870d0ed",
   "metadata": {},
   "source": [
    "---\n",
    "# 9. Memory\n",
    "\n",
    "\n",
    "Memory is the concept of storing and retrieving data in the process of a conversation. \n",
    "\n",
    "There are two main methods:\n",
    "- Based on input, fetch any relevant pieces of data\n",
    "- Based on the input and output, update state accordingly\n",
    "\n",
    "There are two main types of memory: short term and long term.\n",
    "- Short term memory generally refers to how to pass data in the context of a singular conversation (generally is previous ChatMessages or summaries of them).\n",
    "- Long term memory deals with how to fetch and update information between conversations.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Resource**\n",
    "> - Memory Component: https://docs.langchain.com/docs/components/memory/\n",
    "> - Chat Message History: https://docs.langchain.com/docs/components/memory/chat_message_history\n",
    "> - [LangChain: Enhancing Performance with Memory Capacity](https://towardsdatascience.com/langchain-enhancing-performance-with-memory-capacity-c7168e097f81)\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO vs Conversation and buffer memory (check blog)?</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO Long term memory</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ed82a7-7392-4637-85f2-a85b86d40378",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from pprint import pprint\n",
    " \n",
    "chat = ChatOpenAI(temperature=0)\n",
    " \n",
    "history = ChatMessageHistory()\n",
    " \n",
    "history.add_ai_message(\"hi!\")\n",
    " \n",
    "history.add_user_message(\"what is the capital of france?\")\n",
    "\n",
    "#After adding messages to the history, you can pass this history to the language model \n",
    "#to generate context-aware responses:\n",
    "\n",
    "ai_response = chat(history.messages)\n",
    "history.add_ai_message(ai_response.content)\n",
    "\n",
    "print(f\"{ai_response=}\")\n",
    "print(f\"\\nhistory.messages:\")\n",
    "pprint(history.messages, compact=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbd121c-3290-4133-a9c6-c55113fd139f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history.add_user_message(\"what is the population os this city?\")\n",
    "\n",
    "ai_response = chat(history.messages)\n",
    "history.add_ai_message(ai_response.content)\n",
    "\n",
    "print(f\"{ai_response.content=}\")\n",
    "print(f\"\\nhistory.messages:\")\n",
    "pprint(history.messages, compact=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc922de-52de-4c30-8faa-4feded2137d2",
   "metadata": {},
   "source": [
    "---\n",
    "# 10. Chains\n",
    "Chains are sequences of modular components (or other chains) combined in a particular way to accomplish a common use case.\n",
    "\n",
    "\n",
    "Example:\n",
    "- chaining LLM and tool\n",
    "- summarization chain\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Resources**\n",
    "> - Chain Component: https://docs.langchain.com/docs/components/chains/\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO index related chain https://docs.langchain.com/docs/components/chains/index_related_chains  </div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ede334-ad80-49fe-82aa-c9b22c12ac8d",
   "metadata": {},
   "source": [
    "## Simple sequential model\n",
    "\n",
    "A Simple Sequential Chain helps break up tasks to avoid language models getting distracted, confused, or hallucinating when asked to perform too many tasks in a row.\n",
    "\n",
    "In this example, the chain first receives the user location (Rome) and outputs a classic dish from Rome. Then, it provides a simple recipe for that classic dish. The verbose=True parameter ensures that the chain prints statements during its execution, making it easier to debug and understand the chains progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b15c453-5b4a-4857-a232-0ddbfe12067e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain\n",
    " \n",
    "# Cretae a model with high randomness\n",
    "llm = OpenAI(temperature=1)\n",
    " \n",
    "# Step 1 - dish for location\n",
    "\n",
    "template = \"\"\"\n",
    "Your job is to come up with a classic dish from the area that the users suggests. \n",
    "\n",
    "% USER LOCATION {user_location} \n",
    "\n",
    "YOUR RESPONSE: \n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_location\"], template=template)\n",
    "\n",
    "location_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    " \n",
    "\n",
    "# Step 2 - Recipe\n",
    "template = \"\"\"\n",
    "Given a meal, give a short and simple recipe on how to make that dish at home. \n",
    "\n",
    "% MEAL {user_meal} \n",
    "\n",
    "YOUR RESPONSE: \n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
    " \n",
    "meal_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# chain the steps\n",
    "# set verbose to True to check what happes\n",
    "overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=False)\n",
    " \n",
    "review = overall_chain.run(\"Rome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cec1cc-9ce8-4ddc-82c1-69d3a60c977b",
   "metadata": {},
   "source": [
    "## Summarization Chain\n",
    "\n",
    "The Summarization Chain breaks the text into smaller chunks and summarizing each chunk, creating a final summary based on the individual summaries.\n",
    "\n",
    "In this example, the chain first splits the essay into chunks of 700 characters. It then generates summaries for each chunk and creates a final concise summary based on these individual summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cfde91-71be-42ed-91d1-d3154f8e4ab3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import BSHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Cretae a model with low randomness\n",
    "llm = OpenAI(temperature=1)\n",
    "\n",
    "# This is the source document.    \n",
    "document_path = \"data/Alice's Adventures in Wonderland, by Lewis Carroll.html\"\n",
    " \n",
    "# Setup a HTML loader\n",
    "loader = BSHTMLLoader(document_path)\n",
    "documents = loader.load()\n",
    " \n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=50)\n",
    " \n",
    "# Split your docs into texts\n",
    "# only kept first 1 000 characters of the document to save computing\n",
    "texts = text_splitter.split_documents(documents[:1000])\n",
    " \n",
    "# There is a lot of complexity hidden in this one line. \n",
    "# the attribute map_reduce instruct the chain to \n",
    "# - first apply the model to each chunck (map stage) \n",
    "# - then all map results and apply the model (reduce stage)\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)\n",
    "summary = chain.run(texts)\n",
    "    \n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4996a3b2-f9d0-496a-abf4-b28fcb300b5e",
   "metadata": {},
   "source": [
    "**OUTPUT**\n",
    " \n",
    "Some map summaries\n",
    "\n",
    "> Alice hears the White Rabbit muttering to itself, concerned that the Duchess will execute it for losing the fan and pair of white kid-gloves. Alice offers to help the Rabbit search for them, but they are nowhere to be found because everything has changed since Alice's dip in the pool.\n",
    "\n",
    "> Alice meets a Rabbit who accuses her of being his housemaid Mary Ann and orders her to fetch his gloves and fan. She finds a neat little house with the Rabbit's name on a brass plate and goes in without knocking. She is afraid of meeting the real Mary Ann before she can find the fan and gloves.\n",
    "\n",
    "> Alice finds her way into a room with a table in the window, containing a fan and some gloves. She notices a bottle and drinks from it, hoping it will make her grow large again. When she drinks half of the bottle she finds her head pressing against the ceiling, so she hastily puts it down.\n",
    "\n",
    "> A character wishes she wouldn't grow anymore, but sadly she continues to grow rapidly. As a result, she kneels on the floor, puts her arm out the window and her foot up the chimney, and is uncertain of her fate.\n",
    "\n",
    " \n",
    "Final summary\n",
    "\n",
    "> In Lewis Carroll's Alice's Adventures in Wonderland, Alice follows a White Rabbit into a strange world and has to navigate unexpected events and peculiar characters. She eventually meets a Caterpillar who helps her regain control of her changing size. Project Gutenberg is a non-profit organization committed to making electronic books free to the public. Donations up to $5,000 are available, and the full license stipulates amounts and terms of use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80463e50-7e7f-49ed-9596-cdbef0250ced",
   "metadata": {},
   "source": [
    "## Summarize stored documents\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO  make use of the vector db</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706f5599-b76f-45e3-a8fe-566f0ea9b328",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 11. Agents\n",
    "\n",
    "LangChain define agents as decision making engines:\n",
    "> Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an unknown chain that depends on the user's input. In these types of chains, there is a agent which has access to a suite of tools. Depending on the user input, the agent can then decide which, if any, of these tools to call.\n",
    "\n",
    "It splits the documentation into the following sections:\n",
    "> - Tools: How language models interact with other resources.\n",
    "> - Agents: The language model that drives decision making.\n",
    "> - Toolkits: Sets of tools that when used together can accomplish a specific task.\n",
    "> - Agent Executor: The logic for running agents with tools.\n",
    "\n",
    "\n",
    "**Resources**\n",
    "> - Agents: https://docs.langchain.com/docs/components/agents/\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c830fd4-b796-4226-b087-1cef2bdd7efb",
   "metadata": {},
   "source": [
    "## Tool\n",
    "Tools are interfaces an agent can call to interact with other services\n",
    "\n",
    "**Resources**\n",
    "> - Tools: https://python.langchain.com/docs/modules/agents/tools/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a158512-085d-4782-bc72-8575f83630a2",
   "metadata": {},
   "source": [
    "**Instructions**\n",
    "\n",
    "For the example below, make sure that:\n",
    "- Google API client is installed\n",
    "- a Custome Search Engine is available (CSE)\n",
    "- the API key has been setup up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce2ddb0-06b1-4703-9b96-093a66b921da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "search = GoogleSearchAPIWrapper()\n",
    "\n",
    "tool = Tool(\n",
    "    name=\"Google Search\",\n",
    "    description=\"Search Google for recent results.\",\n",
    "    func=search.run,\n",
    ")\n",
    "\n",
    "tool.run(\"Who is the French Prime Minister name since May 2022?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e130c84-a1f4-4621-93c9-5e2e1632da08",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Agent leveraging tools\n",
    "\n",
    "Google Search and LLM-math are predefined tools:\n",
    "- LLM-Math is a langage model trained to do math logic.\n",
    "- Google)search tool allow to place queries on Google Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4811ff27-c48a-4c9e-a5a1-161dde97802d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# create a model\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# load some tools\n",
    "tools = load_tools([\"google-search\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# setup an agent\n",
    "agent = initialize_agent(tools, \n",
    "                         llm, \n",
    "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "                         verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a780cbf-a82f-4e8d-95c1-3d89f04e59b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent.run(\"How many Teslas have been sold in 2022. Multiple by 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a201cd59-8deb-43b5-a547-172ed6233ffe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent.run(\"Multiply by 2 the population of the capital of Frannce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016567b4-fb8e-4260-836a-8cd8fd47ecf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent.run(\"\"\"Who is the current prime minister of France. \n",
    "Is he or she younger than the President?\"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0345f8ca-a107-4999-ac88-8286c70044a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    # too complex\n",
    "    # either fails because it tries to add dates and nulber\n",
    "    # or give weird results like\n",
    "    # 'lisabeth Borne will be 70 in the year 2215.'\n",
    "    agent.run(\"\"\"Who is the current prime minister of France. \n",
    "    When will he or she be 70?\"\"\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d0b205-68f8-4ad4-b0be-c47e3107b836",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"background-color:green;color:black;text-align:center;padding:1rem;font-size:1.5rem;\">LANGCHAIN USE CASES</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a119b54e-5cc1-4069-9175-dda6bda8f422",
   "metadata": {},
   "source": [
    "---\n",
    "# [UC] 1. Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8567af48-3fea-45ba-b915-0dc9c50fddfc",
   "metadata": {},
   "source": [
    "---\n",
    "## Summaries Of Short Text\n",
    "Just write a summarization prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfe6ee3-0eed-4af6-8300-7f28b766ed81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# text to be summarized\n",
    "text_sample = \"\"\"\n",
    "The first thing she heard was a general chorus of \"There goes Bill!\"\n",
    "then the Rabbit's voice alone\"Catch him, you by the hedge!\" Then\n",
    "silence and then another confusion of voices\"Hold up his headBrandy\n",
    "nowDon't choke himWhat happened to you?\"\n",
    "\n",
    "Last came a little feeble, squeaking voice, \"Well, I hardly knowNo\n",
    "more, thank ye. I'm better nowall I know is, something comes at me\n",
    "like a Jack-in-the-box and up I goes like a sky-rocket!\"\n",
    "\n",
    "After a minute or two of silence, they began moving about again, and\n",
    "Alice heard the Rabbit say, \"A barrowful will do, to begin with.\"\n",
    "\n",
    "\"A barrowful of what?\" thought Alice. But she had not long to doubt,\n",
    "for the next moment a shower of little pebbles came rattling in at the\n",
    "window and some of them hit her in the face. Alice noticed, with some\n",
    "surprise, that the pebbles were all turning into little cakes as they\n",
    "lay on the floor and a bright idea came into her head. \"If I eat one of\n",
    "these cakes,\" she thought, \"it's sure to make some< change in my size.\"\n",
    "\n",
    "So she swallowed one of the cakes and was delighted to find that she\n",
    "began shrinking directly. As soon as she was small enough to get through\n",
    "the door, she ran out of the house and found quite a crowd of little\n",
    "animals and birds waiting outside. They all made a rush at Alice the\n",
    "moment she appeared, but she ran off as hard as she could and soon found\n",
    "herself safe in a thick wood.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9646acb1-8cc3-4665-a938-5e6a5fac1d08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Note, the default model is already 'text-davinci-003' \n",
    "# temperature 0 means no randomness\n",
    "llm = OpenAI(temperature=0, model_name='text-davinci-003')\n",
    "\n",
    "# check the number of tokens\n",
    "num_tokens = llm.get_num_tokens(text_sample)\n",
    "print(f\"{num_tokens=}\")\n",
    "\n",
    "# Summarization prompt template\n",
    "template = \"\"\"\n",
    "%INSTRUCTIONS:\n",
    "Please summarize the following piece of text.\n",
    "Respond in a manner that a 5 year old would understand.\n",
    "\n",
    "%TEXT:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "# Create a LangChain prompt template that we can insert values to later\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format(text=text_sample)\n",
    "\n",
    "#print(\"\\nPrompt\")\n",
    "#print(prompt)\n",
    "\n",
    "# run the model\n",
    "output = llm(prompt)\n",
    "\n",
    "print(\"\\nOutput\")\n",
    "print (output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481c1b47-ce62-4d6f-a282-a669747fca0f",
   "metadata": {},
   "source": [
    "---\n",
    "## Summaries of Short text leveraging Summarization Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d45981-0b9c-4ec3-842f-c2a05713441b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Note, the default model is already 'text-davinci-003' \n",
    "# temperature 0 means no randomness\n",
    "llm = OpenAI(temperature=0, model_name='text-davinci-003')\n",
    "\n",
    "# check the number of tokens\n",
    "num_tokens = llm.get_num_tokens(text_sample)\n",
    "print(f\"{num_tokens=}\")\n",
    "\n",
    "# build a document reuse text sampke above\n",
    "doc = Document(\n",
    "    page_content=text_sample,\n",
    "    metadata={\n",
    "        'author':\"Lewis Caroll\",\n",
    "        'title':\"Alice in Wonderland\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# chain expect a list of documents\n",
    "docs = [doc]\n",
    "\n",
    "# the attribute stuff instruct the run the chain once\n",
    "chain = load_summarize_chain(\n",
    "    llm, \n",
    "    chain_type=\"stuff\", \n",
    "    verbose=False)\n",
    "\n",
    "# run the chain against the documment\n",
    "summary = chain.run(docs)\n",
    "    \n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec956f86-893a-495c-afc6-6a37143fd6d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Summaries of Short text leveraging Summarization Chain and custom prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6c1edb-423a-4e89-9551-42eaccfcbdd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Note, the default model is already 'text-davinci-003' \n",
    "# temperature 0 means no randomness\n",
    "llm = OpenAI(temperature=0, model_name='text-davinci-003')\n",
    "\n",
    "# check the number of tokens\n",
    "num_tokens = llm.get_num_tokens(text_sample)\n",
    "print(f\"{num_tokens=}\")\n",
    "\n",
    "# build a document reuse text sampke above\n",
    "doc = Document(\n",
    "    page_content=text_sample,\n",
    "    metadata={\n",
    "        'author':\"Lewis Caroll\",\n",
    "        'title':\"Alice in Wonderland\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# chain expect a list of documents\n",
    "docs = [doc]\n",
    "\n",
    "# setup. a custom prompt\n",
    "# a defaukt one is provide: write a concise summary\n",
    "prompt_template = \"\"\"\n",
    "%INSTRUCTIONS:\n",
    "Please summarize the following piece of text.\n",
    "Respond in a manner that a 5 year old would understand.\n",
    "\n",
    "%TEXT:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "# the attribute stuff instruct the run the chain once\n",
    "chain = load_summarize_chain(\n",
    "    llm, \n",
    "    chain_type=\"stuff\", \n",
    "    prompt=prompt, \n",
    "    verbose=False)\n",
    "\n",
    "# run the chain against the documment\n",
    "summary = chain.run(docs)\n",
    "    \n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee66787c-12f5-4c15-a9fa-78fea9bc7c2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Summaries Of longer Text\n",
    "If the text is longer than the limit in tokens, the text must be splitted in chunks. \n",
    "Langchain components will take care of splitting and chaining the summarization tasks.\n",
    "\n",
    "The Summarization Chain breaks the text into smaller chunks and summarizing each chunk, creating a final summary based on the individual summaries.\n",
    "\n",
    "In this example, the chain first splits the essay into chunks of 700 characters. It then generates summaries for each chunk and creates a final concise summary based on these individual summaries.\n",
    "\n",
    "<br/>\n",
    "**Resources**\n",
    "> - Qummarization quickstart: https://python.langchain.com/docs/modules/chains/popular/summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60af1966-a821-4ee4-807c-387037ae395c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import BSHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    " \n",
    "# Note, the default model is already 'text-davinci-003' \n",
    "# temperature 0 means no randomness\n",
    "llm = OpenAI(temperature=0, model_name='text-davinci-003')\n",
    "\n",
    "# This is the source document.    \n",
    "document_path = \"data/Alice's Adventures in Wonderland, by Lewis Carroll.html\"\n",
    " \n",
    "# Setup a HTML loader\n",
    "loader = BSHTMLLoader(document_path)\n",
    "documents = loader.load()\n",
    " \n",
    "# check the number of tokens\n",
    "num_tokens = llm.get_num_tokens(documents[0].page_content)\n",
    "print(f\"{num_tokens=}\")\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    " \n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    " \n",
    "# setup. a custom prompt\n",
    "# the Summarization Chain provides a defaults prompt: write a concise summary.\n",
    "prompt_template = \"\"\"Write a concise summary of the following text. \n",
    "Focus on the story and ignore details of Project Gutenberg. \n",
    "\n",
    "% TEXT:\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "# the attribute map_reduce instruct the chain to \n",
    "# - first apply the model to each chunck (map stage) \n",
    "# - then all map results and apply the model (reduce stage)\n",
    "chain = load_summarize_chain(\n",
    "    llm, \n",
    "    chain_type=\"map_reduce\", \n",
    "    map_prompt=prompt, \n",
    "    combine_prompt=prompt, \n",
    "    verbose=False)\n",
    "\n",
    "# run the chain against all the document chunks\n",
    "summary = chain.run(texts)\n",
    "\n",
    "# save the final summary\n",
    "with open('alice_summary.txt', 'w') as file:\n",
    "    file.write(summary)\n",
    "    \n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de2aa30-1529-4e4e-b951-d9ac2fd2bec6",
   "metadata": {},
   "source": [
    "**OUTPUT (default prompt)**\n",
    "\n",
    "Alice's Adventures in Wonderland is a classic novel by Lewis Carroll, originally published in 1916. \n",
    "It follows Alice as she falls down a rabbit hole and embarks on a series of strange and wonderful adventures \n",
    "in the magical world of Wonderland. \n",
    "Project Gutenberg is a library of free electronic works owned by the Project Gutenberg Literary Archive Foundation, \n",
    "which allows users to copy, distribute, perform, display or create derivative works based on the work \n",
    "as long as all references to Project Gutenberg are removed. \n",
    "Professor Michael S. Hart was the originator of the concept and has been producing and distributing \n",
    "Project Gutenberg eBooks for 40 years.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f928a54e-871d-4c9d-a633-ff060d237a90",
   "metadata": {},
   "source": [
    "**OUTPUT (custom prompt)**\n",
    "\n",
    "Alice visits the Queen's Croquet Ground and is asked to play a game of croquet with the Queen. \n",
    "Alice is surprised to find that the balls are live hedgehogs and the mallets are flamingos. \n",
    "After winning the game, Alice is invited to join the Queen's procession and finds herself in a court of justice, \n",
    "where she is put on trial for stealing the Queen's tarts. \n",
    "Alice is defended by the White Rabbit and the jury is made up of animals and birds. \n",
    "Alice is found not guilty, but the Queen is furious and orders Alice to leave. \n",
    "Alice is saved by the Cheshire Cat who appears and tells the Queen that she can't do anything to Alice. \n",
    "Alice then meets the Duchess who is out of prison and they walk off together, \n",
    "but the Queen appears and gives Alice a warning. \n",
    "Alice then has a dream in which she encounters a pack of cards that come to life and try to attack her. \n",
    "She wakes up to find her sister and they go home."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f45e8a-c856-439b-a72b-5256c11ab67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "# [UC] 2.  Question & Answering Using Documents As Context\n",
    "Question answering in this context refers to question answering over your document data. F\n",
    "\n",
    "It is basically the example in Indexes.\n",
    "\n",
    "In order to use LLMs for question and answer we must:\n",
    "- Pass the LLM relevant context it needs to answer a question\n",
    "- Pass it our question that we want answered\n",
    "\n",
    "<br/>\n",
    "\n",
    "++Resources**\n",
    "> - [QA] LangChain Question & Answer Docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae034b5-2238-436b-8fa4-33618e0d8f6b",
   "metadata": {},
   "source": [
    "---\n",
    "# [UC] 2.  Question & Answering Using Documents As Context\n",
    "It is basically the example in Indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b21e6ae-1cbb-40b0-baab-95d9dae12a6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import BSHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Annoy\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    " \n",
    "# This is the source document.    \n",
    "document_path = \"data/Alice's Adventures in Wonderland, by Lewis Carroll.html\"\n",
    " \n",
    "# Setup a HTML loader\n",
    "loader = BSHTMLLoader(document_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# Get your splitter ready\n",
    "# in practice they default to 4000 and 200 respectively.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    " \n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(f\"Generated {len(texts)} parts\")\n",
    "\n",
    "# Get embedding engine ready\n",
    "embeddings = OpenAIEmbeddings()\n",
    " \n",
    "# Embedd your texts andd store them in the vector database\n",
    "# dtabase is in memory. it might be savecd to a file and loader later on.\n",
    "db = Annoy.from_documents(texts, embeddings)\n",
    "\n",
    "# Init a retriever for this db\n",
    "#retriever = db.as_retriever()\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":4})\n",
    "\n",
    "# ra query\n",
    "query = \"who is the White Rabbit?\"\n",
    "\n",
    "# retrieve and count indexed documents relevant for the query\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "print(f\"\\nFound {len(docs)} relevant documen(s)\")\n",
    "\n",
    "#samples = \"\\n\\n\".join([x.page_content[:200] for x in docs[:5]])\n",
    "#print(samples)\n",
    "\n",
    "# create a chain to answer questions \n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(), \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    return_source_documents=True)\n",
    "\n",
    "response = qa({\"query\": query})\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f12348f-d1b6-46c4-84d8-8bf320034547",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# using instructions to get a more interesting reponse\n",
    "instructions = \". Give a funny answer 30 words long.\"\n",
    "response = qa({\"query\": query + instructions})\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46668964-0f30-4221-8447-5ef99c56f10f",
   "metadata": {},
   "source": [
    "---\n",
    "### Questions and Answer using a loaded vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c491b0-75c1-4d73-abc1-fb46b5e2fc19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# saving the database for future use\n",
    "\n",
    "docstore_file_path = \"alice_docstore_2\"\n",
    "\n",
    "db.save_local(docstore_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e2276-1c2e-4025-905b-09d2c1532689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loading the database \n",
    "\n",
    "docstore_file_path = \"alice_docstore_2\"\n",
    "\n",
    "loaded_vector_store = Annoy.load_local(\n",
    "   docstore_file_path, embeddings=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# expose this index in a retriever interface\n",
    "retriever = loaded_vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":4})\n",
    "\n",
    "# ra query\n",
    "query = \"who is the White Rabbit?\"\n",
    "\n",
    "# create a chain to answer questions \n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(), \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    return_source_documents=True)\n",
    "\n",
    "instructions = \". Give a pedantic answer 50 words long.\"\n",
    "\n",
    "response = qa({\"query\": query + instructions})\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2523dc19-823e-48fd-9714-c0f5e799963a",
   "metadata": {},
   "source": [
    "---\n",
    "### Complex search on large document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1593a427-460f-4e51-955a-208b273f2ca3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import BSHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    " \n",
    "# Note, the default model is already 'text-davinci-003' \n",
    "# temperature 0 means no randomness\n",
    "llm = OpenAI(temperature=0.3, model_name='text-davinci-003')\n",
    "\n",
    "# This is the source document.    \n",
    "document_path = \"data/Alice's Adventures in Wonderland, by Lewis Carroll.html\"\n",
    " \n",
    "# Setup a HTML loader\n",
    "loader = BSHTMLLoader(document_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    " \n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(f\"\\nFound {len(texts)} part(s)\")\n",
    "\n",
    "\n",
    "# setup. a custom prompt\n",
    "# the Summarization Chain provides a defaults prompt: write a concise summary.\n",
    "map_prompt_template = \"\"\"\n",
    "Make a detailled summary.\n",
    "Focus on the story and ignore details of Project Gutenberg.\n",
    "List all the characters.\n",
    "Output the list of characters as a bullet points lust which showsthe name and description of the characters. \n",
    "\n",
    "% TEXT:\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "\n",
    "# setup. a custom prompt\n",
    "# the Summarization Chain provides a defaults prompt: write a concise summary.\n",
    "# elements of the list of sample will diseapper from the list \n",
    "combine_prompt_template = \"\"\"\n",
    "Make a summary of summaries and merge all character lists.\n",
    "List all the characters.\n",
    "Output the list of characters as a bullet points list which shows the name and description of the characters. \n",
    "\n",
    "% TEXT:\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "combine_prompt = PromptTemplate(template=combine_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "\n",
    "# the attribute map_reduce instruct the chain to \n",
    "# - first apply the model to each chunck (map stage) \n",
    "# - then all map results and apply the model (reduce stage)\n",
    "chain = load_summarize_chain(\n",
    "    llm, \n",
    "    chain_type=\"map_reduce\", \n",
    "    map_prompt=map_prompt, \n",
    "    combine_prompt=combine_prompt, \n",
    "    verbose=False)\n",
    "\n",
    "# run the chain against all the document chunks\n",
    "summary = chain.run(texts)\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a079881-939f-4c89-9a21-9e6c3d319f44",
   "metadata": {},
   "source": [
    "Merged List of Characters: \n",
    "```\n",
    " Alice  A young girl who falls down a rabbit hole into a fantasy world.\n",
    " White Rabbit  A rabbit who wears a waistcoat and is always in a hurry.\n",
    " The Cheshire Cat  A mysterious cat with a wide grin who can disappear and reappear at will.\n",
    " The Mad Hatter  A strange man who wears a top hat and throws tea parties.\n",
    " The Queen of Hearts  A tyrannical ruler who is always shouting \"Off with their heads!\"\n",
    " The March Hare  A hare who is always late and attends the Mad Hatter's tea parties.\n",
    " The Caterpillar  A wise creature who smokes a hookah and gives Alice advice.\n",
    " The Duchess  A rude woman who lives in a chaotic house.\n",
    " The Mock Turtle  A sad creature who tells Alice stories of his past.\n",
    " The Gryphon  A strange creature with the head of an eagle and the body of a lion.\n",
    " Dinah  Alice's cat who she misses and hopes will get her saucer of milk at tea-time.\n",
    " Little Table  A table made of solid glass with a\n",
    " King of Hearts  Character in the trial mentioned in\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67c9786-5bfc-429b-9552-659f9ed96fd6",
   "metadata": {},
   "source": [
    "- Alice: A young girl who falls down a rabbit hole and enters a strange and magical world.\n",
    "- White Rabbit: A white rabbit who Alice follows down the rabbit hole.\n",
    "- Mad Hatter: A strange character who hosts a tea party with the March Hare and the Dormouse.\n",
    "- March Hare: A hare who attends the Mad Hatter's tea party.\n",
    "- Dormouse: A sleepy mouse who attends the Mad Hatter's tea party.\n",
    "- Queen of Hearts: A tyrannical ruler who orders the beheading of anyone who offends her.\n",
    "- King of Hearts: The Queen of Hearts' husband who is easily manipulated by her.\n",
    "- Caterpillar: A large caterpillar who smokes a hookah and speaks in riddles.\n",
    "- Gryphon: A creature with the head and wings of an eagle and the body of a lion.\n",
    "- Mock Turtle: A sad creature who tells Alice a story about his life."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7374d6cb-ab33-46b8-9711-0f111e9f4e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dd08e56-c9c7-4ef4-8c52-67761da569d1",
   "metadata": {},
   "source": [
    "- Caterpillar: A wise caterpillar who smokes a hookah and offers advice to Alice.\n",
    "- Queen of Hearts: A tyrannical queen who demands to have everyone executed for trivial offenses.\n",
    "- Cheshire Cat: A mysterious, mischievous cat with a wide grin that appears and disappears at will.\n",
    "- Mad Hatter: An eccentric character who hosts a tea party and speaks in nonsensical riddles.\n",
    "- Dormouse: A sleepy character that sits near the Mad Hatter at the tea party.\n",
    "- March Hare: A hare that is often seen with the Mad Hatter and Dormouse at the tea party.\n",
    "- Mock Turtle: A turtle with the head of an ox that talks of its school days."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e191ce-8d91-406d-a125-4721df22bc4a",
   "metadata": {},
   "source": [
    "- Luke Skywalker  A young farm boy from Tatooine who discovers his destiny as a Jedi Knight.\n",
    "- Princess Leia  A brave and resourceful leader of the Rebel Alliance.\n",
    "- Han Solo  A roguish smuggler and pilot who joins forces with the Rebel Alliance.\n",
    "- Obi-Wan Kenobi  A wise and powerful Jedi Master who mentors Luke Skywalker.\n",
    "- Darth Vader  A powerful Sith Lord and the main antagonist of the original trilogy.\n",
    "- C-3PO  A protocol droid built by Anakin Skywalker and programmed for etiquette and protocol.\n",
    "- R2-D2  An astromech droid who serves as a companion to Luke Skywalker.\n",
    "- Chewbacca  A loyal Wookiee and Han Solo's co-pilot.\n",
    "- Yoda  A wise and powerful Jedi Master who trains Luke Skywalker in the ways of the Force.\n",
    "- Jabba the Hutt  A powerful crime lord who controls much of the criminal underworld in the galaxy.\n",
    "- Boba Fett  A bounty hunter hired by Darth Vader to capture Han Solo.\n",
    "- Lando Calrissian  A smooth-talking smuggler and former friend of Han Solo.\n",
    "- Emperor Palpatine  The evil Sith Lord who"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0040f1bb-18a7-4ebb-a1ec-48407e47e432",
   "metadata": {},
   "source": [
    "---\n",
    "### Complex search on large document using DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44038a09-5b06-4ffc-ad6f-0f1e90446c7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.vectorstores import Annoy\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    " \n",
    "# Note, the default model is already 'text-davinci-003' \n",
    "# temperature 0 means no randomness\n",
    "#model_name=\"gpt-3.5-turbo\" # fdoes not work with map reduce\n",
    "model_name='text-davinci-003'\n",
    "llm = OpenAI(temperature=0.5, model_name=model_name)\n",
    "\n",
    "# loading the database \n",
    "\n",
    "docstore_file_path = \"alice_docstore_2\"\n",
    "\n",
    "loaded_vector_store = Annoy.load_local(\n",
    "   docstore_file_path, embeddings=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# expose this index in a retriever interface\n",
    "# retriev all the docuents\n",
    "retriever = loaded_vector_store.as_retriever(search_type=\"similarity\", \n",
    "                                             search_kwargs={\"k\":2000, \n",
    "                                                           \"score_threshold\": 0})\n",
    "\n",
    "# retrieve and count indexed documents to ensure all the documents are selected\n",
    "docs = retriever.get_relevant_documents(\"all the story\")\n",
    "print(f\"\\nFound {len(docs)} documen(s)\")\n",
    " \n",
    "# setup. a custom prompt\n",
    "# the Summarization Chain provides a defaults prompt: write a concise summary.\n",
    "map_prompt_template = \"\"\"\n",
    "Make a detailled summary.\n",
    "Focus on the story and ignore details of Project Gutenberg.\n",
    "List all the characters.\n",
    "Output the list of characters as a bullet points lust which showsthe name and description of the characters. \n",
    "\n",
    "% TEXT:\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "\n",
    "# setup. a custom prompt\n",
    "# the Summarization Chain provides a defaults prompt: write a concise summary.\n",
    "combine_prompt_template = \"\"\"\n",
    "Make a summary of summaries and merge all character lists.\n",
    "List all the characters.\n",
    "Output the list of characters as a bullet points list which shows the name and description of the characters. \n",
    "\n",
    "% TEXT:\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "combine_prompt = PromptTemplate(template=combine_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "\n",
    "# the attribute map_reduce instruct the chain to \n",
    "# - first apply the model to each chunck (map stage) \n",
    "# - then all map results and apply the model (reduce stage)\n",
    "chain = load_summarize_chain(\n",
    "    llm, \n",
    "    chain_type=\"map_reduce\", \n",
    "    map_prompt=map_prompt, \n",
    "    combine_prompt=combine_prompt, \n",
    "    verbose=False)\n",
    "\n",
    "# run the chain against all the document chunks\n",
    "summary = chain.run(docs)\n",
    "    \n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58660588-3230-4945-8431-53604f077ec3",
   "metadata": {},
   "source": [
    "- Lory: A bird who is looking for a way out of the wood\n",
    "- Duck: A bird who is looking for a way out of the wood\n",
    "- Eaglet: A bird who is looking for a way out of the wood\n",
    "- Dodo: A bird who is looking for a way out of the wood\n",
    "- Caterpillar: A creature who is smoking a hookah\n",
    "- King and Queen of Hearts: The rulers of the court\n",
    "- Knave of Hearts: A character who is accused of stealing the tarts\n",
    "- Three Gardeners: Characters who are painting white roses red\n",
    "- Five and Seven: Two characters who are guarding the Queen\n",
    "- Two: A character who is the White Rabbit's servant\n",
    "- Ten Soldiers: Characters who are guarding the Queen\n",
    "- Ten Courtiers: Characters who are attending the Queen\n",
    "- Royal Children: Characters who are attending the Queen\n",
    "- Guests: Characters who are attending the Queen\n",
    "- Small Door in a Wall: A mysterious door that Alice finds\n",
    "- Mushroom: A mysterious mushroom that Alice finds\n",
    "- Bottle with the Words \"DRINK ME\" Printed on It: A mysterious bottle that Alice finds\n",
    "- Fan: A mysterious fan that Alice finds\n",
    "- Pair of White Kid-Gloves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dc1ab5-a9e6-4c04-8b55-1bae6564a3a9",
   "metadata": {},
   "source": [
    "Characters:\n",
    "- Alice: The protagonist of the story. She is a young girl who finds herself in a strange world.\n",
    "- White Rabbit: A talking rabbit who is always in a hurry and is late for important appointments.\n",
    "- Cake: A cake with the words \"Eat Me\" written on it.\n",
    "- Key: A tiny golden key found under the table. \n",
    "- Rabbit-Hole: A deep dark hole leading into a corridor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad973be-fc89-4349-afff-2b6515082696",
   "metadata": {},
   "source": [
    "### same using with formatted lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "be94f05e-1bea-4804-ad35-a873e408d02a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 50 part(s)\n",
      "\n",
      "Map Prompt\n",
      "input_variables=['text'] output_parser=None partial_variables={'format_instructions': 'Your response should be a list of comma separated values, eg: `foo, bar, baz`'} template='\\nMake a detailled summary.\\nFocus on the story and ignore details of Project Gutenberg.\\nFind the characters and list thier names.\\n\\n{format_instructions}\\n\\n% TEXT:\\n\\n{text}\\n' template_format='f-string' validate_template=True\n",
      "\n",
      "Combine Prompt\n",
      "input_variables=['text'] output_parser=None partial_variables={} template='\\nTo make a summary, keep all lines starting with the word characters.\\n\\n% TEXT:\\n\\n{text}\\n' template_format='f-string' validate_template=True\n",
      "\n",
      "Response\n",
      "No summary.\n",
      "\n",
      "Characters\n",
      "['No answer required']\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import BSHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "\n",
    "# Note, the default model is already 'text-davinci-003' \n",
    "# temperature 0 means no randomness\n",
    "#model_name=\"gpt-3.5-turbo\" # fdoes not work with map reduce\n",
    "model_name='text-davinci-003'\n",
    "llm = OpenAI(temperature=0.3, model_name=model_name)\n",
    "\n",
    "# This is the source document.    \n",
    "document_path = \"data/Alice's Adventures in Wonderland, by Lewis Carroll.html\"\n",
    " \n",
    "# Setup a HTML loader\n",
    "loader = BSHTMLLoader(document_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    " \n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(f\"\\nFound {len(texts)} part(s)\")\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# setup. a custom prompt\n",
    "# the Summarization Chain provides a defaults prompt: write a concise summary.\n",
    "map_prompt_template = \"\"\"\n",
    "Make a detailled summary.\n",
    "Focus on the story and ignore details of Project Gutenberg.\n",
    "Find the characters and list their names.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% TEXT:\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "map_prompt = PromptTemplate(template=map_prompt_template, \n",
    "                        input_variables=[\"text\"],\n",
    "                        partial_variables={\"format_instructions\": format_instructions}\n",
    "                       )\n",
    "\n",
    "print(\"\\nMap Prompt\")\n",
    "print(map_prompt)\n",
    "\n",
    "# setup. a custom prompt\n",
    "# the Summarization Chain provides a defaults prompt: write a concise summary.\n",
    "combine_prompt_template = \"\"\"\n",
    "To make a summary, keep all lines starting with the word characters.\n",
    "\n",
    "% TEXT:\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "combine_prompt = PromptTemplate(template=combine_prompt_template, \n",
    "                        input_variables=[\"text\"]\n",
    "                        #partial_variables={\"format_instructions\": format_instructions}\n",
    "                       )\n",
    "\n",
    "print(\"\\nCombine Prompt\")\n",
    "print(combine_prompt)\n",
    "\n",
    "\n",
    "# the attribute map_reduce instruct the chain to \n",
    "# - first apply the model to each chunck (map stage) \n",
    "# - then all map results and apply the model (reduce stage)\n",
    "chain = load_summarize_chain(\n",
    "    llm, \n",
    "    chain_type=\"map_reduce\", \n",
    "    map_prompt=map_prompt, \n",
    "    combine_prompt=combine_prompt, \n",
    "    verbose=False)\n",
    "\n",
    "# run the chain against all the document chunks\n",
    "response = chain.run(texts)\n",
    "\n",
    "print(\"\\nResponse\")\n",
    "print(response)\n",
    "\n",
    "#characters = output_parser.parse(response)\n",
    "\n",
    "print(\"\\nCharacters\")\n",
    "print(characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ac82b0-5d0e-4533-b281-782cc65a7e26",
   "metadata": {},
   "source": [
    "**OUTPUT**\n",
    "\n",
    "Characters:\n",
    "- Alice: The protagonist of the story. She is a young girl who finds herself in a strange world.\n",
    "- White Rabbit: A talking rabbit who is always in a hurry and is late for important appointments.\n",
    "- Cake: A cake with the words \"Eat Me\" written on it.\n",
    "- Key: A tiny golden key found under the table. \n",
    "- Rabbit-Hole: A deep dark hole leading into a corridor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8274d4ae-6da1-4513-84dd-f2aab3a4ecb7",
   "metadata": {},
   "source": [
    "---\n",
    "### get a list then query each character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29567160-24b7-4f48-aa50-f95679f686a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 50 part(s)\n",
      "\n",
      "Map Prompt\n",
      "input_variables=['text'] output_parser=None partial_variables={'format_instructions': 'Your response should be a list of comma separated values, eg: `foo, bar, baz`'} template='\\nou will be given a text.\\nExtract the characters\\'s names.\\nIgnore details of Project Gutenberg.\\n\\n{format_instructions}. Add \"characters:\" in front of the list.\\n\\n% TEXT:\\n\\n{text}\\n' template_format='f-string' validate_template=True\n",
      "\n",
      "Part Characters 1\n",
      "['Alice', 'The Duchess', 'The White Rabbit', 'The Cheshire Cat', 'The Mad Hatter', 'The March Hare', 'The Queen of Hearts.']\n",
      "\n",
      "Part Characters 2\n",
      "['Alice', 'White Rabbit', \"Alice's sister\"]\n",
      "\n",
      "Part Characters 3\n",
      "['Alice', 'Dinah', 'White Rabbit']\n",
      "\n",
      "Part Characters 4\n",
      "['Alice']\n",
      "\n",
      "Part Characters 5\n",
      "['Alice,']\n",
      "\n",
      "Part Characters 6\n",
      "['Alice', 'White Rabbit']\n",
      "\n",
      "Part Characters 7\n",
      "['Alice', 'Mouse']\n",
      "\n",
      "Part Characters 8\n",
      "['Alice', 'William the Conqueror', 'Mouse', 'Dinah']\n",
      "\n",
      "Part Characters 9\n",
      "['Duck', 'Dodo', 'Lory', 'Eaglet']\n",
      "\n",
      "Part Characters 10\n",
      "['III', 'Caucus-Race', 'Long Tale', 'birds', 'animals']\n",
      "\n",
      "Part Characters 11\n",
      "['Alice', 'Mouse', 'Lory', 'Duck', 'William the Conqueror', 'pope', 'English', 'Edwin', 'Morcar', 'Earls of Mercia and Northumbria', 'Stigand', 'archbishop of Canterbury', 'Edgar Atheling', 'Dodo', 'Eaglet.']\n",
      "\n",
      "Part Characters 12\n",
      "['Alice']\n",
      "\n",
      "Part Characters 13\n",
      "['Alice', 'Dodo', 'C', 'D', 'Mouse']\n",
      "\n",
      "Part Characters 14\n",
      "['Alice', 'Mouse', 'Canary', 'Dinah']\n",
      "\n",
      "Part Characters 15\n",
      "['White Rabbit', 'Alice', 'Mary Ann', 'W. Rabbit']\n",
      "\n",
      "Part Characters 16\n",
      "['Alice', 'Bottle']\n",
      "\n",
      "Part Characters 17\n",
      "['Alice', 'Rabbit', 'Mary Ann', 'Pat']\n",
      "\n",
      "Part Characters 18\n",
      "['Alice', 'Bill', 'Rabbit', 'Jack-in-the-box', 'birds', 'animals.']\n",
      "\n",
      "Part Characters 19\n",
      "['Alice', 'Duchess', 'Caterpillar']\n",
      "\n",
      "Part Characters 20\n",
      "['Alice', 'Caterpillar']\n",
      "\n",
      "Part Characters 21\n",
      "['Alice', 'Caterpillar']\n",
      "\n",
      "Part Characters 22\n",
      "['Alice', 'Caterpillar', 'pigeon']\n",
      "\n",
      "Part Characters 23\n",
      "['Alice', 'Pigeon']\n",
      "\n",
      "Part Characters 24\n",
      "['Alice']\n",
      "\n",
      "Part Characters 25\n",
      "['Pig', 'Pepper', 'Footman 1', 'Footman 2.']\n",
      "\n",
      "Part Characters 26\n",
      "['Alice', 'Fish-Footman', 'Frog-Footman', 'Duchess', 'Cook', 'Baby', 'Cheshire-Cat.']\n",
      "\n",
      "Part Characters 27\n",
      "['Alice', 'Duchess', 'Cook', 'Baby', 'Cheshire-Cat', 'Queen']\n",
      "\n",
      "Part Characters 28\n",
      "['Alice', 'Cheshire-Puss', 'Hatter', 'March Hare', 'Queen']\n",
      "\n",
      "Part Characters 29\n",
      "['Alice', 'March Hare', 'Hatter', 'Dormouse']\n",
      "\n",
      "Part Characters 30\n",
      "['Alice', 'Hatter', 'March Hare', 'Dormouse', 'Knave of Hearts']\n",
      "\n",
      "Part Characters 31\n",
      "['Alice', 'Five', 'Seven', 'Two', 'Ten Soldiers', 'Ten Courtiers', 'Royal Children', 'White Rabbit', 'Knave of Hearts', 'King and Queen of Hearts.']\n",
      "\n",
      "Part Characters 32\n",
      "['Alice', 'White Rabbit', 'Queen', 'Duchess', 'Cheshire-Cat']\n",
      "\n",
      "Part Characters 33\n",
      "['Alice', 'Hedgehog', 'Flamingo', 'Duchess', 'Queen']\n",
      "\n",
      "Part Characters 34\n",
      "['King', 'Queen', 'Alice']\n",
      "\n",
      "Part Characters 35\n",
      "['King of Hearts', 'Queen of Hearts', 'Knave of Hearts', 'White Rabbit', 'Judge', 'Twelve Creatures', 'Herald.']\n",
      "\n",
      "Part Characters 36\n",
      "['Hatter', 'March Hare', 'Dormouse', 'King', 'White Rabbit', 'Alice', \"Duchess's Cook.\"]\n",
      "\n",
      "Part Characters 37\n",
      "['Alice', 'King', 'Queen']\n",
      "\n",
      "Part Characters 38\n",
      "['Alice', 'King', 'White Rabbit', 'Knave', 'Queen.']\n",
      "\n",
      "Part Characters 39\n",
      "['Alice', 'Queen', \"Alice's sister\"]\n",
      "\n",
      "Part Characters 40\n",
      "['None']\n",
      "\n",
      "Part Characters 41\n",
      "['None']\n",
      "\n",
      "Part Characters 42\n",
      "['None']\n",
      "\n",
      "Part Characters 43\n",
      "['']\n",
      "\n",
      "Part Characters 44\n",
      "['None']\n",
      "\n",
      "Part Characters 45\n",
      "['None']\n",
      "\n",
      "Part Characters 46\n",
      "['']\n",
      "\n",
      "Part Characters 47\n",
      "['None']\n",
      "\n",
      "Part Characters 48\n",
      "['None']\n",
      "\n",
      "Part Characters 49\n",
      "['']\n",
      "\n",
      "Part Characters 50\n",
      "['Professor Michael S. Hart', 'U.S.']\n",
      "\n",
      "All Characters 50\n",
      "{'', 'Ten Courtiers', 'Lory', 'The Cheshire Cat', 'Stigand', 'birds', 'Alice,', 'Judge', 'King of Hearts', 'Ten Soldiers', 'Mouse', 'Queen', 'archbishop of Canterbury', 'W. Rabbit', 'Long Tale', 'Royal Children', 'Eaglet', 'pigeon', 'Footman 2.', 'Rabbit', 'Dormouse', 'Edwin', 'King', 'Cheshire-Cat', 'pope', 'Mary Ann', 'Five', 'Bottle', 'The Queen of Hearts.', 'Two', 'Professor Michael S. Hart', 'Frog-Footman', 'Jack-in-the-box', 'Alice', 'Earls of Mercia and Northumbria', 'Dinah', 'Knave', 'Hatter', 'animals.', 'English', 'The March Hare', 'C', 'Dodo', 'The Duchess', 'Seven', 'Pat', 'William the Conqueror', 'Eaglet.', 'The Mad Hatter', 'Bill', 'Cook', 'Pig', 'Footman 1', \"Duchess's Cook.\", 'Queen of Hearts', 'Cheshire-Cat.', 'Flamingo', 'Knave of Hearts', 'The White Rabbit', 'Queen.', 'Cheshire-Puss', 'Duck', \"Alice's sister\", 'Pigeon', 'Hedgehog', 'Caucus-Race', 'D', 'animals', 'Baby', 'Herald.', 'III', 'Edgar Atheling', 'Caterpillar', 'Twelve Creatures', 'Fish-Footman', 'White Rabbit', 'None', 'Morcar', 'King and Queen of Hearts.', 'U.S.', 'Duchess', 'Canary', 'March Hare', 'Pepper'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import BSHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Note, the default model is already 'text-davinci-003' \n",
    "# temperature 0 means no randomness\n",
    "#model_name=\"gpt-3.5-turbo\" # fdoes not work with map reduce\n",
    "model_name='text-davinci-003'\n",
    "llm = OpenAI(temperature=0.3, model_name=model_name)\n",
    "\n",
    "# This is the source document.    \n",
    "document_path = \"data/Alice's Adventures in Wonderland, by Lewis Carroll.html\"\n",
    " \n",
    "# Setup a HTML loader\n",
    "loader = BSHTMLLoader(document_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    " \n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(f\"\\nFound {len(texts)} part(s)\")\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# setup. a custom prompt\n",
    "# the Summarization Chain provides a defaults prompt: write a concise summary.\n",
    "map_prompt_template = \"\"\"\n",
    "ou will be given a text.\n",
    "Extract the characters's names.\n",
    "Ignore details of Project Gutenberg.\n",
    "\n",
    "{format_instructions}. Add \"characters:\" in front of the list.\n",
    "\n",
    "% TEXT:\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "map_prompt = PromptTemplate(template=map_prompt_template, \n",
    "                        input_variables=[\"text\"],\n",
    "                        partial_variables={\"format_instructions\": format_instructions}\n",
    "                       )\n",
    "\n",
    "print(\"\\nMap Prompt\")\n",
    "print(map_prompt)\n",
    "\n",
    "\n",
    "# the attribute map_reduce instruct the chain to \n",
    "# - first apply the model to each chunck (map stage) \n",
    "# - then all map results and apply the model (reduce stage)\n",
    "chain = load_summarize_chain(\n",
    "    llm, \n",
    "    chain_type=\"stuff\", \n",
    "    prompt=map_prompt,  \n",
    "    verbose=False)\n",
    "\n",
    "i = 0\n",
    "characters = []\n",
    "prefix = \"Characters:\"\n",
    "for text in texts:\n",
    "    i += 1\n",
    "    \n",
    "    # run the chain against all the document chunks\n",
    "    # chain expect a list of documents\n",
    "    response = chain.run([text])\n",
    "\n",
    "    #print(f\"\\nResponse {i}\")\n",
    "    #print(response)\n",
    "\n",
    "    lines = response.split('\\n')\n",
    "    #print(f\"\\nlines {i}\")\n",
    "    #print(lines)\n",
    "    for line in lines:\n",
    "        #print(f\"\\nline {i}\")\n",
    "        #print(line)\n",
    "        if line.startswith(prefix):\n",
    "            part_characters = output_parser.parse(line.replace(prefix, ''))\n",
    "\n",
    "            print(f\"\\nPart Characters {i}\")\n",
    "            print(part_characters)\n",
    "\n",
    "            characters.extend(part_characters)\n",
    "    \n",
    "\n",
    "print(f\"\\nAll Characters {i}\")\n",
    "print(set(characters))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1519d54-8c28-4da1-b266-3234a4ebcbcf",
   "metadata": {},
   "source": [
    "**OUTPUT**\n",
    "\n",
    "All Characters 50\n",
    "{'', 'Ten Courtiers', 'Lory', 'The Cheshire Cat', 'Stigand', 'birds', 'Alice,', 'Judge', 'King of Hearts', 'Ten Soldiers', 'Mouse', 'Queen', 'archbishop of Canterbury', 'W. Rabbit', 'Long Tale', 'Royal Children', 'Eaglet', 'pigeon', 'Footman 2.', 'Rabbit', 'Dormouse', 'Edwin', 'King', 'Cheshire-Cat', 'pope', 'Mary Ann', 'Five', 'Bottle', 'The Queen of Hearts.', 'Two', 'Professor Michael S. Hart', 'Frog-Footman', 'Jack-in-the-box', 'Alice', 'Earls of Mercia and Northumbria', 'Dinah', 'Knave', 'Hatter', 'animals.', 'English', 'The March Hare', 'C', 'Dodo', 'The Duchess', 'Seven', 'Pat', 'William the Conqueror', 'Eaglet.', 'The Mad Hatter', 'Bill', 'Cook', 'Pig', 'Footman 1', \"Duchess's Cook.\", 'Queen of Hearts', 'Cheshire-Cat.', 'Flamingo', 'Knave of Hearts', 'The White Rabbit', 'Queen.', 'Cheshire-Puss', 'Duck', \"Alice's sister\", 'Pigeon', 'Hedgehog', 'Caucus-Race', 'D', 'animals', 'Baby', 'Herald.', 'III', 'Edgar Atheling', 'Caterpillar', 'Twelve Creatures', 'Fish-Footman', 'White Rabbit', 'None', 'Morcar', 'King and Queen of Hearts.', 'U.S.', 'Duchess', 'Canary', 'March Hare', 'Pepper'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d47e5a-cb1e-4370-891a-a8c24b8b3828",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO remove extra characters + filter relevant part of the document</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "463640e8-0f94-4e32-9080-eae1be9c7f33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) - Found 5 relevant documen(s) for \n",
      "(2) - Found 5 relevant documen(s) for Ten Courtiers\n",
      "(3) - Found 5 relevant documen(s) for Lory\n",
      "(4) - Found 5 relevant documen(s) for The Cheshire Cat\n",
      "(5) - Found 5 relevant documen(s) for Stigand\n",
      "(6) - Found 5 relevant documen(s) for birds\n",
      "(7) - Found 5 relevant documen(s) for Alice,\n",
      "(8) - Found 5 relevant documen(s) for Judge\n",
      "(9) - Found 5 relevant documen(s) for King of Hearts\n",
      "(10) - Found 5 relevant documen(s) for Ten Soldiers\n",
      "(11) - Found 5 relevant documen(s) for Mouse\n",
      "(12) - Found 5 relevant documen(s) for Queen\n",
      "(13) - Found 5 relevant documen(s) for archbishop of Canterbury\n",
      "(14) - Found 5 relevant documen(s) for W. Rabbit\n",
      "(15) - Found 5 relevant documen(s) for Long Tale\n",
      "(16) - Found 5 relevant documen(s) for Royal Children\n",
      "(17) - Found 5 relevant documen(s) for Eaglet\n",
      "(18) - Found 5 relevant documen(s) for pigeon\n",
      "(19) - Found 5 relevant documen(s) for Footman 2.\n",
      "(20) - Found 5 relevant documen(s) for Rabbit\n",
      "(21) - Found 5 relevant documen(s) for Dormouse\n",
      "(22) - Found 5 relevant documen(s) for Edwin\n",
      "(23) - Found 5 relevant documen(s) for King\n",
      "(24) - Found 5 relevant documen(s) for Cheshire-Cat\n",
      "(25) - Found 5 relevant documen(s) for pope\n",
      "(26) - Found 5 relevant documen(s) for Mary Ann\n",
      "(27) - Found 5 relevant documen(s) for Five\n",
      "(28) - Found 5 relevant documen(s) for Bottle\n",
      "(29) - Found 5 relevant documen(s) for The Queen of Hearts.\n",
      "(30) - Found 5 relevant documen(s) for Two\n",
      "(31) - Found 5 relevant documen(s) for Professor Michael S. Hart\n",
      "(32) - Found 5 relevant documen(s) for Frog-Footman\n",
      "(33) - Found 5 relevant documen(s) for Jack-in-the-box\n",
      "(34) - Found 5 relevant documen(s) for Alice\n",
      "(35) - Found 5 relevant documen(s) for Earls of Mercia and Northumbria\n",
      "(36) - Found 5 relevant documen(s) for Dinah\n",
      "(37) - Found 5 relevant documen(s) for Knave\n",
      "(38) - Found 5 relevant documen(s) for Hatter\n",
      "(39) - Found 5 relevant documen(s) for animals.\n",
      "(40) - Found 5 relevant documen(s) for English\n",
      "(41) - Found 5 relevant documen(s) for The March Hare\n",
      "(42) - Found 5 relevant documen(s) for C\n",
      "(43) - Found 5 relevant documen(s) for Dodo\n",
      "(44) - Found 5 relevant documen(s) for The Duchess\n",
      "(45) - Found 5 relevant documen(s) for Seven\n",
      "(46) - Found 5 relevant documen(s) for Pat\n",
      "(47) - Found 5 relevant documen(s) for William the Conqueror\n",
      "(48) - Found 5 relevant documen(s) for Eaglet.\n",
      "(49) - Found 5 relevant documen(s) for The Mad Hatter\n",
      "(50) - Found 5 relevant documen(s) for Bill\n",
      "(51) - Found 5 relevant documen(s) for Cook\n",
      "(52) - Found 5 relevant documen(s) for Pig\n",
      "(53) - Found 5 relevant documen(s) for Footman 1\n",
      "(54) - Found 5 relevant documen(s) for Duchess's Cook.\n",
      "(55) - Found 5 relevant documen(s) for Queen of Hearts\n",
      "(56) - Found 5 relevant documen(s) for Cheshire-Cat.\n",
      "(57) - Found 5 relevant documen(s) for Flamingo\n",
      "(58) - Found 5 relevant documen(s) for Knave of Hearts\n",
      "(59) - Found 5 relevant documen(s) for The White Rabbit\n",
      "(60) - Found 5 relevant documen(s) for Queen.\n",
      "(61) - Found 5 relevant documen(s) for Cheshire-Puss\n",
      "(62) - Found 5 relevant documen(s) for Duck\n",
      "(63) - Found 5 relevant documen(s) for Alice's sister\n",
      "(64) - Found 5 relevant documen(s) for Pigeon\n",
      "(65) - Found 5 relevant documen(s) for Hedgehog\n",
      "(66) - Found 5 relevant documen(s) for Caucus-Race\n",
      "(67) - Found 5 relevant documen(s) for D\n",
      "(68) - Found 5 relevant documen(s) for animals\n",
      "(69) - Found 5 relevant documen(s) for Baby\n",
      "(70) - Found 5 relevant documen(s) for Herald.\n",
      "(71) - Found 5 relevant documen(s) for III\n",
      "(72) - Found 5 relevant documen(s) for Edgar Atheling\n",
      "(73) - Found 5 relevant documen(s) for Caterpillar\n",
      "(74) - Found 5 relevant documen(s) for Twelve Creatures\n",
      "(75) - Found 5 relevant documen(s) for Fish-Footman\n",
      "(76) - Found 5 relevant documen(s) for White Rabbit\n",
      "(77) - Found 5 relevant documen(s) for None\n",
      "(78) - Found 5 relevant documen(s) for Morcar\n",
      "(79) - Found 5 relevant documen(s) for King and Queen of Hearts.\n",
      "(80) - Found 5 relevant documen(s) for U.S.\n",
      "(81) - Found 5 relevant documen(s) for Duchess\n",
      "(82) - Found 5 relevant documen(s) for Canary\n",
      "(83) - Found 5 relevant documen(s) for March Hare\n",
      "(84) - Found 5 relevant documen(s) for Pepper\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Annoy\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from pprint import pformat\n",
    "\n",
    "# Get embedding engine ready\n",
    "embeddings = OpenAIEmbeddings()\n",
    " \n",
    "# Embedd your texts andd store them in the vector database\n",
    "# dtabase is in memory. it might be savecd to a file and loader later on.\n",
    "db = Annoy.from_documents(texts, embeddings)\n",
    "\n",
    "# Init a retriever for this db\n",
    "#retriever = db.as_retriever()\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":5})\n",
    "\n",
    "instructions = \". Give a funny answer 30 words long.\"\n",
    "\n",
    "summary = {}\n",
    "i = 0\n",
    "# set remove duplicate strings\n",
    "for character in set(characters):\n",
    "    i += 1\n",
    "\n",
    "    # ra query\n",
    "    query = f\"Who is {character}? {instructions}\"\n",
    "\n",
    "    # retrieve and count indexed documents relevant for the query\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    print(f\"({i}) - Found {len(docs)} relevant documen(s) for {character}\")\n",
    "\n",
    "    #samples = \"\\n\\n\".join([x.page_content[:200] for x in docs[:5]])\n",
    "    #print(samples)\n",
    "\n",
    "    # create a chain to answer questions \n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=OpenAI(), \n",
    "        chain_type=\"stuff\", \n",
    "        retriever=retriever, \n",
    "        return_source_documents=False)\n",
    "\n",
    "    response = qa({\"query\": query})\n",
    "    \n",
    "    summary[character] = response\n",
    "\n",
    "with open('alice_characters_summary.txt', 'w') as file:\n",
    "    file.write(pformat(summary))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d716c9b4-9368-4176-a698-1353055fcccb",
   "metadata": {},
   "source": [
    "**OPUTPUT**\n",
    "\n",
    "check alice_characters_summary.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786bf073-1cc2-4dbe-a49d-b647ce451451",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e87062-5498-42fd-8d37-237a9fd3fddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "889128fa-f108-4e4a-8d75-aeb7b43bace9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO</div>\n",
    "\n",
    "vector store backed retriever \n",
    "\n",
    "https://python.langchain.com/docs/modules/data_connection/retrievers/how_to/vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a5c5a0-01f8-4cf7-8b67-9fa9db8749a6",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/modules/chains/additional/question_answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db949ae-3aaa-400a-b7d8-d84f87aff51b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bc34ab-72dd-4aa3-9bdb-cd9e8c060100",
   "metadata": {},
   "outputs": [],
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO explain refine and mapreduce </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41e4ceb-0127-417a-87cc-f1e3a50a8a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fa8320b-3668-4f6a-8e6f-56ef5462c7e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO </div>\n",
    "\n",
    "prompt\n",
    "parse and map\n",
    "seq chain\n",
    "```python\n",
    "output_parser = RegexParser(\n",
    "    regex=r\"(.*?)\\nScore: (.*)\",\n",
    "    output_keys=[\"answer\", \"score\"],\n",
    ")\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    "    output_parser=output_parser,\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598856aa-2626-4080-8087-c5e3718d3a88",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO \n",
    "how to use qa in chain and do something like make a list and gie details.\n",
    "Another option parsed output and browse the list Ouotput parser as list ?\n",
    "alternative conversation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f0cc91-36bc-4b05-888e-78be46d97415",
   "metadata": {},
   "source": [
    "---\n",
    "# [UC] 3. Extraction\n",
    "\n",
    "Extraction is the process of parsing data from a piece of text. This is commonly used with output parsing in order to structure our data.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**Resources**\n",
    "> - https://python.langchain.com/en/latest/use_cases/extraction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "058884b5-0b5f-436d-a0a5-a855413ebf14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To help construct our Chat Messages\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# We will be using a chat model, defaults to gpt-3.5-turbo\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# To parse outputs and get structured data back\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edc7654-a4d8-451c-bccb-d517d58f88a4",
   "metadata": {},
   "source": [
    "\n",
    "## Vanilla Extraction\n",
    "\n",
    "Let's start off withan easy example. Here I simply supply a prompt with instructions with the type of output I want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fbaf178e-3449-4e75-9df5-a570e941e862",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Apple\": \"\",\n",
      "  \"Pear\": \"\",\n",
      "  \"kiwi\": \"\"\n",
      "}\n",
      "<class 'str'>\n",
      "{'Apple': '', 'Pear': '', 'kiwi': ''}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "instructions = \"\"\"\n",
    "You will be given a sentence with fruit names, extract those fruit names and assign an emoji to them\n",
    "Return the fruit name and emojis in a python dictionary\n",
    "\"\"\"\n",
    "\n",
    "fruit_names = \"\"\"\n",
    "Apple, Pear, this is an kiwi\n",
    "\"\"\"\n",
    "\n",
    "# Make your prompt which combines the instructions w/ the fruit names\n",
    "prompt = (instructions + fruit_names)\n",
    "\n",
    "# Call the LLM\n",
    "output = chat_model([HumanMessage(content=prompt)])\n",
    "\n",
    "print (output.content)\n",
    "print (type(output.content))\n",
    "\n",
    "#Let's turn this into a proper python dictionary\n",
    "\n",
    "output_dict = eval(output.content)\n",
    "\n",
    "print (output_dict)\n",
    "print (type(output_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f056ff8-c389-4ccd-b17a-4b36ea9e6f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "## Using LangChain's Response Schema\n",
    "\n",
    "LangChain's response schema will does two things for us:\n",
    "- Autogenerate the a prompt with bonafide format instructions. This is great because I don't need to worry about the prompt engineering side, I'll leave that up to LangChain!\n",
    "- Read the output from the LLM and turn it into a proper python object for me\n",
    "\n",
    "Here I define the schema I want. I'm going to pull out the song and artist that a user wants to play from a pseudo chat message.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09bdc39e-10cd-49a5-b49f-0beba24c33af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**FORMAT**\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"artist\": string  // The name of the musical artist\n",
      "\t\"song\": string  // The name of the song that the artist plays\n",
      "}\n",
      "```\n",
      "**QUERY**\n",
      "\n",
      "Given a command from the user, extract the artist and song names\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"artist\": string  // The name of the musical artist\n",
      "\t\"song\": string  // The name of the song that the artist plays\n",
      "}\n",
      "```\n",
      "\n",
      "I really like So Young by Portugal. The Man\n",
      "\n",
      "\n",
      "**RESPONSE*\n",
      "{'artist': 'Portugal. The Man', 'song': 'So Young'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# The schema I want out\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"artist\", description=\"The name of the musical artist\"),\n",
    "    ResponseSchema(name=\"song\", description=\"The name of the song that the artist plays\")\n",
    "]\n",
    "\n",
    "# The parser that will look for the LLM output in my schema and return it back to me\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "# The format instructions that LangChain makes. Let's look at them\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(\"\\n**FORMAT**\")\n",
    "print(format_instructions)\n",
    "\n",
    "#The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
    "# rxample\n",
    "#{\n",
    "#\t\"artist\": string  // The name of the musical artist\n",
    "#\t\"song\": string  // The name of the song that the artist plays\n",
    "#}\n",
    "\n",
    "# The prompt template that brings it all together\n",
    "# Note: This is a different prompt template than before because we are using a Chat Model\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Given a command from the user, extract the artist and song names\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "{user_prompt}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate.from_template(prompt_template)  \n",
    "    ],\n",
    "    input_variables=[\"user_prompt\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "chat_query = prompt.format_prompt(\n",
    "    user_prompt=\"I really like So Young by Portugal. The Man\"\n",
    ")\n",
    "print(\"**QUERY**\")\n",
    "print (chat_query.messages[0].content)\n",
    "\n",
    "# Given a command from the user, extract the artist and song names \n",
    "# The output should be a markdown code snippet formatted in the following schema, \n",
    "#including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
    "## ```json\n",
    "# {\n",
    "# \t\"artist\": string  // The name of the musical artist\n",
    "# \t\"song\": string  // The name of the song that the artist plays\n",
    "#}\n",
    "#```\n",
    "\n",
    "chat_output = chat_model(chat_query.to_messages())\n",
    "response = output_parser.parse(chat_output.content)\n",
    "\n",
    "print(\"\\n**RESPONSE*\")\n",
    "print (response)\n",
    "print (type(response))\n",
    "\n",
    "# example\n",
    "#{'artist': 'Portugal. The Man', 'song': 'So Young'}\n",
    "\n",
    "#Warning: The parser looks for an output from the LLM in a specific format. \n",
    "##our model may not output the same format every time. \n",
    "#Make sure to handle errors with this one. GPT4 and future iterations will be more reliable.\n",
    "\n",
    "# For more advanced parsing check out Kor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c460f2d-e64e-4aab-8da7-0e265676719f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'artist': 'Porcupine Tree', 'song': 'Sound of Muzak'}\n"
     ]
    }
   ],
   "source": [
    "chat_query = prompt.format_prompt(\n",
    "    user_prompt=\"I would like to listen Sound of Muzak by Porcupine Tree\"\n",
    ")\n",
    "chat_output = chat_model(chat_query.to_messages())\n",
    "response = output_parser.parse(chat_output.content)\n",
    "\n",
    "print (response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9a24ae-02a0-440f-9b22-4f2fac69a7ce",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9de8da-1014-4a22-8e6b-f39bec5c020c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO Kor\n",
    "</div>\n",
    "\n",
    "Kor\n",
    "\n",
    "This is a half-baked prototype that helps you extract structured data from text using LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b134a99a-0fee-4dc4-a367-785287b44f04",
   "metadata": {},
   "source": [
    "---\n",
    "# [UC] 4. Evaluation\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee1de83-3d33-4065-b240-e3815011e5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "269fd6e6-7e08-41b1-a006-cc326d1a3b2a",
   "metadata": {},
   "source": [
    "# [UC] ...\n",
    "AAnalyzing stuctured data\n",
    "\n",
    "https://python.langchain.com/docs/use_cases/tabular.html\n",
    "\n",
    "https://python.langchain.com/docs/modules/agents/toolkits/csv.html\n",
    "\n",
    "https://python.langchain.com/docs/modules/agents/toolkits/sql_database.html\n",
    "\n",
    "https://python.langchain.com/docs/modules/agents/toolkits/pandas.html\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6247896-89cb-4b0d-9e57-de435db0c233",
   "metadata": {},
   "source": [
    "# [UC] ...\n",
    "API Chains\n",
    "\n",
    "https://python.langchain.com/docs/modules/chains/popular/api.html\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3ba49b-c9cf-48ee-80f0-9294354b8d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [UC] ...\n",
    "graph index creator\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO </div>"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-3:615547856133:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
