{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00afbf75-6012-4c57-9dad-4080e36c5687",
   "metadata": {},
   "source": [
    "# Langchain test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1be20e-ba74-42e4-a144-3d05841998bd",
   "metadata": {},
   "source": [
    "---\n",
    "## Documentation\n",
    "\n",
    "> LangChain resources\n",
    "> - Landpage: https://readthedocs.org/projects/langchain/db2d\n",
    "> - git: https://github.com/hwchase17/langchain.git\n",
    "> - API Reference: https://api.python.langchain.com/en/latest/\n",
    "\n",
    "> Tutos\n",
    "> - https://towardsdatascience.com/a-gentle-intro-to-chaining-llms-agents-and-utils-via-langchain-16cd385fca81\n",
    "> - videos Greg Kamradt on YouTube\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006f2f80-db93-4c99-812b-1873e99e7850",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51de5b76-5e76-4c8f-bd11-9445f98c6941",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8713a09-0a7a-416c-807a-e2560796f7d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.0.216-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /opt/conda/lib/python3.10/site-packages (from langchain) (5.4.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.4.39)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Using cached aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
      "  Using cached dataclasses_json-0.5.8-py3-none-any.whl (26 kB)\n",
      "Collecting langchainplus-sdk>=0.0.17 (from langchain)\n",
      "  Using cached langchainplus_sdk-0.0.17-py3-none-any.whl (25 kB)\n",
      "Collecting numexpr<3.0.0,>=2.8.4 (from langchain)\n",
      "  Using cached numexpr-2.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (381 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.24.3)\n",
      "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
      "  Using cached openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
      "Collecting pydantic<2,>=1 (from langchain)\n",
      "  Using cached pydantic-1.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain)\n",
      "  Using cached tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
      "  Using cached marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
      "Collecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
      "  Using cached marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<2,>=1->langchain) (4.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (1.1.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (21.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (0.4.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (3.0.9)\n",
      "Installing collected packages: typing-inspect, tenacity, pydantic, numexpr, multidict, frozenlist, async-timeout, yarl, openapi-schema-pydantic, marshmallow, langchainplus-sdk, aiosignal, marshmallow-enum, aiohttp, dataclasses-json, langchain\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.0.1\n",
      "    Uninstalling tenacity-8.0.1:\n",
      "      Successfully uninstalled tenacity-8.0.1\n",
      "  Attempting uninstall: numexpr\n",
      "    Found existing installation: numexpr 2.8.3\n",
      "    Uninstalling numexpr-2.8.3:\n",
      "      Successfully uninstalled numexpr-2.8.3\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 dataclasses-json-0.5.8 frozenlist-1.3.3 langchain-0.0.216 langchainplus-sdk-0.0.17 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 numexpr-2.8.4 openapi-schema-pydantic-1.2.4 pydantic-1.10.9 tenacity-8.2.2 typing-inspect-0.9.0 yarl-1.9.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bc38381-5ee0-4107-9b3a-40dd9cb9302e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Using cached openai-0.27.8-py3-none-any.whl (73 kB)\n",
      "Requirement already satisfied: requests>=2.20 in /opt/conda/lib/python3.10/site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (2023.5.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Installing collected packages: openai\n",
      "Successfully installed openai-0.27.8\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4397c800-3ee5-40aa-b0a7-5021985e334a",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## API keys and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6671d484-9c5d-4818-8449-6df0189fd307",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash --out secrets \n",
    "# using AWS's Secret Manager to store keys\n",
    "# garb the keys and store it into a Pytthon variable\n",
    "export RESPONSE=$(aws secretsmanager get-secret-value --secret-id 'labbenach/sednara/api_keys' )\n",
    "export SECRETS=$( echo $RESPONSE | jq '.SecretString | fromjson')\n",
    "\n",
    "echo $SECRETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be008522-bd17-47d4-bd6d-58da90214050",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = eval(secrets)[\"OPENAI_API_KEY\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6986800-5d16-496f-aaa2-3e48587a492f",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"background-color:green;color:black;text-align:center;padding:1rem;font-size:1.5rem;\">LANGCHAIN OVERVIEW</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6bd9f8-efef-4797-ba5d-9eb135e6b862",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Basic features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da068c2f-26b2-4c5e-b72c-23a1d50c8543",
   "metadata": {},
   "source": [
    "---\n",
    "## Get prediction from a langage model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "68e8dd21-8ada-4cd4-a532-43c2e1c3fc63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Germany \n",
      "2. Switzerland \n",
      "3. Denmark \n",
      "4. Norway \n",
      "5. Luxembourg\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "# OPENAI_API_KEY is requested. Get it from the OpenAI site.\n",
    "# a paid account and available units are requested to be able to place a request.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "text = \"what are the 5 best countries in Europe\"\n",
    "\n",
    "# Actual API call - may tale a while.\n",
    "print(llm(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17250fa0-fe5d-47c1-9406-9822dd38fc9e",
   "metadata": {},
   "source": [
    "---\n",
    "## Manage prompts with templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2bdf0d54-abaa-4aa6-aeb5-275bebb5246b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# setup a prompt\n",
    "prompt = PromptTemplate (\n",
    "    input_variables=[\"interest\"],\n",
    "    template=\"what are the 5 best countries in Europe ranked by {interest}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f09393d7-6674-42f3-97c0-08cd12439d3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='what are the 5 best countries in Europe ranked on food'\n",
      "\n",
      "\n",
      "1. Italy \n",
      "2. France \n",
      "3. Spain \n",
      "4. Greece \n",
      "5. Portugal\n"
     ]
    }
   ],
   "source": [
    "text = prompt.format(interest=\"food\")\n",
    "print(f\"{text=}\")\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "87e3e1e6-871b-42e3-9106-96249b3f726e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='what are the 5 best countries in Europe ranked on siteseeing'\n",
      "\n",
      "\n",
      "1. Italy\n",
      "2. France\n",
      "3. Spain\n",
      "4. Greece\n",
      "5. United Kingdom\n"
     ]
    }
   ],
   "source": [
    "text = prompt.format(interest=\"siteseeing\")\n",
    "print(f\"{text=}\")\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c344d1-9478-46f0-a5df-20ff5644e011",
   "metadata": {},
   "source": [
    "---\n",
    "## Prompt with multiple tokens \n",
    "<div class=\"alert alert-block alert-warning\"> TODO </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db89d7fa-10d1-495d-8e89-24ab1dce738f",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ae61a5-823a-4585-b308-8e83ea8b6a7a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO  what is a chain </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d607ac-9d7f-40f3-86f4-243670f42fe2",
   "metadata": {},
   "source": [
    "---\n",
    "## Built-in chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0d68f8d0-4ee4-48c5-a6b6-5f515c581eea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mdef solution():\n",
      "    \"\"\"If my age is half of my dad's age and he is going to be 60 next year, what is my current age?\"\"\"\n",
      "    dad_age_next_year = 60\n",
      "    my_age = dad_age_next_year / 2\n",
      "    result = my_age\n",
      "    return result\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'30.0'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import PALChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "palchain = PALChain.from_math_prompt(llm=llm, verbose=True)\n",
    "\n",
    "\n",
    "text = \"\"\"If my age is half of my dad's age \n",
    "and he is going to be 60 next year, \n",
    "what is my current age?\"\"\"\n",
    "#palchain.run(\"If my age is half of my dad's age and he is going to be 60 next year, what is my current age?\")\n",
    "palchain.run(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ed407-348c-4c00-a405-c8233fe07bb4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "    TODO <br>\n",
    "    - different result each run <br>\n",
    "    - and should be 29.5\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8337725d-b79a-41b9-99f8-d7d55339c149",
   "metadata": {},
   "source": [
    "> Entering new  chain...\n",
    "def solution():\n",
    "    \"\"\"If my age is half of my dad's age and he is going to be 60 next year, what is my current age?\"\"\"\n",
    "    dad_age_next_year = 60\n",
    "    my_age_fraction = 0.5\n",
    "    my_age_now = dad_age_next_year * my_age_fraction\n",
    "    result = my_age_now\n",
    "    return result\n",
    "\n",
    "> Finished chain.\n",
    "'30.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8acdd9d-e9d5-4ca1-b3db-6e8b2a47d81b",
   "metadata": {},
   "source": [
    "> Entering new  chain...\n",
    "def solution():\n",
    "    \"\"\"If my age is half of my dad's age and he is going to be 60 next year, what is my current age?\"\"\"\n",
    "    dad_age_current = 59\n",
    "    my_age_current = dad_age_current / 2\n",
    "    result = my_age_current\n",
    "    return result\n",
    "\n",
    "> Finished chain.\n",
    "'29.5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b09463-2f2e-4840-9602-1a22478bdd5c",
   "metadata": {},
   "source": [
    "---\n",
    "## Multi-step workflow to feed prompt into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3fe1a4f0-4c08-47aa-9304-0b6a7002968b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# setup a prompt\n",
    "prompt = PromptTemplate (\n",
    "    input_variables=[\"interest\"],\n",
    "    template=\"what are the 5 best countries in Europe ranked on {interest}\"\n",
    ")\n",
    "\n",
    "# chain feeds the prompt into the langage mmodel.\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "97622a8b-eb22-4218-803d-34d1a76c1af2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n1. Germany\\n2. Sweden\\n3. Switzerland\\n4. United Kingdom\\n5. Netherlands'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7f73c33b-634d-48fb-9552-7329658c4e23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. United Kingdom\n",
      "2. France\n",
      "3. Germany\n",
      "4. Italy\n",
      "5. Spain\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"tv shows\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c72f8b-7623-4e34-a2b2-c8bf785fa719",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Using OpenAI Chat API (less expensive)\n",
    "requires a chain to feed the prompt into the chat \n",
    "\n",
    "Other Chat APIs\n",
    "- https://api.python.langchain.com/en/latest/modules/chat_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f18f9ea2-c494-462a-aa3e-5e7ca5952d31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI language model, I do not have personal preferences. However, the following are five countries in Europe that are known for their delicious cuisine:\n",
      "\n",
      "1. Italy - Italian cuisine is famous for its pasta, pizza, gelato, and wines. Italy is also known for its flavorful seafood and meat dishes.\n",
      "\n",
      "2. France - French cuisine is renowned for its delicate flavors and rich sauces. It includes dishes such as coq au vin, ratatouille, and escargots.\n",
      "\n",
      "3. Spain - Spanish cuisine is known for its tapas, paella, and seafood dishes. It also features delicious cured meats and cheeses.\n",
      "\n",
      "4. Greece - Greek cuisine is characterized by fresh vegetables, grilled meats, and flavorful dips such as tzatziki and hummus. Greek cuisine also includes dishes like moussaka and souvlaki.\n",
      "\n",
      "5. Turkey - Turkish cuisine is a fusion of Middle Eastern and Mediterranean flavors. It includes dishes such as kebabs, baklava, and Turkish delight. Turkish cuisine also features delicacies like stuffed grape leaves and Turkish coffee.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chatopenai = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "prompt = PromptTemplate (\n",
    "    input_variables=[\"interest\"],\n",
    "    template=\"what are the 5 best countries in Europe ranked on {interest}\"\n",
    ")\n",
    "\n",
    "llmchain_chat = LLMChain(llm=chatopenai, prompt=prompt)\n",
    "print(llmchain_chat.run(\"food\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3204392e-b1de-47f0-983e-ae05901179ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Leverage LLM Math\n",
    "\n",
    "Evaluating chains that know how to do math.\n",
    "\n",
    "https://python.langchain.com/docs/guides/evaluation/llm_math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "26ed6528-f540-4b2f-9617-e5ae8ffe6d26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `_type` key found, defaulting to `prompt`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 19\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "from langchain.chains import LLMMathChain\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "prompt = load_prompt('lc://prompts/llm_math/prompt.json')\n",
    "\n",
    "# deprecated\n",
    "##chain = LLMMathChain(llm=llm, prompt=prompt)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "print(chain.run(\"what is the largest prime number lower than 20\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899d9cb9-c483-4c79-ae73-cafbacb19b7d",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522dcef8-4941-4d62-94f0-6b8dd132589e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO  what is a tool </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ea5a95-b5c1-4221-afb1-b83f86ded7a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Leverage Goocle Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31520149-8e0f-4b08-b4a5-08c118fb638f",
   "metadata": {
    "tags": []
   },
   "source": [
    ">How to configure the Google search in Langchain \n",
    "> - https://python.langchain.com/docs/ecosystem/integrations/google_search\n",
    "\n",
    "> Custom Search Engine configuration \n",
    "> - https://stackoverflow.com/questions/37083058/programmatically-searching-google-in-python-using-custom-search\n",
    "\n",
    "> CSE API \n",
    "> - repo: https://github.com/google/google-api-python-client\n",
    "> - more info: https://developers.google.com/api-client-library/python/apis/customsearch/v1\n",
    "> - complete docs: https://api-python-client-doc.appspot.com/\n",
    "\n",
    "> Get an API key\n",
    "> - https://developers.google.com/custom-search/v1/introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2e169ba1-9d2d-4bfa-843c-afe61e1228e2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-api-python-client\n",
      "  Using cached google_api_python_client-2.90.0-py2.py3-none-any.whl (11.4 MB)\n",
      "Collecting httplib2<1.dev0,>=0.15.0 (from google-api-python-client)\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Collecting google-auth<3.0.0.dev0,>=1.19.0 (from google-api-python-client)\n",
      "  Using cached google_auth-2.20.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting google-auth-httplib2>=0.1.0 (from google-api-python-client)\n",
      "  Using cached google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 (from google-api-python-client)\n",
      "  Using cached google_api_core-2.11.1-py3-none-any.whl (120 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client)\n",
      "  Using cached uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Using cached googleapis_common_protos-1.59.1-py2.py3-none-any.whl (224 kB)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.20.3)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.31.0)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client)\n",
      "  Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (4.7.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (1.16.0)\n",
      "Collecting urllib3<2.0 (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client)\n",
      "  Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.10/site-packages (from httplib2<1.dev0,>=0.15.0->google-api-python-client) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2023.5.7)\n",
      "Installing collected packages: urllib3, uritemplate, httplib2, googleapis-common-protos, cachetools, google-auth, google-auth-httplib2, google-api-core, google-api-python-client\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.3\n",
      "    Uninstalling urllib3-2.0.3:\n",
      "      Successfully uninstalled urllib3-2.0.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed cachetools-5.3.1 google-api-core-2.11.1 google-api-python-client-2.90.0 google-auth-2.20.0 google-auth-httplib2-0.1.0 googleapis-common-protos-1.59.1 httplib2-0.22.0 uritemplate-4.1.1 urllib3-1.26.16\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "09c65606-95d2-4b96-813f-db44b9ffcbb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Unlock the API and get a key \n",
    "os.environ[\"GOOGLE_API_KEY\"] = eval(secrets)[\"GOOGLE_API_KEY\"]\n",
    "# Create or use an existing Custom Search Engine\n",
    "# on the CSE page under Searcg Engone ID\n",
    "os.environ[\"GOOGLE_CSE_ID\"] = eval(secrets)[\"GOOGLE_CSE_ID\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f98c7adc-982e-4693-bb7c-ee7a3e1d2e1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Élisabeth Borne has served as Prime Minister since 16 May 2022. Fifth Republic recordsEdit. Length of the successive governments\\xa0... May 16, 2022 ... President Emmanuel Macron has named Labour Minister Elisabeth Borne as prime minister to lead his ambitious reform plans, the first woman to\\xa0... Feb 22, 2018 ... SEVEN months after their prime minister was appointed in May 2017, fully 35% of the French could not name him accurately in a poll. May 16, 2022 ... Elisabeth Borne has been named the new Prime Minister of France, the first time in 30 years that a woman has held the position. May 16, 2022 ... Élisabeth Borne, the minister of labor who previously was in charge of the environment, will be the second woman to hold the post in France. Jun 24, 2022 ... The name of Lafayette is famous and respected on both sides of the Atlantic. It is our third conversation in a month, which is quite a good\\xa0... May 5, 2017 ... France's Macron says he has chosen prime minister, won't reveal name ... PARIS (Reuters) - French presidential election front-runner Emmanuel\\xa0... May 16, 2022 ... 6:30pm: Macron names minister Elisabeth Borne new French PM. President Emmanuel Macron on Monday named Labour Minister Elisabeth Borne as his\\xa0... Apr 25, 2022 ... Castex could have been asked to stay on as prime minister, but told French radio earlier this month that France would be looking for a “new\\xa0... May 16, 2022 ... PARIS (AP) — Centrist politician Elisabeth Borne was appointed France's new prime minister on Monday, becoming only the second woman in\\xa0...\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "search = GoogleSearchAPIWrapper()\n",
    "\n",
    "tool = Tool(\n",
    "    name=\"Google Search\",\n",
    "    description=\"Search Google for recent results.\",\n",
    "    func=search.run,\n",
    ")\n",
    "\n",
    "tool.run(\"French Prime Minister name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb52e51-3733-4ff5-a8cb-1094a391b98f",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543f13ea-82c7-4522-91b9-453183bb7310",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO  what is an agent </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c68946-559e-497d-a971-45c473ea5770",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c64311c2-8e52-4278-a4ac-42a22bceec08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# create a model\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# load some tools\n",
    "tools = load_tools([\"google-search\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# setup an agent\n",
    "agent = initialize_agent(tools, \n",
    "                         llm, \n",
    "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "                         verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ff155cf5-99f9-4ba4-8dbc-9d9366825475",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out how many Teslas have been sold in 2022\n",
      "Action: google_search\n",
      "Action Input: \"how many Teslas have been sold in 2022\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApr 15, 2023 ... Tesla total revenue for 2022 was 81,462 billion USD. We show it from 2018 – 2022. Tesla annual revenue 2018 - 2022. Year, Annual ... Jun 7, 2023 ... How many Tesla vehicles were delivered in 2023? ... As of June 2022, Tesla was the most valuable brand within the global automotive sector. Jan 25, 2023 ... The Model 3 and Model Y make up around 95% of the 1.31 million Teslas sold in 2022. Tesla. Tesla's finished 2022 on a tear, bolstered by recent ... Jan 7, 2023 ... Overall, Tesla reported delivering about 1.25 million Model Y and Model 3 vehicles globally in 2022. The Model 3 ranked 13th in sales at 211,641 ... Jan 3, 2023 ... The electric automaker delivered 1.3 million vehicles in 2022, up 40% from 2021. It produced nearly 1.4 million vehicles, up 47% from the prior ... May 30, 2022 ... If every vehicle sold by Tesla since its inception up to the most recent Tesla sales figures released for Q1 of 2022 are accounted for, there ... Jul 23, 2022 ... In 2022, Tesla sold 1,2 million Model 3s and Ys combined. The last sales record was around 900K in 2021, which translates to a more than 30% ... It left behind BMW, which had worn this crown for the last three years but sold only 332,388 cars in the United States in 2022 and was thus relegated to second ... Nov 4, 2022 ... As of October 2022, Tesla, the Texas-based EV automaker, has sold more than 3 million units on a global scale. The company has found that ... Jan 2, 2023 ... AUSTIN, Texas, January 2, 2023 – In the fourth quarter, we produced over 439000 vehicles and delivered over 405000 vehicles. In 2022 ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the number of Teslas sold in 2022\n",
      "Action: Calculator\n",
      "Action Input: 1,310,000 * 2\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2620000\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 2,620,000 Teslas were sold in 2022.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2,620,000 Teslas were sold in 2022.'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"How many Teslas have been sold in 2022. Multiple by 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13982ee0-5296-4ee4-ad84-d7e8f795bf96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "20688891-5c6e-4279-bfa4-8914980abb90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out who the current prime minister is and then compare their age to the President.\n",
      "Action: google_search\n",
      "Action Input: \"current prime minister of France\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPresentEdit. Élisabeth Borne has served as Prime Minister since 16 May 2022. Fifth Republic records ... May 16, 2022 ... Who is France's new Prime Minister Elisabeth Borne? French President Emmanuel Macron picked Labour Minister Elisabeth Borne as his new prime ... The current Prime Minister of France is Élisabeth Borne. She was given the job by President Emmanuel Macron on 16 May 2022. May 16, 2022 ... President Emmanuel Macron has named Labour Minister Elisabeth Borne as prime minister to lead his ambitious reform plans, the first woman to ... May 16, 2022 ... Élisabeth Borne, the minister of labor who previously was in charge of the environment, will be the second woman to hold the post in France. May 2, 2014 ... On the recommendation of the Prime Minister, President Hollande has appointed the following ministers: ; Mr Benoit HAMON, Minister of Education ... Feb 8, 2023 ... France's prime minister, Élisabeth Borne, sat on a recent, rainy evening in a dim room at a Red Cross shelter, listening to young women ... Jun 12, 2023 ... Élisabeth Borne, Prime Minister of France. France Élisabeth Borne · Olaf Scholz, Chancellor of the Federal Republic of Germany. Jan 10, 2023 ... France's Prime Minister Elisabeth Borne attends a press conference to present the government's plan for a. By —. Sylvie Corbet, Associated Press ... Mar 31, 2023 ... ... keep Élisabeth Borne as prime minister. Macron is adamant that he will, though Borne needs to find a way out of the current impasse.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to find out the age of the President and compare it to the Prime Minister.\n",
      "Action: google_search\n",
      "Action Input: \"age of French President\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAt the age of 39, Macron became the youngest president in French history. In the 2017 legislative election in June, Macron's party, renamed La République En ... Mar 16, 2023 ... PARIS (AP) — French President Emmanuel Macron ordered his prime minister to wield a special constitutional power Thursday that skirts ... Apr 15, 2023 ... French President Emmanuel Macron has signed into law his government's highly unpopular pension reforms, which raise the state pension age from ... Apr 14, 2023 ... PARIS — French President Emmanuel Macron's unpopular plan to raise France's retirement age from 62 to 64 was enacted into law Saturday, ... May 3, 2017 ... Who is Emmanuel Macron? ... PARIS — Emmanuel Macron, the front-runner in Sunday's French presidential election, shares something with President ... Aug 19, 2017 ... &#151; -- French first lady Brigitte Macron is speaking out about the ... her and husband Emmanuel Macron, France's 39-year-old president. Mar 16, 2023 ... French President Emmanuel Macron on Thursday resorted to using special constitutional powers to push his plan to raise the retirement age to 64 ... May 2, 2017 ... If Mr Macron is successful, the 39-year-old would become the youngest president in France's history since the election in 1848 of Napoléon ... Mar 16, 2023 ... French President Emmanuel Macron raised the retirement age in France on Thursday, from 62 to 64, without waiting for a legislative vote. In the second round, held on May 7, 2017, Macron won a convincing two-thirds of the vote, becoming, at age 39, France's youngest president.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The current Prime Minister of France, Élisabeth Borne, is younger than the President, Emmanuel Macron, who is 39 years old.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The current Prime Minister of France, Élisabeth Borne, is younger than the President, Emmanuel Macron, who is 39 years old.'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"\"\"Who is the current prime minister of France. \n",
    "Is he or sheyounger than the President?\"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c456a279",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out who the current prime minister is and when they will be 70.\n",
      "Action: google_search\n",
      "Action Input: \"current prime minister of France\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPresentEdit. Élisabeth Borne has served as Prime Minister since 16 May 2022. Fifth Republic records ... May 16, 2022 ... Who is France's new Prime Minister Elisabeth Borne? French President Emmanuel Macron picked Labour Minister Elisabeth Borne as his new prime ... The current Prime Minister of France is Élisabeth Borne. She was given the job by President Emmanuel Macron on 16 May 2022. May 16, 2022 ... President Emmanuel Macron has named Labour Minister Elisabeth Borne as prime minister to lead his ambitious reform plans, the first woman to ... May 16, 2022 ... Élisabeth Borne, the minister of labor who previously was in charge of the environment, will be the second woman to hold the post in France. May 2, 2014 ... On the recommendation of the Prime Minister, President Hollande has appointed the following ministers: ; Mr Benoit HAMON, Minister of Education ... Feb 8, 2023 ... France's prime minister, Élisabeth Borne, sat on a recent, rainy evening in a dim room at a Red Cross shelter, listening to young women ... Jun 12, 2023 ... Élisabeth Borne, Prime Minister of France. France Élisabeth Borne · Olaf Scholz, Chancellor of the Federal Republic of Germany. Jan 10, 2023 ... France's Prime Minister Elisabeth Borne attends a press conference to present the government's plan for a. By —. Sylvie Corbet, Associated Press ... Mar 31, 2023 ... ... keep Élisabeth Borne as prime minister. Macron is adamant that he will, though Borne needs to find a way out of the current impasse.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to calculate when Élisabeth Borne will be 70.\n",
      "Action: Calculator\n",
      "Action Input: 16 May 2022 + 70 years\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2092\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: Élisabeth Borne will be 70 in the year 2092.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Élisabeth Borne will be 70 in the year 2092.'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"\"\"Who is the current prime minister of France. \n",
    "When will he or she be 70?\"\"\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28631b0b-d1ec-4d14-bf44-091f8abd1584",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Memory - Conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bde2f8-060f-4da3-a69f-24df8df6aa63",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO  what is a conversation </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "13541ec0-684d-48da-a205-ef1c6e3e7937",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi There\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi there! It's nice to meet you. How can I help you today?\""
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import OpenAI, ConversationChain\n",
    "\n",
    "# create a model\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "conversation.predict(input=\"Hi There\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7271c38f-0a3e-44fb-9d8a-1014667c3467",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi There\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: What is the first thing that I said to you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' You said \"Hi there!\"'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is the first thing that I said to you?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e2613f8b-beb3-47b3-b842-e5f2b4746143",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi There\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: What is the first thing that I said to you?\n",
      "AI:  You said \"Hi there!\"\n",
      "Human: What is an alternative for the first thing that I said to you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' An alternative for the first thing you said to me is \"Hello!\"'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is an alternative for the first thing that I said to you?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3912d8-050d-4c4a-89e8-1d9ce0a1f4cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "<div style=\"background-color:green;color:black;text-align:center;padding:1rem;font-size:1.5rem;\">LANGCHAIN COMPONENTS</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684914b4-7b16-4f0b-bde2-6ef6c7df5079",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Schemas\n",
    "\n",
    "There are 3 types of schemas\n",
    "- text (see above)\n",
    "- Messages \n",
    "- Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7bb2c3-a6cd-4fee-b6a8-964a5f811433",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8843ce9-1f71-4b43-a704-d456dc5653e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Switzerland\n",
      "2. Germany\n",
      "3. Netherlands\n",
      "4. United Kingdom\n",
      "5. Norway\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "# OPENAI_API_KEY is requested. Get it from the OpenAI site.\n",
    "# a paid account and available units are requested to be able to place a request.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "text = \"what are the 5 best countries in Europe\"\n",
    "\n",
    "# Actual API call - may tale a while.\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf610897-44b3-4d72-8e38-3dd50b0776bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Chat messages\n",
    "Chat messages are like text with a type\n",
    "\n",
    "There are 3 types\n",
    "- System: background context that tells the AI what to do\n",
    "- Human: inputs sent by the user\n",
    "- AI : response of the AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c8d5a96-f105-45f2-ba15-0f0b108cd9ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f7a0269-e45c-445b-b44f-b19fc4be5e53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [ SystemMessage(content=\"You are a nice AI and help users to feature out what to eat.\")]\n",
    "     \n",
    "messages.append( HumanMessage(content=\"I like tuna, list some recipes.\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e203228-09cd-4405-9679-c4eebfbd7c1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here are some tuna recipes that you might enjoy:\n",
      "\n",
      "1. Tuna salad: Mix canned tuna with chopped celery, onions, and mayonnaise. You can also add some chopped pickles, mustard, and salt and pepper to taste. Serve on a bed of lettuce or between two slices of bread.\n",
      "\n",
      "2. Tuna melt: Spread canned tuna on a slice of bread, top with sliced tomato and cheese, and broil until the cheese is melted and bubbly.\n",
      "\n",
      "3. Tuna pasta salad: Combine cooked pasta with canned tuna, chopped vegetables like bell peppers and onions, and a dressing made of mayonnaise, lemon juice, and herbs.\n",
      "\n",
      "4. Tuna patties: Mix canned tuna with bread crumbs, egg, and seasonings like garlic, onion powder, and parsley. Form into patties and pan-fry until golden brown.\n",
      "\n",
      "5. Tuna poke bowl: Top cooked rice with cubed raw tuna, avocado, cucumber, and edamame. Drizzle with a soy sauce-based dressing and garnish with sesame seeds.\n",
      "\n",
      "I hope these ideas help! Let me know if you have any specific dietary restrictions or preferences, and I can suggest more personalized recipes.\n"
     ]
    }
   ],
   "source": [
    "response = chat(messages)\n",
    "messages.append( AIMessage(content=response.content) )\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d140f70f-a86d-4634-899e-12401f0315cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here is a recipe for tuna salad:\n",
      "\n",
      "Ingredients:\n",
      "- 2 cans of tuna, drained\n",
      "- 2 stalks of celery, chopped\n",
      "- 1 small onion, chopped\n",
      "- 1/4 cup mayonnaise\n",
      "- 1 tablespoon chopped pickles (optional)\n",
      "- 1 teaspoon mustard (optional)\n",
      "- Salt and pepper to taste\n",
      "\n",
      "Instructions:\n",
      "1. In a mixing bowl, combine the drained tuna, chopped celery, and chopped onion.\n",
      "2. Add the mayonnaise, pickles, and mustard (if using) to the bowl and mix well until everything is combined.\n",
      "3. Season with salt and pepper to taste.\n",
      "4. Serve the tuna salad on a bed of lettuce or between two slices of bread.\n",
      "\n",
      "Enjoy!\n"
     ]
    }
   ],
   "source": [
    "messages.append( HumanMessage(content=\"show the first one.\") )\n",
    "\n",
    "response = chat(messages)\n",
    "messages.append( AIMessage(content=response.content) )\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af92d18-1d2b-4c55-aa71-e5a91fde59bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Examples\n",
    "An list of input output pairs thet represent the input and expected output.\n",
    "\n",
    "Used to fine tune a model or do in-context learning.\n",
    "\n",
    "Documentation: https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/few_shot_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca97e03b-8d6c-4156-8358-a9504901d733",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== exemple prompt ===\n",
      "question: red bold\n",
      "color:red; font-style:bold;\n",
      "\n",
      "=== prompt ===\n",
      "question: red bold\n",
      "color:red; font-style:bold;\n",
      "\n",
      "question: green italic\n",
      "color:green; font-style:italic;\n",
      "\n",
      "question: blue bold\n",
      "color:blue; font-style:bold;\n",
      "\n",
      "question: pink\n",
      "color:pink;\n",
      "\n",
      "question: green\n",
      "color:green;\n",
      "\n",
      "question: pink italic\n",
      "color:pink; font-style:italic;\n",
      "\n",
      "question: pink bold\n",
      "\n",
      "=== answer ===\n",
      "\n",
      "color:pink; font-style:bold;\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# create the example set\n",
    "\n",
    "examples = [\n",
    "    { \"question\": \"red bold\", \"answer\": \"color:red; font-style:bold;\"},\n",
    "    { \"question\": \"green italic\", \"answer\":  \"color:green; font-style:italic;\"},\n",
    "    { \"question\": \"blue bold\", \"answer\":  \"color:blue; font-style:bold;\"},\n",
    "    { \"question\": \"pink\", \"answer\":  \"color:pink;\"},\n",
    "    { \"question\": \"green\", \"answer\":  \"color:green;\"},\n",
    "    { \"question\": \"pink italic\", \"answer\":  \"color:pink; font-style:italic;\"}\n",
    "    \n",
    "]    \n",
    "\n",
    "# Configure a formatter that will format the few shot examples into a string. \n",
    "# This formatter should be a PromptTemplate object.\n",
    "\n",
    "example_prompt = PromptTemplate (\n",
    "    input_variables=[\"question\", \"answer\"], \n",
    "    template=\"question: {question}\\n{answer}\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== exemple prompt ===\")\n",
    "print(example_prompt.format(**examples[0]))\n",
    "\n",
    "\n",
    "# Finally, create a FewShotPromptTemplate object. \n",
    "# This object takes in the few shot examples and the formatter for the few shot examples.\n",
    "\n",
    "prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples, \n",
    "    example_prompt=example_prompt, \n",
    "    suffix=\"question: {input}\", \n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format(input=\"pink bold\")\n",
    "\n",
    "print(\"\\n=== prompt ===\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"\\n=== answer ===\")\n",
    "print(llm(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2471f9d0-6000-4f48-ad1a-f5e187a8aa54",
   "metadata": {},
   "source": [
    "---\n",
    "## Documents\n",
    "\n",
    "An unstructured object that conaints a pieces of text and metadatas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648ce2f7-0d40-49ea-b0a8-6e45f9b5f2f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO how to use this concept? \n",
    "make some knowledge available?\n",
    "how to use metadata?\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "097d47df-18e7-4a58-b76c-d7438d3135bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='This is my document. it contains useful information', metadata={'author': 'Claude', 'identifier': '1234'})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "Document(\n",
    "    page_content=\"This is my document. it contains useful information\",\n",
    "    metadata={\n",
    "        'author':\"Claude\",\n",
    "        'identifier':\"1234\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee98eca-37a3-45c4-90e5-eb2360e1f5e1",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Models\n",
    "\n",
    "List of models: https://platform.openai.com/docs/models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6a1df5-7967-4841-b1eb-1ff58d0a15cb",
   "metadata": {},
   "source": [
    "---\n",
    "## Langage Model \n",
    "Text in Text out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddd6cc95-2731-47d9-8624-2392e3a83315",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSaturday.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# additnal parameters to select a mode, pass the API key ...\n",
    "llm = OpenAI(model_name=\"text-ada-001\", temperature=0.7)\n",
    "\n",
    "llm(\"What day comes after Friday?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981bf185-6a93-4eb5-bdc4-779724aab952",
   "metadata": {},
   "source": [
    "---\n",
    "## Chat Model \n",
    "Takes a series of messages and return an AI response\n",
    "\n",
    "Also make sense for a unique interaction as Chat API is less expensive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2099b96-d00a-4aa0-908d-e75a0e4b3a16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daafb170-e231-4132-bcc2-c88aac2fe475",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sure, here are some delicious tuna recipes you can try: \\n\\n1. Tuna Salad: Mix canned tuna with mayonnaise, pickle relish, chopped celery and onion. Serve on salad greens, in a sandwich or on crackers for a light lunch.\\n\\n2. Tuna Nicoise Salad: Top a bed of salad greens with cooked green beans, boiled potatoes, hard-boiled eggs, canned tuna, and cherry tomatoes. Toss with a simple vinaigrette for a healthy Mediterranean-inspired meal.\\n\\n3. Tuna Melt: Arrange tuna salad on a slice of crusty bread, top with sliced tomato and cheese, and broil until melted and bubbly.\\n\\n4. Tuna Poke Bowl: Combine cubed raw tuna with soy sauce, sesame oil, lime juice, green onions, and sesame seeds. Serve over steamed rice with sliced avocado and edamame.\\n\\n5. Grilled Tuna Steaks: Brush fresh tuna steaks with olive oil and season with salt and pepper. Grill for a few minutes on each side until cooked to your liking, and serve with a side of sautéed vegetables.\\n\\nI hope these ideas inspire you to create some tasty dishes with tuna!', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [ \n",
    "    SystemMessage(content=\"You are a nice AI and help users to feature out what to eat.\"),\n",
    "    HumanMessage(content=\"I like tuna, list some recipes.\")\n",
    "]\n",
    "     \n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91de6b77-6cb7-414b-a475-c471c3ca6c5b",
   "metadata": {},
   "source": [
    "---\n",
    "### Text Embedding Model\n",
    "\n",
    "Convert text into a series of numbers (a vector) which holds the meaning of the text.\n",
    "\n",
    "Mainly used for text comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e7d5813-a505-47d6-a8ef-98881a175c46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding length: 1536\n",
      "5 first values of the vector: [-0.0020272971596568823, -0.016961609944701195, 0.013975410722196102, -0.014824817888438702, 0.001639920868910849]\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "text=\"A leader should know all about truth and honesty, and when to see the difference. (Truck) - Bromeliad Trilogy\"\n",
    "\n",
    "text_embedding = embeddings.embed_query(text)\n",
    "\n",
    "print(f\"embedding length: {len(text_embedding)}\")\n",
    "print(f\"5 first values of the vector: {text_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab0bbcc-adc2-449f-97e4-839a36b3d4f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# 8. prompts\n",
    "Text sent the langage model\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc36ebe4-aba6-4cdb-850d-1937839c8608",
   "metadata": {},
   "source": [
    "---\n",
    "## Simple prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d37b8bcc-33ca-4deb-9845-81675d529758",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Today is Monday. Tomorrow is Wednesday.\n",
      "\n",
      "The statement is incorrect because tomorrow is Tuesday, not Wednesday.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0.9)\n",
    "\n",
    "# write a simple  prompt. use \"\"\" to allow multiline string.\n",
    "prompt = \"\"\"\n",
    "Today is Monday. Tomorrow is Wednesday.\n",
    "\n",
    "What is wrong with this statement?\n",
    "\"\"\"\n",
    "\n",
    "# query the model\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f8a344-70ab-471d-918e-ee98e7e4fdf1",
   "metadata": {},
   "source": [
    "---\n",
    "## Prompt with template and placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "837116ab-bf8b-49ed-bb3f-a896e14fb14d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt='\\n    Today is Monday. Tomorrow is Wednesday.\\n\\n    What is wrong with this statement?\\n    '\n",
      "\n",
      "This statement is incorrect because tomorrow is Tuesday, not Wednesday.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0.9)\n",
    "\n",
    "# setup a prompt. use \"\"\" to allow multiline string.\n",
    "template = PromptTemplate (\n",
    "    input_variables=[\"today\", \"tomorrow\"],\n",
    "    template=\"\"\"\n",
    "    Today is {today}. Tomorrow is {tomorrow}.\n",
    "\n",
    "    What is wrong with this statement?\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "prompt = template.format(today=\"Monday\", tomorrow=\"Wednesday\")\n",
    "print(f\"{prompt=}\")\n",
    "\n",
    "# query the model\n",
    "\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa35aff7-3105-4aed-82ab-7281c517eca3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt='\\n    Today is Thursday. Tomorrow is Friday.\\n\\n    What is wrong with this statement?\\n    '\n",
      "\n",
      "This statement is not incorrect; however, it does not accurately indicate what day it currently is.\n"
     ]
    }
   ],
   "source": [
    "prompt = template.format(today=\"Thursday\", tomorrow=\"Friday\")\n",
    "print(f\"{prompt=}\")\n",
    "\n",
    "# query the model\n",
    "\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc54ed33-925e-405f-8eaf-a4a57ed78771",
   "metadata": {},
   "source": [
    "---\n",
    "## Example selectors and Few Prompt Learning\n",
    "\n",
    "A way to select from a series of examples in few shot learning \n",
    "\n",
    "Documentation: https://api.python.langchain.com/en/latest/modules/example_selector.html\n",
    "Documentation: https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/few_shot_examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e40343-df4e-4b65-8951-8372060ca6d0",
   "metadata": {},
   "source": [
    "### Example selectors and Few Prompt Learning with NGra"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f315f07-af06-480f-80b0-23c56c61a67d",
   "metadata": {
    "tags": []
   },
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.example_selector import NGramOverlapExampleSelector\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# create the example set\n",
    "\n",
    "examples = [\n",
    "    { \"question\": \"red bold\", \"answer\": \"color:red; font-style:bold;\"},\n",
    "    { \"question\": \"green italic\", \"answer\":  \"color:green; font-style:italic;\"},\n",
    "    { \"question\": \"blue bold\", \"answer\":  \"color:blue; font-style:bold;\"},\n",
    "    { \"question\": \"pink\", \"answer\":  \"color:pink;\"},\n",
    "    { \"question\": \"green\", \"answer\":  \"color:green;\"},\n",
    "    { \"question\": \"pink italic\", \"answer\":  \"color:pink; font-style:italic;\"}\n",
    "    \n",
    "]    \n",
    "\n",
    "# Configure a formatter that will format the few shot examples into a string. \n",
    "# This formatter should be a PromptTemplate object.\n",
    "\n",
    "example_prompt = PromptTemplate (\n",
    "    input_variables=[\"question\", \"answer\"], \n",
    "    template=\"question: {question}\\n{answer}\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== exemple prompt ===\")\n",
    "print(example_prompt.format(**examples[0]))\n",
    "\n",
    "\n",
    "# Select and order examples based on ngram overlap score (sentence_bleu score).\n",
    "\n",
    "question = \"pink bold\"\n",
    "\n",
    "example_selector = NGramOverlapExampleSelector.select_examples(\n",
    "    examples,\n",
    "    question\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # This is the list of examples available to select from.\n",
    "    examples,\n",
    "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    OpenAIEmbeddings(),\n",
    "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    Chroma,\n",
    "    # This is the number of examples to produce.\n",
    "    k=1\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Finally, create a FewShotPromptTemplate object. \n",
    "# This object takes in the few shot examples and the formatter for the few shot examples.\n",
    "\n",
    "prompt_template = FewShotPromptTemplate(\n",
    "    #example_selector=example_selector, \n",
    "    examples=selected_examples, \n",
    "    example_prompt=example_prompt, \n",
    "    suffix=\"question: {input}\", \n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format(input=question)\n",
    "\n",
    "print(\"\\n=== prompt ===\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"\\n=== answer ===\")\n",
    "print(llm(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fcb411-2657-4e46-b373-5e1355066745",
   "metadata": {},
   "source": [
    "### setup a vector database \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d143b018-d870-428f-bfcc-837d900e873f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Install GCC\n",
    "\n",
    "packages below requires GCC to compile the wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e80afeb3-cc1a-4207-8a26-a319832a49ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://deb.debian.org/debian bullseye InRelease [116 kB]\n",
      "Get:2 http://security.debian.org/debian-security bullseye-security InRelease [48.4 kB]\n",
      "Get:3 http://deb.debian.org/debian bullseye-updates InRelease [44.1 kB]\n",
      "Get:4 http://security.debian.org/debian-security bullseye-security/main amd64 Packages [245 kB]\n",
      "Get:5 http://deb.debian.org/debian bullseye/main amd64 Packages [8183 kB]\n",
      "Get:6 http://deb.debian.org/debian bullseye-updates/main amd64 Packages [14.8 kB]\n",
      "Fetched 8651 kB in 2s (5485 kB/s)\n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  dirmngr dpkg-dev fakeroot g++ g++-10 gnupg gnupg-l10n gnupg-utils gpg\n",
      "  gpg-agent gpg-wks-client gpg-wks-server gpgconf gpgsm gpgv\n",
      "  libalgorithm-diff-perl libalgorithm-diff-xs-perl libalgorithm-merge-perl\n",
      "  libassuan0 libdpkg-perl libfakeroot libfile-fcntllock-perl libksba8\n",
      "  liblocale-gettext-perl libnpth0 libstdc++-10-dev make patch pinentry-curses\n",
      "  xz-utils\n",
      "Suggested packages:\n",
      "  dbus-user-session libpam-systemd pinentry-gnome3 tor debian-keyring\n",
      "  g++-multilib g++-10-multilib gcc-10-doc parcimonie xloadimage scdaemon bzr\n",
      "  libstdc++-10-doc make-doc ed diffutils-doc pinentry-doc\n",
      "The following NEW packages will be installed:\n",
      "  build-essential dirmngr dpkg-dev fakeroot g++ g++-10 gnupg gnupg-l10n\n",
      "  gnupg-utils gpg gpg-agent gpg-wks-client gpg-wks-server gpgconf gpgsm\n",
      "  libalgorithm-diff-perl libalgorithm-diff-xs-perl libalgorithm-merge-perl\n",
      "  libassuan0 libdpkg-perl libfakeroot libfile-fcntllock-perl libksba8\n",
      "  liblocale-gettext-perl libnpth0 libstdc++-10-dev make patch pinentry-curses\n",
      "  xz-utils\n",
      "The following packages will be upgraded:\n",
      "  gpgv\n",
      "1 upgraded, 30 newly installed, 0 to remove and 32 not upgraded.\n",
      "Need to get 24.3 MB of archives.\n",
      "After this operation, 70.6 MB of additional disk space will be used.\n",
      "Get:1 http://deb.debian.org/debian bullseye/main amd64 liblocale-gettext-perl amd64 1.07-4+b1 [19.0 kB]\n",
      "Get:2 http://deb.debian.org/debian bullseye/main amd64 gpgv amd64 2.2.27-2+deb11u2 [626 kB]\n",
      "Get:3 http://deb.debian.org/debian bullseye/main amd64 xz-utils amd64 5.2.5-2.1~deb11u1 [220 kB]\n",
      "Get:4 http://deb.debian.org/debian bullseye/main amd64 libstdc++-10-dev amd64 10.2.1-6 [1741 kB]\n",
      "Get:5 http://deb.debian.org/debian bullseye/main amd64 g++-10 amd64 10.2.1-6 [9380 kB]\n",
      "Get:6 http://deb.debian.org/debian bullseye/main amd64 g++ amd64 4:10.2.1-1 [1644 B]\n",
      "Get:7 http://deb.debian.org/debian bullseye/main amd64 make amd64 4.3-4.1 [396 kB]\n",
      "Get:8 http://deb.debian.org/debian bullseye/main amd64 libdpkg-perl all 1.20.12 [1551 kB]\n",
      "Get:9 http://deb.debian.org/debian bullseye/main amd64 patch amd64 2.7.6-7 [128 kB]\n",
      "Get:10 http://deb.debian.org/debian bullseye/main amd64 dpkg-dev all 1.20.12 [2312 kB]\n",
      "Get:11 http://deb.debian.org/debian bullseye/main amd64 build-essential amd64 12.9 [7704 B]\n",
      "Get:12 http://deb.debian.org/debian bullseye/main amd64 libassuan0 amd64 2.5.3-7.1 [50.5 kB]\n",
      "Get:13 http://deb.debian.org/debian bullseye/main amd64 gpgconf amd64 2.2.27-2+deb11u2 [548 kB]\n",
      "Get:14 http://deb.debian.org/debian bullseye/main amd64 libksba8 amd64 1.5.0-3+deb11u2 [123 kB]\n",
      "Get:15 http://deb.debian.org/debian bullseye/main amd64 libnpth0 amd64 1.6-3 [19.0 kB]\n",
      "Get:16 http://deb.debian.org/debian bullseye/main amd64 dirmngr amd64 2.2.27-2+deb11u2 [763 kB]\n",
      "Get:17 http://deb.debian.org/debian bullseye/main amd64 libfakeroot amd64 1.25.3-1.1 [47.0 kB]\n",
      "Get:18 http://deb.debian.org/debian bullseye/main amd64 fakeroot amd64 1.25.3-1.1 [87.0 kB]\n",
      "Get:19 http://deb.debian.org/debian bullseye/main amd64 gnupg-l10n all 2.2.27-2+deb11u2 [1086 kB]\n",
      "Get:20 http://deb.debian.org/debian bullseye/main amd64 gnupg-utils amd64 2.2.27-2+deb11u2 [905 kB]\n",
      "Get:21 http://deb.debian.org/debian bullseye/main amd64 gpg amd64 2.2.27-2+deb11u2 [928 kB]\n",
      "Get:22 http://deb.debian.org/debian bullseye/main amd64 pinentry-curses amd64 1.1.0-4 [64.9 kB]\n",
      "Get:23 http://deb.debian.org/debian bullseye/main amd64 gpg-agent amd64 2.2.27-2+deb11u2 [669 kB]\n",
      "Get:24 http://deb.debian.org/debian bullseye/main amd64 gpg-wks-client amd64 2.2.27-2+deb11u2 [524 kB]\n",
      "Get:25 http://deb.debian.org/debian bullseye/main amd64 gpg-wks-server amd64 2.2.27-2+deb11u2 [516 kB]\n",
      "Get:26 http://deb.debian.org/debian bullseye/main amd64 gpgsm amd64 2.2.27-2+deb11u2 [645 kB]\n",
      "Get:27 http://deb.debian.org/debian bullseye/main amd64 gnupg all 2.2.27-2+deb11u2 [825 kB]\n",
      "Get:28 http://deb.debian.org/debian bullseye/main amd64 libalgorithm-diff-perl all 1.201-1 [43.3 kB]\n",
      "Get:29 http://deb.debian.org/debian bullseye/main amd64 libalgorithm-diff-xs-perl amd64 0.04-6+b1 [12.0 kB]\n",
      "Get:30 http://deb.debian.org/debian bullseye/main amd64 libalgorithm-merge-perl all 0.08-3 [12.7 kB]\n",
      "Get:31 http://deb.debian.org/debian bullseye/main amd64 libfile-fcntllock-perl amd64 0.22-3+b7 [35.5 kB]\n",
      "Fetched 24.3 MB in 0s (109 MB/s)\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package liblocale-gettext-perl.\n",
      "(Reading database ... 21534 files and directories currently installed.)\n",
      "Preparing to unpack .../liblocale-gettext-perl_1.07-4+b1_amd64.deb ...\n",
      "Unpacking liblocale-gettext-perl (1.07-4+b1) ...\n",
      "Preparing to unpack .../gpgv_2.2.27-2+deb11u2_amd64.deb ...\n",
      "Unpacking gpgv (2.2.27-2+deb11u2) over (2.2.27-2+deb11u1) ...\n",
      "Setting up gpgv (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package xz-utils.\n",
      "(Reading database ... 21549 files and directories currently installed.)\n",
      "Preparing to unpack .../00-xz-utils_5.2.5-2.1~deb11u1_amd64.deb ...\n",
      "Unpacking xz-utils (5.2.5-2.1~deb11u1) ...\n",
      "Selecting previously unselected package libstdc++-10-dev:amd64.\n",
      "Preparing to unpack .../01-libstdc++-10-dev_10.2.1-6_amd64.deb ...\n",
      "Unpacking libstdc++-10-dev:amd64 (10.2.1-6) ...\n",
      "Selecting previously unselected package g++-10.\n",
      "Preparing to unpack .../02-g++-10_10.2.1-6_amd64.deb ...\n",
      "Unpacking g++-10 (10.2.1-6) ...\n",
      "Selecting previously unselected package g++.\n",
      "Preparing to unpack .../03-g++_4%3a10.2.1-1_amd64.deb ...\n",
      "Unpacking g++ (4:10.2.1-1) ...\n",
      "Selecting previously unselected package make.\n",
      "Preparing to unpack .../04-make_4.3-4.1_amd64.deb ...\n",
      "Unpacking make (4.3-4.1) ...\n",
      "Selecting previously unselected package libdpkg-perl.\n",
      "Preparing to unpack .../05-libdpkg-perl_1.20.12_all.deb ...\n",
      "Unpacking libdpkg-perl (1.20.12) ...\n",
      "Selecting previously unselected package patch.\n",
      "Preparing to unpack .../06-patch_2.7.6-7_amd64.deb ...\n",
      "Unpacking patch (2.7.6-7) ...\n",
      "Selecting previously unselected package dpkg-dev.\n",
      "Preparing to unpack .../07-dpkg-dev_1.20.12_all.deb ...\n",
      "Unpacking dpkg-dev (1.20.12) ...\n",
      "Selecting previously unselected package build-essential.\n",
      "Preparing to unpack .../08-build-essential_12.9_amd64.deb ...\n",
      "Unpacking build-essential (12.9) ...\n",
      "Selecting previously unselected package libassuan0:amd64.\n",
      "Preparing to unpack .../09-libassuan0_2.5.3-7.1_amd64.deb ...\n",
      "Unpacking libassuan0:amd64 (2.5.3-7.1) ...\n",
      "Selecting previously unselected package gpgconf.\n",
      "Preparing to unpack .../10-gpgconf_2.2.27-2+deb11u2_amd64.deb ...\n",
      "Unpacking gpgconf (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package libksba8:amd64.\n",
      "Preparing to unpack .../11-libksba8_1.5.0-3+deb11u2_amd64.deb ...\n",
      "Unpacking libksba8:amd64 (1.5.0-3+deb11u2) ...\n",
      "Selecting previously unselected package libnpth0:amd64.\n",
      "Preparing to unpack .../12-libnpth0_1.6-3_amd64.deb ...\n",
      "Unpacking libnpth0:amd64 (1.6-3) ...\n",
      "Selecting previously unselected package dirmngr.\n",
      "Preparing to unpack .../13-dirmngr_2.2.27-2+deb11u2_amd64.deb ...\n",
      "Unpacking dirmngr (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package libfakeroot:amd64.\n",
      "Preparing to unpack .../14-libfakeroot_1.25.3-1.1_amd64.deb ...\n",
      "Unpacking libfakeroot:amd64 (1.25.3-1.1) ...\n",
      "Selecting previously unselected package fakeroot.\n",
      "Preparing to unpack .../15-fakeroot_1.25.3-1.1_amd64.deb ...\n",
      "Unpacking fakeroot (1.25.3-1.1) ...\n",
      "Selecting previously unselected package gnupg-l10n.\n",
      "Preparing to unpack .../16-gnupg-l10n_2.2.27-2+deb11u2_all.deb ...\n",
      "Unpacking gnupg-l10n (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package gnupg-utils.\n",
      "Preparing to unpack .../17-gnupg-utils_2.2.27-2+deb11u2_amd64.deb ...\n",
      "Unpacking gnupg-utils (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package gpg.\n",
      "Preparing to unpack .../18-gpg_2.2.27-2+deb11u2_amd64.deb ...\n",
      "Unpacking gpg (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package pinentry-curses.\n",
      "Preparing to unpack .../19-pinentry-curses_1.1.0-4_amd64.deb ...\n",
      "Unpacking pinentry-curses (1.1.0-4) ...\n",
      "Selecting previously unselected package gpg-agent.\n",
      "Preparing to unpack .../20-gpg-agent_2.2.27-2+deb11u2_amd64.deb ...\n",
      "Unpacking gpg-agent (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package gpg-wks-client.\n",
      "Preparing to unpack .../21-gpg-wks-client_2.2.27-2+deb11u2_amd64.deb ...\n",
      "Unpacking gpg-wks-client (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package gpg-wks-server.\n",
      "Preparing to unpack .../22-gpg-wks-server_2.2.27-2+deb11u2_amd64.deb ...\n",
      "Unpacking gpg-wks-server (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package gpgsm.\n",
      "Preparing to unpack .../23-gpgsm_2.2.27-2+deb11u2_amd64.deb ...\n",
      "Unpacking gpgsm (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package gnupg.\n",
      "Preparing to unpack .../24-gnupg_2.2.27-2+deb11u2_all.deb ...\n",
      "Unpacking gnupg (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package libalgorithm-diff-perl.\n",
      "Preparing to unpack .../25-libalgorithm-diff-perl_1.201-1_all.deb ...\n",
      "Unpacking libalgorithm-diff-perl (1.201-1) ...\n",
      "Selecting previously unselected package libalgorithm-diff-xs-perl.\n",
      "Preparing to unpack .../26-libalgorithm-diff-xs-perl_0.04-6+b1_amd64.deb ...\n",
      "Unpacking libalgorithm-diff-xs-perl (0.04-6+b1) ...\n",
      "Selecting previously unselected package libalgorithm-merge-perl.\n",
      "Preparing to unpack .../27-libalgorithm-merge-perl_0.08-3_all.deb ...\n",
      "Unpacking libalgorithm-merge-perl (0.08-3) ...\n",
      "Selecting previously unselected package libfile-fcntllock-perl.\n",
      "Preparing to unpack .../28-libfile-fcntllock-perl_0.22-3+b7_amd64.deb ...\n",
      "Unpacking libfile-fcntllock-perl (0.22-3+b7) ...\n",
      "Setting up libksba8:amd64 (1.5.0-3+deb11u2) ...\n",
      "Setting up libstdc++-10-dev:amd64 (10.2.1-6) ...\n",
      "Setting up g++-10 (10.2.1-6) ...\n",
      "Setting up libfile-fcntllock-perl (0.22-3+b7) ...\n",
      "Setting up libalgorithm-diff-perl (1.201-1) ...\n",
      "Setting up libnpth0:amd64 (1.6-3) ...\n",
      "Setting up libassuan0:amd64 (2.5.3-7.1) ...\n",
      "Setting up libfakeroot:amd64 (1.25.3-1.1) ...\n",
      "Setting up fakeroot (1.25.3-1.1) ...\n",
      "update-alternatives: using /usr/bin/fakeroot-sysv to provide /usr/bin/fakeroot (fakeroot) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/fakeroot.1.gz because associated file /usr/share/man/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/faked.1.gz because associated file /usr/share/man/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/es/man1/fakeroot.1.gz because associated file /usr/share/man/es/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/es/man1/faked.1.gz because associated file /usr/share/man/es/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/fr/man1/fakeroot.1.gz because associated file /usr/share/man/fr/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/fr/man1/faked.1.gz because associated file /usr/share/man/fr/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/sv/man1/fakeroot.1.gz because associated file /usr/share/man/sv/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/sv/man1/faked.1.gz because associated file /usr/share/man/sv/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "Setting up make (4.3-4.1) ...\n",
      "Setting up gnupg-l10n (2.2.27-2+deb11u2) ...\n",
      "Setting up xz-utils (5.2.5-2.1~deb11u1) ...\n",
      "update-alternatives: using /usr/bin/xz to provide /usr/bin/lzma (lzma) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzma.1.gz because associated file /usr/share/man/man1/xz.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/unlzma.1.gz because associated file /usr/share/man/man1/unxz.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzcat.1.gz because associated file /usr/share/man/man1/xzcat.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzmore.1.gz because associated file /usr/share/man/man1/xzmore.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzless.1.gz because associated file /usr/share/man/man1/xzless.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzdiff.1.gz because associated file /usr/share/man/man1/xzdiff.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzcmp.1.gz because associated file /usr/share/man/man1/xzcmp.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzgrep.1.gz because associated file /usr/share/man/man1/xzgrep.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzegrep.1.gz because associated file /usr/share/man/man1/xzegrep.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzfgrep.1.gz because associated file /usr/share/man/man1/xzfgrep.1.gz (of link group lzma) doesn't exist\n",
      "Setting up patch (2.7.6-7) ...\n",
      "Setting up libdpkg-perl (1.20.12) ...\n",
      "Setting up g++ (4:10.2.1-1) ...\n",
      "update-alternatives: using /usr/bin/g++ to provide /usr/bin/c++ (c++) in auto mode\n",
      "Setting up gpgconf (2.2.27-2+deb11u2) ...\n",
      "Setting up libalgorithm-diff-xs-perl (0.04-6+b1) ...\n",
      "Setting up liblocale-gettext-perl (1.07-4+b1) ...\n",
      "Setting up gpg (2.2.27-2+deb11u2) ...\n",
      "Setting up libalgorithm-merge-perl (0.08-3) ...\n",
      "Setting up gnupg-utils (2.2.27-2+deb11u2) ...\n",
      "Setting up pinentry-curses (1.1.0-4) ...\n",
      "Setting up gpg-agent (2.2.27-2+deb11u2) ...\n",
      "Setting up gpgsm (2.2.27-2+deb11u2) ...\n",
      "Setting up dpkg-dev (1.20.12) ...\n",
      "Setting up dirmngr (2.2.27-2+deb11u2) ...\n",
      "Setting up gpg-wks-server (2.2.27-2+deb11u2) ...\n",
      "Setting up build-essential (12.9) ...\n",
      "Setting up gpg-wks-client (2.2.27-2+deb11u2) ...\n",
      "Setting up gnupg (2.2.27-2+deb11u2) ...\n",
      "Processing triggers for libc-bin (2.31-13+deb11u3) ...\n"
     ]
    }
   ],
   "source": [
    "!apt-get update && apt-get install -y build-essential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c71c71e-983c-4f1e-aba9-28660f0a1ecd",
   "metadata": {},
   "source": [
    "### Install Annoy, a vector database\n",
    "\n",
    "alternatives chromadb, faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "64fcbd11-60c0-48db-9fb4-e68c30f6da09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting annoy\n",
      "  Using cached annoy-1.17.3.tar.gz (647 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: annoy\n",
      "  Building wheel for annoy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for annoy: filename=annoy-1.17.3-cp310-cp310-linux_x86_64.whl size=76573 sha256=31850389a337fd1502405ccab083434fe8d454dab49a6e3ececaa5cbd95ec8ea\n",
      "  Stored in directory: /root/.cache/pip/wheels/64/8a/da/f714bcf46c5efdcfcac0559e63370c21abe961c48e3992465a\n",
      "Successfully built annoy\n",
      "Installing collected packages: annoy\n",
      "Successfully installed annoy-1.17.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install annoy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d11b441-451c-4b0c-8982-1a24e1c1480d",
   "metadata": {},
   "source": [
    "#### Install tiktoken, a tokenizer\n",
    "\n",
    "requested for embeddjngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "46a4bb8c-c03a-49af-90e8-1d262f518633",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2022.7.9)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2023.5.7)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be5012c-2708-417f-a6e8-4a1beb5349df",
   "metadata": {},
   "source": [
    "### Example selectors and Few Prompt Learning with similarities\n",
    "\n",
    "requires a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "be6e5756-6345-4b70-844b-f7959fe0d70e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== exemple prompt ===\n",
      "question: red bold\n",
      "color:red; font-style:bold;\n",
      "\n",
      "=== prompt ===\n",
      "question: red bold\n",
      "color:red; font-style:bold;\n",
      "\n",
      "question: pink italic\n",
      "color:pink; font-style:italic;\n",
      "\n",
      "question: pink bold\n",
      "\n",
      "=== answer ===\n",
      "\n",
      "color:pink; font-style:bold;\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Annoy\n",
    "#from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# create the example set\n",
    "\n",
    "examples = [\n",
    "    { \"question\": \"red bold\", \"answer\": \"color:red; font-style:bold;\"},\n",
    "    { \"question\": \"green italic\", \"answer\":  \"color:green; font-style:italic;\"},\n",
    "    { \"question\": \"blue bold\", \"answer\":  \"color:blue; font-style:bold;\"},\n",
    "    { \"question\": \"pink\", \"answer\":  \"color:pink;\"},\n",
    "    { \"question\": \"green\", \"answer\":  \"color:green;\"},\n",
    "    { \"question\": \"pink italic\", \"answer\":  \"color:pink; font-style:italic;\"}\n",
    "    \n",
    "]    \n",
    "\n",
    "# Configure a formatter that will format the few shot examples into a string. \n",
    "# This formatter should be a PromptTemplate object.\n",
    "\n",
    "example_prompt = PromptTemplate (\n",
    "    input_variables=[\"question\", \"answer\"], \n",
    "    template=\"question: {question}\\n{answer}\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== exemple prompt ===\")\n",
    "print(example_prompt.format(**examples[0]))\n",
    "\n",
    "# Example selector that selects examples based on SemanticSimilarity.\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # This is the list of examples available to select from.\n",
    "    examples,\n",
    "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    OpenAIEmbeddings(),\n",
    "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    #Chroma,\n",
    "    Annoy,\n",
    "    # This is the number of examples to produce.\n",
    "    k=2\n",
    ")\n",
    "\n",
    "# Finally, create a FewShotPromptTemplate object. \n",
    "# This object takes in the few shot examples and the formatter for the few shot examples.\n",
    "\n",
    "prompt_template = FewShotPromptTemplate(\n",
    "    example_selector=example_selector, \n",
    "    example_prompt=example_prompt, \n",
    "    suffix=\"question: {input}\", \n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format(input=\"pink bold\")\n",
    "\n",
    "print(\"\\n=== prompt ===\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"\\n=== answer ===\")\n",
    "print(llm(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2b5aae-d6c0-4ebe-a12e-521b2f807045",
   "metadata": {},
   "source": [
    "## Output Parser and response format\n",
    "\n",
    "A way to format the outpu\n",
    "- Format nstructions: An autogenerated prompt telling how the result should be formatted\n",
    "- parser: a method which will extract the output int hte desired format. you may prvie a custom parser\n",
    "\n",
    "\n",
    "Documentation: https://docs.langchain.com/docs/components/prompts/output-parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46e0cc93-9499-4ac7-a3b2-669bb3f121e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "format_instructions\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This is a poorly formatted string.\n",
      "\t\"good_string\": string  // This is a your string reformatted.\n",
      "}\n",
      "```\n",
      "\n",
      "prompt\n",
      "\n",
      "You will be given a poorly formatted string from a user. \n",
      "Reformat it and make sure all the words are spelled correctly.\n",
      "\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This is a poorly formatted string.\n",
      "\t\"good_string\": string  // This is a your string reformatted.\n",
      "}\n",
      "```\n",
      "\n",
      "% USER_INPUT:\n",
      "Wellcom to Californya!\n",
      "\n",
      "YOUR RESPONSE:\n",
      "\n",
      "\n",
      "response=\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": \"Wellcom to Californya!\",\n",
      "\t\"good_string\": \"Welcome to California!\"\n",
      "}\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bad_string': 'Wellcom to Californya!',\n",
       " 'good_string': 'Welcome to California!'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0.9)\n",
    "\n",
    "# how you would like the response to be structured\n",
    "# periods at the send of sentence are required. \n",
    "# If not there description ends up in the json text and break the JSON format\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"bad_string\", description=\"This is a poorly formatted string.\"),\n",
    "    ResponseSchema(name=\"good_string\", description=\"This is a your string reformatted.\")\n",
    "]\n",
    "\n",
    "# How you would like to parse your output\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# check instructions\n",
    "format_instructions =output_parser.get_format_instructions()\n",
    "print(\"\\nformat_instructions\")      \n",
    "print(format_instructions)      \n",
    "\n",
    "template = \"\"\"\n",
    "You will be given a poorly formatted string from a user. \n",
    "Reformat it and make sure all the words are spelled correctly.\n",
    "\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER_INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=['user_input'],\n",
    "    partial_variables={'format_instructions': format_instructions},\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# format the user input as a prompt\n",
    "# for whateveer reason it does not work well with format.\n",
    "# format_promt retruns an object, not a string and should be converted to a string \n",
    "prompt = prompt_template.format_prompt(user_input=\"Wellcom to Californya!\").to_string()\n",
    "print(\"\\nprompt\")\n",
    "print(prompt)\n",
    "\n",
    "# gets the response\n",
    "response = llm(prompt)\n",
    "print(\"\\nresponse=\")      \n",
    "print(response)      \n",
    "\n",
    "# gets the JSON document\n",
    "print(\"\\parsed output=\")      \n",
    "output_parser.parse(response)                   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc922de-52de-4c30-8faa-4feded2137d2",
   "metadata": {},
   "source": [
    "---\n",
    "# 9. X\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babb2462-d3a2-4d88-b15a-2c811781175b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-3:615547856133:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
