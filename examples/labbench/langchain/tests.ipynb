{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00afbf75-6012-4c57-9dad-4080e36c5687",
   "metadata": {},
   "source": [
    "# Langchain test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1be20e-ba74-42e4-a144-3d05841998bd",
   "metadata": {},
   "source": [
    "---\n",
    "## Documentation\n",
    "\n",
    "> LangChain resources\n",
    "> - Landpage: https://readthedocs.org/projects/langchain/db2d\n",
    "> - git: https://github.com/hwchase17/langchain.git\n",
    "> - API Reference: https://api.python.langchain.com/en/latest/\n",
    "\n",
    "> Tutos\n",
    "> - https://towardsdatascience.com/a-gentle-intro-to-chaining-llms-agents-and-utils-via-langchain-16cd385fca81\n",
    "> - videos Greg Kamradt on YouTube\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006f2f80-db93-4c99-812b-1873e99e7850",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51de5b76-5e76-4c8f-bd11-9445f98c6941",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8713a09-0a7a-416c-807a-e2560796f7d5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Using cached langchain-0.0.215-py3-none-any.whl (1.1 MB)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /opt/conda/lib/python3.10/site-packages (from langchain) (5.4.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.4.39)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Using cached aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
      "  Using cached dataclasses_json-0.5.8-py3-none-any.whl (26 kB)\n",
      "Collecting langchainplus-sdk>=0.0.17 (from langchain)\n",
      "  Using cached langchainplus_sdk-0.0.17-py3-none-any.whl (25 kB)\n",
      "Collecting numexpr<3.0.0,>=2.8.4 (from langchain)\n",
      "  Using cached numexpr-2.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (381 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.24.3)\n",
      "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
      "  Using cached openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
      "Collecting pydantic<2,>=1 (from langchain)\n",
      "  Using cached pydantic-1.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain)\n",
      "  Using cached tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
      "  Using cached marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
      "Collecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
      "  Using cached marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<2,>=1->langchain) (4.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (1.1.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (21.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (0.4.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (3.0.9)\n",
      "Installing collected packages: typing-inspect, tenacity, pydantic, numexpr, multidict, frozenlist, async-timeout, yarl, openapi-schema-pydantic, marshmallow, langchainplus-sdk, aiosignal, marshmallow-enum, aiohttp, dataclasses-json, langchain\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.0.1\n",
      "    Uninstalling tenacity-8.0.1:\n",
      "      Successfully uninstalled tenacity-8.0.1\n",
      "  Attempting uninstall: numexpr\n",
      "    Found existing installation: numexpr 2.8.3\n",
      "    Uninstalling numexpr-2.8.3:\n",
      "      Successfully uninstalled numexpr-2.8.3\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 dataclasses-json-0.5.8 frozenlist-1.3.3 langchain-0.0.215 langchainplus-sdk-0.0.17 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 numexpr-2.8.4 openapi-schema-pydantic-1.2.4 pydantic-1.10.9 tenacity-8.2.2 typing-inspect-0.9.0 yarl-1.9.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bc38381-5ee0-4107-9b3a-40dd9cb9302e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Using cached openai-0.27.8-py3-none-any.whl (73 kB)\n",
      "Requirement already satisfied: requests>=2.20 in /opt/conda/lib/python3.10/site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (2023.5.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Installing collected packages: openai\n",
      "Successfully installed openai-0.27.8\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4397c800-3ee5-40aa-b0a7-5021985e334a",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## API keys and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6671d484-9c5d-4818-8449-6df0189fd307",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash --out secrets \n",
    "# using AWS's Secret Manager to store keys\n",
    "# garb the keys and store it into a Pytthon variable\n",
    "export RESPONSE=$(aws secretsmanager get-secret-value --secret-id 'labbenach/sednara/api_keys' )\n",
    "export SECRETS=$( echo $RESPONSE | jq '.SecretString | fromjson')\n",
    "\n",
    "echo $SECRETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be008522-bd17-47d4-bd6d-58da90214050",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = eval(secrets)[\"OPENAI_API_KEY\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6bd9f8-efef-4797-ba5d-9eb135e6b862",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Basic features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da068c2f-26b2-4c5e-b72c-23a1d50c8543",
   "metadata": {},
   "source": [
    "---\n",
    "## Get prediction from a langage model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "68e8dd21-8ada-4cd4-a532-43c2e1c3fc63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Germany \n",
      "2. Switzerland \n",
      "3. Denmark \n",
      "4. Norway \n",
      "5. Luxembourg\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "# OPENAI_API_KEY is requested. Get it from the OpenAI site.\n",
    "# a paid account and available units are requested to be able to place a request.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "text = \"what are the 5 best countries in Europe\"\n",
    "\n",
    "# Actual API call - may tale a while.\n",
    "print(llm(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17250fa0-fe5d-47c1-9406-9822dd38fc9e",
   "metadata": {},
   "source": [
    "---\n",
    "## Manage prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2bdf0d54-abaa-4aa6-aeb5-275bebb5246b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# setup a prompt\n",
    "prompt = PromptTemplate (\n",
    "    input_variables=[\"interest\"],\n",
    "    template=\"what are the 5 best countries in Europe ranked on {interest}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f09393d7-6674-42f3-97c0-08cd12439d3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='what are the 5 best countries in Europe ranked on food'\n",
      "\n",
      "\n",
      "1. Italy \n",
      "2. France \n",
      "3. Spain \n",
      "4. Greece \n",
      "5. Portugal\n"
     ]
    }
   ],
   "source": [
    "text = prompt.format(interest=\"food\")\n",
    "print(f\"{text=}\")\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "87e3e1e6-871b-42e3-9106-96249b3f726e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='what are the 5 best countries in Europe ranked on siteseeing'\n",
      "\n",
      "\n",
      "1. Italy\n",
      "2. France\n",
      "3. Spain\n",
      "4. Greece\n",
      "5. United Kingdom\n"
     ]
    }
   ],
   "source": [
    "text = prompt.format(interest=\"siteseeing\")\n",
    "print(f\"{text=}\")\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c344d1-9478-46f0-a5df-20ff5644e011",
   "metadata": {},
   "source": [
    "---\n",
    "## Prompt with multiple tokens \n",
    "<div class=\"alert alert-block alert-warning\"> TODO </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db89d7fa-10d1-495d-8e89-24ab1dce738f",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ae61a5-823a-4585-b308-8e83ea8b6a7a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO  what is a chain </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d607ac-9d7f-40f3-86f4-243670f42fe2",
   "metadata": {},
   "source": [
    "---\n",
    "## Built-in chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0d68f8d0-4ee4-48c5-a6b6-5f515c581eea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mdef solution():\n",
      "    \"\"\"If my age is half of my dad's age and he is going to be 60 next year, what is my current age?\"\"\"\n",
      "    dad_age_next_year = 60\n",
      "    my_age = dad_age_next_year / 2\n",
      "    result = my_age\n",
      "    return result\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'30.0'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import PALChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "palchain = PALChain.from_math_prompt(llm=llm, verbose=True)\n",
    "\n",
    "\n",
    "text = \"\"\"If my age is half of my dad's age \n",
    "and he is going to be 60 next year, \n",
    "what is my current age?\"\"\"\n",
    "#palchain.run(\"If my age is half of my dad's age and he is going to be 60 next year, what is my current age?\")\n",
    "palchain.run(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ed407-348c-4c00-a405-c8233fe07bb4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "    TODO <br>\n",
    "    - different result each run <br>\n",
    "    - and should be 29.5\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8337725d-b79a-41b9-99f8-d7d55339c149",
   "metadata": {},
   "source": [
    "> Entering new  chain...\n",
    "def solution():\n",
    "    \"\"\"If my age is half of my dad's age and he is going to be 60 next year, what is my current age?\"\"\"\n",
    "    dad_age_next_year = 60\n",
    "    my_age_fraction = 0.5\n",
    "    my_age_now = dad_age_next_year * my_age_fraction\n",
    "    result = my_age_now\n",
    "    return result\n",
    "\n",
    "> Finished chain.\n",
    "'30.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8acdd9d-e9d5-4ca1-b3db-6e8b2a47d81b",
   "metadata": {},
   "source": [
    "> Entering new  chain...\n",
    "def solution():\n",
    "    \"\"\"If my age is half of my dad's age and he is going to be 60 next year, what is my current age?\"\"\"\n",
    "    dad_age_current = 59\n",
    "    my_age_current = dad_age_current / 2\n",
    "    result = my_age_current\n",
    "    return result\n",
    "\n",
    "> Finished chain.\n",
    "'29.5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b09463-2f2e-4840-9602-1a22478bdd5c",
   "metadata": {},
   "source": [
    "---\n",
    "## Multi-step workflow to feed prompt into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3fe1a4f0-4c08-47aa-9304-0b6a7002968b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# setup a prompt\n",
    "prompt = PromptTemplate (\n",
    "    input_variables=[\"interest\"],\n",
    "    template=\"what are the 5 best countries in Europe ranked on {interest}\"\n",
    ")\n",
    "\n",
    "# chain feeds the prompt into the langage mmodel.\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "97622a8b-eb22-4218-803d-34d1a76c1af2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n1. Germany\\n2. Sweden\\n3. Switzerland\\n4. United Kingdom\\n5. Netherlands'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7f73c33b-634d-48fb-9552-7329658c4e23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. United Kingdom\n",
      "2. France\n",
      "3. Germany\n",
      "4. Italy\n",
      "5. Spain\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"tv shows\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c72f8b-7623-4e34-a2b2-c8bf785fa719",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Using OpenAI Chat API (less expensive)\n",
    "requires a chain to feed the prompt into the chat \n",
    "\n",
    "Other Chat APIs\n",
    "- https://api.python.langchain.com/en/latest/modules/chat_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f18f9ea2-c494-462a-aa3e-5e7ca5952d31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI language model, I do not have personal preferences. However, the following are five countries in Europe that are known for their delicious cuisine:\n",
      "\n",
      "1. Italy - Italian cuisine is famous for its pasta, pizza, gelato, and wines. Italy is also known for its flavorful seafood and meat dishes.\n",
      "\n",
      "2. France - French cuisine is renowned for its delicate flavors and rich sauces. It includes dishes such as coq au vin, ratatouille, and escargots.\n",
      "\n",
      "3. Spain - Spanish cuisine is known for its tapas, paella, and seafood dishes. It also features delicious cured meats and cheeses.\n",
      "\n",
      "4. Greece - Greek cuisine is characterized by fresh vegetables, grilled meats, and flavorful dips such as tzatziki and hummus. Greek cuisine also includes dishes like moussaka and souvlaki.\n",
      "\n",
      "5. Turkey - Turkish cuisine is a fusion of Middle Eastern and Mediterranean flavors. It includes dishes such as kebabs, baklava, and Turkish delight. Turkish cuisine also features delicacies like stuffed grape leaves and Turkish coffee.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chatopenai = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "prompt = PromptTemplate (\n",
    "    input_variables=[\"interest\"],\n",
    "    template=\"what are the 5 best countries in Europe ranked on {interest}\"\n",
    ")\n",
    "\n",
    "llmchain_chat = LLMChain(llm=chatopenai, prompt=prompt)\n",
    "print(llmchain_chat.run(\"food\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3204392e-b1de-47f0-983e-ae05901179ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Leverage LLM Math\n",
    "\n",
    "Evaluating chains that know how to do math.\n",
    "\n",
    "https://python.langchain.com/docs/guides/evaluation/llm_math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "26ed6528-f540-4b2f-9617-e5ae8ffe6d26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `_type` key found, defaulting to `prompt`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 19\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "from langchain.chains import LLMMathChain\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "prompt = load_prompt('lc://prompts/llm_math/prompt.json')\n",
    "\n",
    "# deprecated\n",
    "##chain = LLMMathChain(llm=llm, prompt=prompt)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "print(chain.run(\"what is the largest prime number lower than 20\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899d9cb9-c483-4c79-ae73-cafbacb19b7d",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522dcef8-4941-4d62-94f0-6b8dd132589e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO  what is a tool </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ea5a95-b5c1-4221-afb1-b83f86ded7a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Leverage Goocle Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31520149-8e0f-4b08-b4a5-08c118fb638f",
   "metadata": {
    "tags": []
   },
   "source": [
    ">How to configure the Google search in Langchain \n",
    "> - https://python.langchain.com/docs/ecosystem/integrations/google_search\n",
    "\n",
    "> Custom Search Engine configuration \n",
    "> - https://stackoverflow.com/questions/37083058/programmatically-searching-google-in-python-using-custom-search\n",
    "\n",
    "> CSE API \n",
    "> - repo: https://github.com/google/google-api-python-client\n",
    "> - more info: https://developers.google.com/api-client-library/python/apis/customsearch/v1\n",
    "> - complete docs: https://api-python-client-doc.appspot.com/\n",
    "\n",
    "> Get an API key\n",
    "> - https://developers.google.com/custom-search/v1/introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2e169ba1-9d2d-4bfa-843c-afe61e1228e2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-api-python-client\n",
      "  Using cached google_api_python_client-2.90.0-py2.py3-none-any.whl (11.4 MB)\n",
      "Collecting httplib2<1.dev0,>=0.15.0 (from google-api-python-client)\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Collecting google-auth<3.0.0.dev0,>=1.19.0 (from google-api-python-client)\n",
      "  Using cached google_auth-2.20.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting google-auth-httplib2>=0.1.0 (from google-api-python-client)\n",
      "  Using cached google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 (from google-api-python-client)\n",
      "  Using cached google_api_core-2.11.1-py3-none-any.whl (120 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client)\n",
      "  Using cached uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Using cached googleapis_common_protos-1.59.1-py2.py3-none-any.whl (224 kB)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.20.3)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.31.0)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client)\n",
      "  Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (4.7.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (1.16.0)\n",
      "Collecting urllib3<2.0 (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client)\n",
      "  Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.10/site-packages (from httplib2<1.dev0,>=0.15.0->google-api-python-client) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2023.5.7)\n",
      "Installing collected packages: urllib3, uritemplate, httplib2, googleapis-common-protos, cachetools, google-auth, google-auth-httplib2, google-api-core, google-api-python-client\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.3\n",
      "    Uninstalling urllib3-2.0.3:\n",
      "      Successfully uninstalled urllib3-2.0.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed cachetools-5.3.1 google-api-core-2.11.1 google-api-python-client-2.90.0 google-auth-2.20.0 google-auth-httplib2-0.1.0 googleapis-common-protos-1.59.1 httplib2-0.22.0 uritemplate-4.1.1 urllib3-1.26.16\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "09c65606-95d2-4b96-813f-db44b9ffcbb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Unlock the API and get a key \n",
    "os.environ[\"GOOGLE_API_KEY\"] = eval(secrets)[\"GOOGLE_API_KEY\"]\n",
    "# Create or use an existing Custom Search Engine\n",
    "# on the CSE page under Searcg Engone ID\n",
    "os.environ[\"GOOGLE_CSE_ID\"] = eval(secrets)[\"GOOGLE_CSE_ID\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f98c7adc-982e-4693-bb7c-ee7a3e1d2e1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Élisabeth Borne has served as Prime Minister since 16 May 2022. Fifth Republic recordsEdit. Length of the successive governments\\xa0... May 16, 2022 ... President Emmanuel Macron has named Labour Minister Elisabeth Borne as prime minister to lead his ambitious reform plans, the first woman to\\xa0... Feb 22, 2018 ... SEVEN months after their prime minister was appointed in May 2017, fully 35% of the French could not name him accurately in a poll. May 16, 2022 ... Elisabeth Borne has been named the new Prime Minister of France, the first time in 30 years that a woman has held the position. May 16, 2022 ... Élisabeth Borne, the minister of labor who previously was in charge of the environment, will be the second woman to hold the post in France. Jun 24, 2022 ... The name of Lafayette is famous and respected on both sides of the Atlantic. It is our third conversation in a month, which is quite a good\\xa0... May 5, 2017 ... France's Macron says he has chosen prime minister, won't reveal name ... PARIS (Reuters) - French presidential election front-runner Emmanuel\\xa0... May 16, 2022 ... 6:30pm: Macron names minister Elisabeth Borne new French PM. President Emmanuel Macron on Monday named Labour Minister Elisabeth Borne as his\\xa0... Apr 25, 2022 ... Castex could have been asked to stay on as prime minister, but told French radio earlier this month that France would be looking for a “new\\xa0... May 16, 2022 ... PARIS (AP) — Centrist politician Elisabeth Borne was appointed France's new prime minister on Monday, becoming only the second woman in\\xa0...\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "search = GoogleSearchAPIWrapper()\n",
    "\n",
    "tool = Tool(\n",
    "    name=\"Google Search\",\n",
    "    description=\"Search Google for recent results.\",\n",
    "    func=search.run,\n",
    ")\n",
    "\n",
    "tool.run(\"French Prime Minister name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb52e51-3733-4ff5-a8cb-1094a391b98f",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543f13ea-82c7-4522-91b9-453183bb7310",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO  what is an agent </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c68946-559e-497d-a971-45c473ea5770",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c64311c2-8e52-4278-a4ac-42a22bceec08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# create a model\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# load some tools\n",
    "tools = load_tools([\"google-search\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# setup an agent\n",
    "agent = initialize_agent(tools, \n",
    "                         llm, \n",
    "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "                         verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ff155cf5-99f9-4ba4-8dbc-9d9366825475",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out how many Teslas have been sold in 2022\n",
      "Action: google_search\n",
      "Action Input: \"how many Teslas have been sold in 2022\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApr 15, 2023 ... Tesla total revenue for 2022 was 81,462 billion USD. We show it from 2018 – 2022. Tesla annual revenue 2018 - 2022. Year, Annual ... Jun 7, 2023 ... How many Tesla vehicles were delivered in 2023? ... As of June 2022, Tesla was the most valuable brand within the global automotive sector. Jan 25, 2023 ... The Model 3 and Model Y make up around 95% of the 1.31 million Teslas sold in 2022. Tesla. Tesla's finished 2022 on a tear, bolstered by recent ... Jan 7, 2023 ... Overall, Tesla reported delivering about 1.25 million Model Y and Model 3 vehicles globally in 2022. The Model 3 ranked 13th in sales at 211,641 ... Jan 3, 2023 ... The electric automaker delivered 1.3 million vehicles in 2022, up 40% from 2021. It produced nearly 1.4 million vehicles, up 47% from the prior ... May 30, 2022 ... If every vehicle sold by Tesla since its inception up to the most recent Tesla sales figures released for Q1 of 2022 are accounted for, there ... Jul 23, 2022 ... In 2022, Tesla sold 1,2 million Model 3s and Ys combined. The last sales record was around 900K in 2021, which translates to a more than 30% ... It left behind BMW, which had worn this crown for the last three years but sold only 332,388 cars in the United States in 2022 and was thus relegated to second ... Nov 4, 2022 ... As of October 2022, Tesla, the Texas-based EV automaker, has sold more than 3 million units on a global scale. The company has found that ... Jan 2, 2023 ... AUSTIN, Texas, January 2, 2023 – In the fourth quarter, we produced over 439000 vehicles and delivered over 405000 vehicles. In 2022 ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the number of Teslas sold in 2022\n",
      "Action: Calculator\n",
      "Action Input: 1,310,000 * 2\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2620000\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 2,620,000 Teslas were sold in 2022.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2,620,000 Teslas were sold in 2022.'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"How many Teslas have been sold in 2022. Multiple by 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13982ee0-5296-4ee4-ad84-d7e8f795bf96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "20688891-5c6e-4279-bfa4-8914980abb90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out who the current prime minister is and then compare their age to the President.\n",
      "Action: google_search\n",
      "Action Input: \"current prime minister of France\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPresentEdit. Élisabeth Borne has served as Prime Minister since 16 May 2022. Fifth Republic records ... May 16, 2022 ... Who is France's new Prime Minister Elisabeth Borne? French President Emmanuel Macron picked Labour Minister Elisabeth Borne as his new prime ... The current Prime Minister of France is Élisabeth Borne. She was given the job by President Emmanuel Macron on 16 May 2022. May 16, 2022 ... President Emmanuel Macron has named Labour Minister Elisabeth Borne as prime minister to lead his ambitious reform plans, the first woman to ... May 16, 2022 ... Élisabeth Borne, the minister of labor who previously was in charge of the environment, will be the second woman to hold the post in France. May 2, 2014 ... On the recommendation of the Prime Minister, President Hollande has appointed the following ministers: ; Mr Benoit HAMON, Minister of Education ... Feb 8, 2023 ... France's prime minister, Élisabeth Borne, sat on a recent, rainy evening in a dim room at a Red Cross shelter, listening to young women ... Jun 12, 2023 ... Élisabeth Borne, Prime Minister of France. France Élisabeth Borne · Olaf Scholz, Chancellor of the Federal Republic of Germany. Jan 10, 2023 ... France's Prime Minister Elisabeth Borne attends a press conference to present the government's plan for a. By —. Sylvie Corbet, Associated Press ... Mar 31, 2023 ... ... keep Élisabeth Borne as prime minister. Macron is adamant that he will, though Borne needs to find a way out of the current impasse.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to find out the age of the President and compare it to the Prime Minister.\n",
      "Action: google_search\n",
      "Action Input: \"age of French President\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAt the age of 39, Macron became the youngest president in French history. In the 2017 legislative election in June, Macron's party, renamed La République En ... Mar 16, 2023 ... PARIS (AP) — French President Emmanuel Macron ordered his prime minister to wield a special constitutional power Thursday that skirts ... Apr 15, 2023 ... French President Emmanuel Macron has signed into law his government's highly unpopular pension reforms, which raise the state pension age from ... Apr 14, 2023 ... PARIS — French President Emmanuel Macron's unpopular plan to raise France's retirement age from 62 to 64 was enacted into law Saturday, ... May 3, 2017 ... Who is Emmanuel Macron? ... PARIS — Emmanuel Macron, the front-runner in Sunday's French presidential election, shares something with President ... Aug 19, 2017 ... &#151; -- French first lady Brigitte Macron is speaking out about the ... her and husband Emmanuel Macron, France's 39-year-old president. Mar 16, 2023 ... French President Emmanuel Macron on Thursday resorted to using special constitutional powers to push his plan to raise the retirement age to 64 ... May 2, 2017 ... If Mr Macron is successful, the 39-year-old would become the youngest president in France's history since the election in 1848 of Napoléon ... Mar 16, 2023 ... French President Emmanuel Macron raised the retirement age in France on Thursday, from 62 to 64, without waiting for a legislative vote. In the second round, held on May 7, 2017, Macron won a convincing two-thirds of the vote, becoming, at age 39, France's youngest president.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The current Prime Minister of France, Élisabeth Borne, is younger than the President, Emmanuel Macron, who is 39 years old.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The current Prime Minister of France, Élisabeth Borne, is younger than the President, Emmanuel Macron, who is 39 years old.'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"\"\"Who is the current prime minister of France. \n",
    "Is he or sheyounger than the President?\"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c456a279",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out who the current prime minister is and when they will be 70.\n",
      "Action: google_search\n",
      "Action Input: \"current prime minister of France\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPresentEdit. Élisabeth Borne has served as Prime Minister since 16 May 2022. Fifth Republic records ... May 16, 2022 ... Who is France's new Prime Minister Elisabeth Borne? French President Emmanuel Macron picked Labour Minister Elisabeth Borne as his new prime ... The current Prime Minister of France is Élisabeth Borne. She was given the job by President Emmanuel Macron on 16 May 2022. May 16, 2022 ... President Emmanuel Macron has named Labour Minister Elisabeth Borne as prime minister to lead his ambitious reform plans, the first woman to ... May 16, 2022 ... Élisabeth Borne, the minister of labor who previously was in charge of the environment, will be the second woman to hold the post in France. May 2, 2014 ... On the recommendation of the Prime Minister, President Hollande has appointed the following ministers: ; Mr Benoit HAMON, Minister of Education ... Feb 8, 2023 ... France's prime minister, Élisabeth Borne, sat on a recent, rainy evening in a dim room at a Red Cross shelter, listening to young women ... Jun 12, 2023 ... Élisabeth Borne, Prime Minister of France. France Élisabeth Borne · Olaf Scholz, Chancellor of the Federal Republic of Germany. Jan 10, 2023 ... France's Prime Minister Elisabeth Borne attends a press conference to present the government's plan for a. By —. Sylvie Corbet, Associated Press ... Mar 31, 2023 ... ... keep Élisabeth Borne as prime minister. Macron is adamant that he will, though Borne needs to find a way out of the current impasse.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to calculate when Élisabeth Borne will be 70.\n",
      "Action: Calculator\n",
      "Action Input: 16 May 2022 + 70 years\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2092\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: Élisabeth Borne will be 70 in the year 2092.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Élisabeth Borne will be 70 in the year 2092.'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"\"\"Who is the current prime minister of France. \n",
    "When will he or she be 70?\"\"\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28631b0b-d1ec-4d14-bf44-091f8abd1584",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Memory - Conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bde2f8-060f-4da3-a69f-24df8df6aa63",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO  what is a conversation </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "13541ec0-684d-48da-a205-ef1c6e3e7937",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi There\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi there! It's nice to meet you. How can I help you today?\""
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import OpenAI, ConversationChain\n",
    "\n",
    "# create a model\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "conversation.predict(input=\"Hi There\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7271c38f-0a3e-44fb-9d8a-1014667c3467",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi There\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: What is the first thing that I said to you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' You said \"Hi there!\"'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is the first thing that I said to you?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e2613f8b-beb3-47b3-b842-e5f2b4746143",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi There\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: What is the first thing that I said to you?\n",
      "AI:  You said \"Hi there!\"\n",
      "Human: What is an alternative for the first thing that I said to you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' An alternative for the first thing you said to me is \"Hello!\"'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is an alternative for the first thing that I said to you?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684914b4-7b16-4f0b-bde2-6ef6c7df5079",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Schemas\n",
    "\n",
    "There are 3 types of schemas\n",
    "- text (see above)\n",
    "- Messages \n",
    "- Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7bb2c3-a6cd-4fee-b6a8-964a5f811433",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf610897-44b3-4d72-8e38-3dd50b0776bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Chat messages\n",
    "Chat messages are like text with a type\n",
    "\n",
    "There are 3 types\n",
    "- System: background context that tells the AI what to do\n",
    "- Human: inputs sent by the user\n",
    "- AI : response of the AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c8d5a96-f105-45f2-ba15-0f0b108cd9ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f7a0269-e45c-445b-b44f-b19fc4be5e53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [ SystemMessage(content=\"You are a nice AI and help users to feature out what to eat.\")]\n",
    "     \n",
    "messages.append( HumanMessage(content=\"I like tuna, list some recipes.\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e203228-09cd-4405-9679-c4eebfbd7c1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here are some tuna recipes that you might enjoy:\n",
      "\n",
      "1. Tuna salad: Mix canned tuna with chopped celery, onions, and mayonnaise. You can also add some chopped pickles, mustard, and salt and pepper to taste. Serve on a bed of lettuce or between two slices of bread.\n",
      "\n",
      "2. Tuna melt: Spread canned tuna on a slice of bread, top with sliced tomato and cheese, and broil until the cheese is melted and bubbly.\n",
      "\n",
      "3. Tuna pasta salad: Combine cooked pasta with canned tuna, chopped vegetables like bell peppers and onions, and a dressing made of mayonnaise, lemon juice, and herbs.\n",
      "\n",
      "4. Tuna patties: Mix canned tuna with bread crumbs, egg, and seasonings like garlic, onion powder, and parsley. Form into patties and pan-fry until golden brown.\n",
      "\n",
      "5. Tuna poke bowl: Top cooked rice with cubed raw tuna, avocado, cucumber, and edamame. Drizzle with a soy sauce-based dressing and garnish with sesame seeds.\n",
      "\n",
      "I hope these ideas help! Let me know if you have any specific dietary restrictions or preferences, and I can suggest more personalized recipes.\n"
     ]
    }
   ],
   "source": [
    "response = chat(messages)\n",
    "messages.append( AIMessage(content=response.content) )\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d140f70f-a86d-4634-899e-12401f0315cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here is a recipe for tuna salad:\n",
      "\n",
      "Ingredients:\n",
      "- 2 cans of tuna, drained\n",
      "- 2 stalks of celery, chopped\n",
      "- 1 small onion, chopped\n",
      "- 1/4 cup mayonnaise\n",
      "- 1 tablespoon chopped pickles (optional)\n",
      "- 1 teaspoon mustard (optional)\n",
      "- Salt and pepper to taste\n",
      "\n",
      "Instructions:\n",
      "1. In a mixing bowl, combine the drained tuna, chopped celery, and chopped onion.\n",
      "2. Add the mayonnaise, pickles, and mustard (if using) to the bowl and mix well until everything is combined.\n",
      "3. Season with salt and pepper to taste.\n",
      "4. Serve the tuna salad on a bed of lettuce or between two slices of bread.\n",
      "\n",
      "Enjoy!\n"
     ]
    }
   ],
   "source": [
    "messages.append( HumanMessage(content=\"show the first one.\") )\n",
    "\n",
    "response = chat(messages)\n",
    "messages.append( AIMessage(content=response.content) )\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2471f9d0-6000-4f48-ad1a-f5e187a8aa54",
   "metadata": {},
   "source": [
    "---\n",
    "## Documents\n",
    "\n",
    "An object that conaints a pieces of text and metadatas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648ce2f7-0d40-49ea-b0a8-6e45f9b5f2f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO how to use this concept? \n",
    "make some knowledge available?\n",
    "how to use metadata?\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "097d47df-18e7-4a58-b76c-d7438d3135bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='This is my document. it contains useful information', metadata={'author': 'Claude', 'identifier': '1234'})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "Document(\n",
    "    page_content=\"This is my document. it contains useful information\",\n",
    "    metadata={\n",
    "        'author':\"Claude\",\n",
    "        'identifier':\"1234\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee98eca-37a3-45c4-90e5-eb2360e1f5e1",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Models\n",
    "\n",
    "List of models: https://platform.openai.com/docs/models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6a1df5-7967-4841-b1eb-1ff58d0a15cb",
   "metadata": {},
   "source": [
    "---\n",
    "## Langage Model \n",
    "Text in Text out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddd6cc95-2731-47d9-8624-2392e3a83315",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSaturday.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# additnal parameters to select a mode, pass the API key ...\n",
    "llm = OpenAI(model_name=\"text-ada-001\", temperature=0.7)\n",
    "\n",
    "llm(\"What day comes after Friday?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981bf185-6a93-4eb5-bdc4-779724aab952",
   "metadata": {},
   "source": [
    "---\n",
    "## Chat Model \n",
    "Takes a series of messages and return an AI response\n",
    "\n",
    "Also make sense for a unique interaction as Chat API is less expensive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2099b96-d00a-4aa0-908d-e75a0e4b3a16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daafb170-e231-4132-bcc2-c88aac2fe475",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sure, here are some delicious tuna recipes you can try: \\n\\n1. Tuna Salad: Mix canned tuna with mayonnaise, pickle relish, chopped celery and onion. Serve on salad greens, in a sandwich or on crackers for a light lunch.\\n\\n2. Tuna Nicoise Salad: Top a bed of salad greens with cooked green beans, boiled potatoes, hard-boiled eggs, canned tuna, and cherry tomatoes. Toss with a simple vinaigrette for a healthy Mediterranean-inspired meal.\\n\\n3. Tuna Melt: Arrange tuna salad on a slice of crusty bread, top with sliced tomato and cheese, and broil until melted and bubbly.\\n\\n4. Tuna Poke Bowl: Combine cubed raw tuna with soy sauce, sesame oil, lime juice, green onions, and sesame seeds. Serve over steamed rice with sliced avocado and edamame.\\n\\n5. Grilled Tuna Steaks: Brush fresh tuna steaks with olive oil and season with salt and pepper. Grill for a few minutes on each side until cooked to your liking, and serve with a side of sautéed vegetables.\\n\\nI hope these ideas inspire you to create some tasty dishes with tuna!', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [ \n",
    "    SystemMessage(content=\"You are a nice AI and help users to feature out what to eat.\"),\n",
    "    HumanMessage(content=\"I like tuna, list some recipes.\")\n",
    "]\n",
    "     \n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91de6b77-6cb7-414b-a475-c471c3ca6c5b",
   "metadata": {},
   "source": [
    "---\n",
    "### Text Embedding Model\n",
    "\n",
    "Convert text into a series of numbers (a vector) which holds the meaning of the text.\n",
    "\n",
    "Mainly used for text comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e7d5813-a505-47d6-a8ef-98881a175c46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding length: 1536\n",
      "5 first values of the vector: [-0.0020272971596568823, -0.016961609944701195, 0.013975410722196102, -0.014824817888438702, 0.001639920868910849]\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "text=\"A leader should know all about truth and honesty, and when to see the difference. (Truck) - Bromeliad Trilogy\"\n",
    "\n",
    "text_embedding = embeddings.embed_query(text)\n",
    "\n",
    "print(f\"embedding length: {len(text_embedding)}\")\n",
    "print(f\"5 first values of the vector: {text_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab0bbcc-adc2-449f-97e4-839a36b3d4f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# 8. prompts\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c8c843-215b-4021-b493-2e7b7f78450d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-3:615547856133:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
