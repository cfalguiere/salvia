{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00afbf75-6012-4c57-9dad-4080e36c5687",
   "metadata": {},
   "source": [
    "# Langchain test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1be20e-ba74-42e4-a144-3d05841998bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## What is Langchain?\n",
    "\n",
    "LangChain is an open source framework that allows AI developers to combine Large Language Models (LLMs) with external data. \n",
    "\n",
    "**Resources**\n",
    "\n",
    "> LangChain resources\n",
    "> - Landpage: https://readthedocs.org/projects/langchain/db2d\n",
    "> - Comonents: https://docs.langchain.com/docs/category/components\n",
    "> - git: https://github.com/hwchase17/langchain.git\n",
    "> - API Reference: https://api.python.langchain.com/en/latest/\n",
    "\n",
    "> This notebook is largely based on Greg Kamradt's videos and cookbooks\n",
    "> - [Langchain tuorial suite](https://www.youtube.com/playlist?list=PLqZXAkvF1bPNQER9mLmDbntNfSpzdDIU5)\n",
    "> - [Cookbook Fundamentals](https://github.com/gkamradt/langchain-tutorials/blob/main/LangChain%20Cookbook%20Part%201%20-%20Fundamentals.ipynb)\n",
    "> - [Cookbook Comprehensive Guide](https://nathankjer.com/introduction-to-langchain/)\n",
    "\n",
    "> Additonal resources and tutorial\n",
    "> - [A Gentle Intro to Chaining LLMs, Agents, and utils via LangChain](https://towardsdatascience.com/a-gentle-intro-to-chaining-llms-agents-and-utils-via-langchain-16cd385fca81)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ab8418-6cca-4c79-9f4c-95604c87214c",
   "metadata": {},
   "source": [
    "## This notebook\n",
    "\n",
    "This notebook collects Python examples. The chapters are based oo the Langchain compoents documented here https://docs.langchain.com/docs/category/components.\n",
    "\n",
    "It has been tested on AWS SageMaker using DataScience 3.0 image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562017b1-fd67-475a-bc77-2019581bf2d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "<div style=\"background-color:green;color:black;text-align:center;padding:1rem;font-size:1.5rem;\">NOTEBOOK SETUP</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c052141f-eb06-498d-b258-536d55715688",
   "metadata": {},
   "source": [
    "**Instructions**\n",
    "\n",
    "All setups are at the top of the notebook so that you can run all this section initialize the notebook.\n",
    "\n",
    "Notebook chapters are not dependant on each other and may be run in isolation.\n",
    "\n",
    "Before running the setup you may need to create the following resources\n",
    "- request an OpenAI API keys. OpenAI APIs are not free.\n",
    "- create a Custom Search Engine in Google Search. it is free.\n",
    "- request an API key for the Google Search service. It is free.\n",
    "\n",
    "Confer to the setup sections for instruction on how to create those resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614b5320-9d11-4e53-9f04-4c3dba3b769d",
   "metadata": {},
   "source": [
    "---\n",
    "## API keys and environment\n",
    "\n",
    "Langchain will get the API keys from environment variables or function parameters.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Never show the keys in shared notebooks, whether it part of the code or a log. A simple way to avoid key leakage, is to use environement variables.  You set the environment variable in the terminal or some local configuration. If so you do not have to set the key here.\n",
    "\n",
    "- If it is easier for you to set the key here by assigning the value, do not forget to empty the string right after you run this block. The environment will be kept in memory as long as the kernel runs.\n",
    "\n",
    "- Be careful when printing the keys. Ensure that you remove the outputs. \n",
    "\n",
    "- Before sharing check that the keys are not printed out by some features of the libraries. Avoid to print libraries' objects. They often hold the API keys as a property and may disclose the key value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4397c800-3ee5-40aa-b0a7-5021985e334a",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "I Store API keys and configuration information in AWS Secrets Manager. The code below retrieves the secret holding the keys. The secret is a JSON string consisting in key/value pairs. It will be used later to set various environnement variables.\n",
    "\n",
    "When using Notebooks an SageMaker do not forget to give permissions to read this secret to SageMaker execution role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6671d484-9c5d-4818-8449-6df0189fd307",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash --out secrets \n",
    "# using AWS's Secret Manager to store keys\n",
    "# garb the keys and store it into a Pytthon variable\n",
    "export RESPONSE=$(aws secretsmanager get-secret-value --secret-id 'salvia/labbench/tests' )\n",
    "export SECRETS=$( echo $RESPONSE | jq '.SecretString | fromjson')\n",
    "\n",
    "echo $SECRETS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7188bf6d-17d8-41c0-8c24-6db9ff6e07bb",
   "metadata": {},
   "source": [
    "---\n",
    "## Langchain Setup\n",
    "\n",
    "**Resources**\n",
    "> - [LangChain GetStarted](https://python.langchain.com/docs/get_started/quickstart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c8713a09-0a7a-416c-807a-e2560796f7d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/conda/lib/python3.10/site-packages (0.0.218)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /opt/conda/lib/python3.10/site-packages (from langchain) (5.4.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.5.8)\n",
      "Requirement already satisfied: langchainplus-sdk>=0.0.17 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.0.17)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.24.3)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.10.9)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<2,>=1->langchain) (4.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (1.1.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (21.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (0.4.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (3.0.9)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51de5b76-5e76-4c8f-bd11-9445f98c6941",
   "metadata": {},
   "source": [
    "---\n",
    "## OpenAI Setup\n",
    "\n",
    "**Resources**\n",
    "> - [OpenAI tutorial on API keys](https://platform.openai.com/docs/quickstart)\n",
    "> - [OpenAI package on Pypi](https://pypi.org/project/openai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "be008522-bd17-47d4-bd6d-58da90214050",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = eval(secrets)[\"OPENAI_API_KEY\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6bc38381-5ee0-4107-9b3a-40dd9cb9302e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/conda/lib/python3.10/site-packages (0.27.8)\n",
      "Requirement already satisfied: requests>=2.20 in /opt/conda/lib/python3.10/site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (2023.5.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31520149-8e0f-4b08-b4a5-08c118fb638f",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Google Search setup\n",
    "\n",
    "**Resources**\n",
    "\n",
    ">How to configure the Google search in Langchain \n",
    "> - https://python.langchain.com/docs/ecosystem/integrations/google_search\n",
    "\n",
    "> Custom Search Engine configuration \n",
    "> - https://stackoverflow.com/questions/37083058/programmatically-searching-google-in-python-using-custom-search\n",
    "\n",
    "> CSE API \n",
    "> - repo: https://github.com/google/google-api-python-client\n",
    "> - more info: https://developers.google.com/api-client-library/python/apis/customsearch/v1\n",
    "> - complete docs: https://api-python-client-doc.appspot.com/\n",
    "\n",
    "> Get an API key\n",
    "> - https://developers.google.com/custom-search/v1/introduction\n",
    "\n",
    "> Package information\n",
    "> - [Google API client package on Pypi](https://pypi.org/project/google-api-python-client/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "09c65606-95d2-4b96-813f-db44b9ffcbb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Unlock the API and get a key \n",
    "os.environ[\"GOOGLE_API_KEY\"] = eval(secrets)[\"GOOGLE_API_KEY\"]\n",
    "# Create or use an existing Custom Search Engine\n",
    "# on the CSE page under Searcg Engone ID\n",
    "os.environ[\"GOOGLE_CSE_ID\"] = eval(secrets)[\"GOOGLE_CSE_ID\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2e169ba1-9d2d-4bfa-843c-afe61e1228e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-api-python-client\n",
      "  Using cached google_api_python_client-2.90.0-py2.py3-none-any.whl (11.4 MB)\n",
      "Collecting httplib2<1.dev0,>=0.15.0 (from google-api-python-client)\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Collecting google-auth<3.0.0.dev0,>=1.19.0 (from google-api-python-client)\n",
      "  Downloading google_auth-2.21.0-py2.py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.1/182.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-httplib2>=0.1.0 (from google-api-python-client)\n",
      "  Using cached google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 (from google-api-python-client)\n",
      "  Using cached google_api_core-2.11.1-py3-none-any.whl (120 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client)\n",
      "  Using cached uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Using cached googleapis_common_protos-1.59.1-py2.py3-none-any.whl (224 kB)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.20.3)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.31.0)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client)\n",
      "  Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (4.7.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (1.16.0)\n",
      "Collecting urllib3<2.0 (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client)\n",
      "  Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.10/site-packages (from httplib2<1.dev0,>=0.15.0->google-api-python-client) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2023.5.7)\n",
      "Installing collected packages: urllib3, uritemplate, httplib2, googleapis-common-protos, cachetools, google-auth, google-auth-httplib2, google-api-core, google-api-python-client\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.3\n",
      "    Uninstalling urllib3-2.0.3:\n",
      "      Successfully uninstalled urllib3-2.0.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed cachetools-5.3.1 google-api-core-2.11.1 google-api-python-client-2.90.0 google-auth-2.21.0 google-auth-httplib2-0.1.0 googleapis-common-protos-1.59.1 httplib2-0.22.0 uritemplate-4.1.1 urllib3-1.26.16\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eac58b-33e7-49cb-981a-a8c2a3dd573b",
   "metadata": {},
   "source": [
    "## Setup Annoy as a vector database \n",
    "\n",
    "Some examples requires a Vector Database (document selector, document retrieval).\n",
    "\n",
    "Langchain use ChromaDB by default. For whatever reason it failed to install. Used Annoy instead. An alterntive is FAIIS. You may also want to use online Vector database like Pinecone or Weaviate. \n",
    "\n",
    "Most of these packages include c++ code and requires GCC at the install time. It is not included in SageMaker DataScience 3 image. So the first step is installing GCC. \n",
    "\n",
    "**Resources**\n",
    "> - [Annoy package on Pypi](https://pypi.org/project/annoy/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "43d6d859-637f-4187-8270-8063f2c4abd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://deb.debian.org/debian bullseye InRelease [116 kB]\n",
      "Get:2 http://security.debian.org/debian-security bullseye-security InRelease [48.4 kB]\n",
      "Get:3 http://deb.debian.org/debian bullseye-updates InRelease [44.1 kB]\n",
      "Get:4 http://security.debian.org/debian-security bullseye-security/main amd64 Packages [245 kB]\n",
      "Get:5 http://deb.debian.org/debian bullseye/main amd64 Packages [8183 kB]\n",
      "Get:6 http://deb.debian.org/debian bullseye-updates/main amd64 Packages [14.8 kB]\n",
      "Fetched 8651 kB in 2s (5020 kB/s)\n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  dirmngr dpkg-dev fakeroot g++ g++-10 gnupg gnupg-l10n gnupg-utils gpg\n",
      "  gpg-agent gpg-wks-client gpg-wks-server gpgconf gpgsm gpgv\n",
      "  libalgorithm-diff-perl libalgorithm-diff-xs-perl libalgorithm-merge-perl\n",
      "  libassuan0 libdpkg-perl libfakeroot libfile-fcntllock-perl libksba8\n",
      "  liblocale-gettext-perl libnpth0 libstdc++-10-dev make patch pinentry-curses\n",
      "  xz-utils\n",
      "Suggested packages:\n",
      "  dbus-user-session libpam-systemd pinentry-gnome3 tor debian-keyring\n",
      "  g++-multilib g++-10-multilib gcc-10-doc parcimonie xloadimage scdaemon bzr\n",
      "  libstdc++-10-doc make-doc ed diffutils-doc pinentry-doc\n",
      "The following NEW packages will be installed:\n",
      "  build-essential dirmngr dpkg-dev fakeroot g++ g++-10 gnupg gnupg-l10n\n",
      "  gnupg-utils gpg gpg-agent gpg-wks-client gpg-wks-server gpgconf gpgsm\n",
      "  libalgorithm-diff-perl libalgorithm-diff-xs-perl libalgorithm-merge-perl\n",
      "  libassuan0 libdpkg-perl libfakeroot libfile-fcntllock-perl libksba8\n",
      "  liblocale-gettext-perl libnpth0 libstdc++-10-dev make patch pinentry-curses\n",
      "  xz-utils\n",
      "The following packages will be upgraded:\n",
      "  gpgv\n",
      "1 upgraded, 30 newly installed, 0 to remove and 32 not upgraded.\n",
      "Need to get 24.3 MB of archives.\n",
      "After this operation, 70.6 MB of additional disk space will be used.\n",
      "Get:1 http://deb.debian.org/debian bullseye/main amd64 liblocale-gettext-perl amd64 1.07-4+b1 [19.0 kB]\n",
      "Get:2 http://deb.debian.org/debian bullseye/main amd64 gpgv amd64 2.2.27-2+deb11u2 [626 kB]\n",
      "Get:3 http://deb.debian.org/debian bullseye/main amd64 xz-utils amd64 5.2.5-2.1~deb11u1 [220 kB]\n",
      "Get:4 http://deb.debian.org/debian bullseye/main amd64 libstdc++-10-dev amd64 10.2.1-6 [1741 kB]\n",
      "Get:5 http://deb.debian.org/debian bullseye/main amd64 g++-10 amd64 10.2.1-6 [9380 kB]\n",
      "Get:6 http://deb.debian.org/debian bullseye/main amd64 g++ amd64 4:10.2.1-1 [1644 B]\n",
      "Get:7 http://deb.debian.org/debian bullseye/main amd64 make amd64 4.3-4.1 [396 kB]\n",
      "Get:8 http://deb.debian.org/debian bullseye/main amd64 libdpkg-perl all 1.20.12 [1551 kB]\n",
      "Get:9 http://deb.debian.org/debian bullseye/main amd64 patch amd64 2.7.6-7 [128 kB]\n",
      "Get:10 http://deb.debian.org/debian bullseye/main amd64 dpkg-dev all 1.20.12 [2312 kB]\n",
      "Get:11 http://deb.debian.org/debian bullseye/main amd64 build-essential amd64 12.9 [7704 B]\n",
      "Get:12 http://deb.debian.org/debian bullseye/main amd64 libassuan0 amd64 2.5.3-7.1 [50.5 kB]\n",
      "Get:13 http://deb.debian.org/debian bullseye/main amd64 gpgconf amd64 2.2.27-2+deb11u2 [548 kB]\n",
      "Get:14 http://deb.debian.org/debian bullseye/main amd64 libksba8 amd64 1.5.0-3+deb11u2 [123 kB]\n",
      "Get:15 http://deb.debian.org/debian bullseye/main amd64 libnpth0 amd64 1.6-3 [19.0 kB]\n",
      "Get:16 http://deb.debian.org/debian bullseye/main amd64 dirmngr amd64 2.2.27-2+deb11u2 [763 kB]\n",
      "Get:17 http://deb.debian.org/debian bullseye/main amd64 libfakeroot amd64 1.25.3-1.1 [47.0 kB]\n",
      "Get:18 http://deb.debian.org/debian bullseye/main amd64 fakeroot amd64 1.25.3-1.1 [87.0 kB]\n",
      "Get:19 http://deb.debian.org/debian bullseye/main amd64 gnupg-l10n all 2.2.27-2+deb11u2 [1086 kB]\n",
      "Get:20 http://deb.debian.org/debian bullseye/main amd64 gnupg-utils amd64 2.2.27-2+deb11u2 [905 kB]\n",
      "Get:21 http://deb.debian.org/debian bullseye/main amd64 gpg amd64 2.2.27-2+deb11u2 [928 kB]\n",
      "Get:22 http://deb.debian.org/debian bullseye/main amd64 pinentry-curses amd64 1.1.0-4 [64.9 kB]\n",
      "Get:23 http://deb.debian.org/debian bullseye/main amd64 gpg-agent amd64 2.2.27-2+deb11u2 [669 kB]\n",
      "Get:24 http://deb.debian.org/debian bullseye/main amd64 gpg-wks-client amd64 2.2.27-2+deb11u2 [524 kB]\n",
      "Get:25 http://deb.debian.org/debian bullseye/main amd64 gpg-wks-server amd64 2.2.27-2+deb11u2 [516 kB]\n",
      "Get:26 http://deb.debian.org/debian bullseye/main amd64 gpgsm amd64 2.2.27-2+deb11u2 [645 kB]\n",
      "Get:27 http://deb.debian.org/debian bullseye/main amd64 gnupg all 2.2.27-2+deb11u2 [825 kB]\n",
      "Get:28 http://deb.debian.org/debian bullseye/main amd64 libalgorithm-diff-perl all 1.201-1 [43.3 kB]\n",
      "Get:29 http://deb.debian.org/debian bullseye/main amd64 libalgorithm-diff-xs-perl amd64 0.04-6+b1 [12.0 kB]\n",
      "Get:30 http://deb.debian.org/debian bullseye/main amd64 libalgorithm-merge-perl all 0.08-3 [12.7 kB]\n",
      "Get:31 http://deb.debian.org/debian bullseye/main amd64 libfile-fcntllock-perl amd64 0.22-3+b7 [35.5 kB]\n",
      "Fetched 24.3 MB in 0s (137 MB/s)              \n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package liblocale-gettext-perl.\n",
      "(Reading database ... 21534 files and directories currently installed.)\n",
      "Preparing to unpack .../liblocale-gettext-perl_1.07-4+b1_amd64.deb ...\n",
      "Unpacking liblocale-gettext-perl (1.07-4+b1) ...\n",
      "Preparing to unpack .../gpgv_2.2.27-2+deb11u2_amd64.deb ...\n",
      "Unpacking gpgv (2.2.27-2+deb11u2) over (2.2.27-2+deb11u1) ...\n",
      "Setting up gpgv (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package xz-utils.\n",
      "(Reading database ... 21549 files and directories currently installed.)\n",
      "Preparing to unpack .../00-xz-utils_5.2.5-2.1~deb11u1_amd64.deb ...\n",
      "Unpacking xz-utils (5.2.5-2.1~deb11u1) ...\n",
      "Selecting previously unselected package libstdc++-10-dev:amd64.\n",
      "Preparing to unpack .../01-libstdc++-10-dev_10.2.1-6_amd64.deb ...\n",
      "Unpacking libstdc++-10-dev:amd64 (10.2.1-6) ...\n",
      "Selecting previously unselected package g++-10.\n",
      "Preparing to unpack .../02-g++-10_10.2.1-6_amd64.deb ...\n",
      "Unpacking g++-10 (10.2.1-6) ...\n",
      "Selecting previously unselected package g++.\n",
      "Preparing to unpack .../03-g++_4%3a10.2.1-1_amd64.deb ...\n",
      "Unpacking g++ (4:10.2.1-1) ...\n",
      "Selecting previously unselected package make.\n",
      "Preparing to unpack .../04-make_4.3-4.1_amd64.deb ...\n",
      "Unpacking make (4.3-4.1) ...\n",
      "Selecting previously unselected package libdpkg-perl.\n",
      "Preparing to unpack .../05-libdpkg-perl_1.20.12_all.deb ...\n",
      "Unpacking libdpkg-perl (1.20.12) ...\n",
      "Selecting previously unselected package patch.\n",
      "Preparing to unpack .../06-patch_2.7.6-7_amd64.deb ...\n",
      "Unpacking patch (2.7.6-7) ...\n",
      "Selecting previously unselected package dpkg-dev.\n",
      "Preparing to unpack .../07-dpkg-dev_1.20.12_all.deb ...\n",
      "Unpacking dpkg-dev (1.20.12) ...\n",
      "Selecting previously unselected package build-essential.\n",
      "Preparing to unpack .../08-build-essential_12.9_amd64.deb ...\n",
      "Unpacking build-essential (12.9) ...\n",
      "Selecting previously unselected package libassuan0:amd64.\n",
      "Preparing to unpack .../09-libassuan0_2.5.3-7.1_amd64.deb ...\n",
      "Unpacking libassuan0:amd64 (2.5.3-7.1) ...\n",
      "Selecting previously unselected package gpgconf.\n",
      "Preparing to unpack .../10-gpgconf_2.2.27-2+deb11u2_amd64.deb ...\n",
      "Unpacking gpgconf (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package libksba8:amd64.\n",
      "Preparing to unpack .../11-libksba8_1.5.0-3+deb11u2_amd64.deb ...\n",
      "Unpacking libksba8:amd64 (1.5.0-3+deb11u2) ...\n",
      "Selecting previously unselected package libnpth0:amd64.\n",
      "Preparing to unpack .../12-libnpth0_1.6-3_amd64.deb ...\n",
      "Unpacking libnpth0:amd64 (1.6-3) ...\n",
      "Selecting previously unselected package dirmngr.\n",
      "Preparing to unpack .../13-dirmngr_2.2.27-2+deb11u2_amd64.deb ...\n",
      "Unpacking dirmngr (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package libfakeroot:amd64.\n",
      "Preparing to unpack .../14-libfakeroot_1.25.3-1.1_amd64.deb ...\n",
      "Unpacking libfakeroot:amd64 (1.25.3-1.1) ...\n",
      "Selecting previously unselected package fakeroot.\n",
      "Preparing to unpack .../15-fakeroot_1.25.3-1.1_amd64.deb ...\n",
      "Unpacking fakeroot (1.25.3-1.1) ...\n",
      "Selecting previously unselected package gnupg-l10n.\n",
      "Preparing to unpack .../16-gnupg-l10n_2.2.27-2+deb11u2_all.deb ...\n",
      "Unpacking gnupg-l10n (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package gnupg-utils.\n",
      "Preparing to unpack .../17-gnupg-utils_2.2.27-2+deb11u2_amd64.deb ...\n",
      "Unpacking gnupg-utils (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package gpg.\n",
      "Preparing to unpack .../18-gpg_2.2.27-2+deb11u2_amd64.deb ...\n",
      "Unpacking gpg (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package pinentry-curses.\n",
      "Preparing to unpack .../19-pinentry-curses_1.1.0-4_amd64.deb ...\n",
      "Unpacking pinentry-curses (1.1.0-4) ...\n",
      "Selecting previously unselected package gpg-agent.\n",
      "Preparing to unpack .../20-gpg-agent_2.2.27-2+deb11u2_amd64.deb ...\n",
      "Unpacking gpg-agent (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package gpg-wks-client.\n",
      "Preparing to unpack .../21-gpg-wks-client_2.2.27-2+deb11u2_amd64.deb ...\n",
      "Unpacking gpg-wks-client (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package gpg-wks-server.\n",
      "Preparing to unpack .../22-gpg-wks-server_2.2.27-2+deb11u2_amd64.deb ...\n",
      "Unpacking gpg-wks-server (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package gpgsm.\n",
      "Preparing to unpack .../23-gpgsm_2.2.27-2+deb11u2_amd64.deb ...\n",
      "Unpacking gpgsm (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package gnupg.\n",
      "Preparing to unpack .../24-gnupg_2.2.27-2+deb11u2_all.deb ...\n",
      "Unpacking gnupg (2.2.27-2+deb11u2) ...\n",
      "Selecting previously unselected package libalgorithm-diff-perl.\n",
      "Preparing to unpack .../25-libalgorithm-diff-perl_1.201-1_all.deb ...\n",
      "Unpacking libalgorithm-diff-perl (1.201-1) ...\n",
      "Selecting previously unselected package libalgorithm-diff-xs-perl.\n",
      "Preparing to unpack .../26-libalgorithm-diff-xs-perl_0.04-6+b1_amd64.deb ...\n",
      "Unpacking libalgorithm-diff-xs-perl (0.04-6+b1) ...\n",
      "Selecting previously unselected package libalgorithm-merge-perl.\n",
      "Preparing to unpack .../27-libalgorithm-merge-perl_0.08-3_all.deb ...\n",
      "Unpacking libalgorithm-merge-perl (0.08-3) ...\n",
      "Selecting previously unselected package libfile-fcntllock-perl.\n",
      "Preparing to unpack .../28-libfile-fcntllock-perl_0.22-3+b7_amd64.deb ...\n",
      "Unpacking libfile-fcntllock-perl (0.22-3+b7) ...\n",
      "Setting up libksba8:amd64 (1.5.0-3+deb11u2) ...\n",
      "Setting up libstdc++-10-dev:amd64 (10.2.1-6) ...\n",
      "Setting up g++-10 (10.2.1-6) ...\n",
      "Setting up libfile-fcntllock-perl (0.22-3+b7) ...\n",
      "Setting up libalgorithm-diff-perl (1.201-1) ...\n",
      "Setting up libnpth0:amd64 (1.6-3) ...\n",
      "Setting up libassuan0:amd64 (2.5.3-7.1) ...\n",
      "Setting up libfakeroot:amd64 (1.25.3-1.1) ...\n",
      "Setting up fakeroot (1.25.3-1.1) ...\n",
      "update-alternatives: using /usr/bin/fakeroot-sysv to provide /usr/bin/fakeroot (fakeroot) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/fakeroot.1.gz because associated file /usr/share/man/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/faked.1.gz because associated file /usr/share/man/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/es/man1/fakeroot.1.gz because associated file /usr/share/man/es/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/es/man1/faked.1.gz because associated file /usr/share/man/es/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/fr/man1/fakeroot.1.gz because associated file /usr/share/man/fr/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/fr/man1/faked.1.gz because associated file /usr/share/man/fr/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/sv/man1/fakeroot.1.gz because associated file /usr/share/man/sv/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/sv/man1/faked.1.gz because associated file /usr/share/man/sv/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "Setting up make (4.3-4.1) ...\n",
      "Setting up gnupg-l10n (2.2.27-2+deb11u2) ...\n",
      "Setting up xz-utils (5.2.5-2.1~deb11u1) ...\n",
      "update-alternatives: using /usr/bin/xz to provide /usr/bin/lzma (lzma) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzma.1.gz because associated file /usr/share/man/man1/xz.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/unlzma.1.gz because associated file /usr/share/man/man1/unxz.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzcat.1.gz because associated file /usr/share/man/man1/xzcat.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzmore.1.gz because associated file /usr/share/man/man1/xzmore.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzless.1.gz because associated file /usr/share/man/man1/xzless.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzdiff.1.gz because associated file /usr/share/man/man1/xzdiff.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzcmp.1.gz because associated file /usr/share/man/man1/xzcmp.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzgrep.1.gz because associated file /usr/share/man/man1/xzgrep.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzegrep.1.gz because associated file /usr/share/man/man1/xzegrep.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzfgrep.1.gz because associated file /usr/share/man/man1/xzfgrep.1.gz (of link group lzma) doesn't exist\n",
      "Setting up patch (2.7.6-7) ...\n",
      "Setting up libdpkg-perl (1.20.12) ...\n",
      "Setting up g++ (4:10.2.1-1) ...\n",
      "update-alternatives: using /usr/bin/g++ to provide /usr/bin/c++ (c++) in auto mode\n",
      "Setting up gpgconf (2.2.27-2+deb11u2) ...\n",
      "Setting up libalgorithm-diff-xs-perl (0.04-6+b1) ...\n",
      "Setting up liblocale-gettext-perl (1.07-4+b1) ...\n",
      "Setting up gpg (2.2.27-2+deb11u2) ...\n",
      "Setting up libalgorithm-merge-perl (0.08-3) ...\n",
      "Setting up gnupg-utils (2.2.27-2+deb11u2) ...\n",
      "Setting up pinentry-curses (1.1.0-4) ...\n",
      "Setting up gpg-agent (2.2.27-2+deb11u2) ...\n",
      "Setting up gpgsm (2.2.27-2+deb11u2) ...\n",
      "Setting up dpkg-dev (1.20.12) ...\n",
      "Setting up dirmngr (2.2.27-2+deb11u2) ...\n",
      "Setting up gpg-wks-server (2.2.27-2+deb11u2) ...\n",
      "Setting up build-essential (12.9) ...\n",
      "Setting up gpg-wks-client (2.2.27-2+deb11u2) ...\n",
      "Setting up gnupg (2.2.27-2+deb11u2) ...\n",
      "Processing triggers for libc-bin (2.31-13+deb11u3) ...\n"
     ]
    }
   ],
   "source": [
    "!apt-get update && apt-get install -y build-essential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb5d265a-34ef-4c53-940b-2b762c09859d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: annoy in /opt/conda/lib/python3.10/site-packages (1.17.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install annoy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04436cf8-3983-487f-9688-a929ba1ab86b",
   "metadata": {},
   "source": [
    "## Setup additional tools for embeddings\n",
    "\n",
    "When working with embeddings additonal packages are required.\n",
    "\n",
    "- tiktoken, as a encoder and tokenizer\n",
    "\n",
    "**Resources**\n",
    "> - [Tiktoken package on Pypi](https://pypi.org/project/tiktoken/)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "189b27d4-8afc-481d-b5bb-610a28171566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /opt/conda/lib/python3.10/site-packages (0.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2022.7.9)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2023.5.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6986800-5d16-496f-aaa2-3e48587a492f",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"background-color:green;color:black;text-align:center;padding:1rem;font-size:1.5rem;\">LANGCHAIN OVERVIEW</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6bd9f8-efef-4797-ba5d-9eb135e6b862",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Basic features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da068c2f-26b2-4c5e-b72c-23a1d50c8543",
   "metadata": {},
   "source": [
    "---\n",
    "## Get prediction from a langage model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "68e8dd21-8ada-4cd4-a532-43c2e1c3fc63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Switzerland\n",
      "2. Germany\n",
      "3. Norway\n",
      "4. Austria\n",
      "5. Iceland\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "# OPENAI_API_KEY is requested. Get it from the OpenAI site.\n",
    "# a paid account and available units are requested to be able to place a request.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "text = \"what are the 5 best countries in Europe\"\n",
    "\n",
    "# Actual API call - may tale a while.\n",
    "print(llm(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17250fa0-fe5d-47c1-9406-9822dd38fc9e",
   "metadata": {},
   "source": [
    "---\n",
    "## Manage prompts with templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2bdf0d54-abaa-4aa6-aeb5-275bebb5246b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# setup a prompt\n",
    "prompt = PromptTemplate (\n",
    "    input_variables=[\"interest\"],\n",
    "    template=\"what are the 5 best countries in Europe ranked by {interest}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f09393d7-6674-42f3-97c0-08cd12439d3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='what are the 5 best countries in Europe ranked by food'\n",
      "\n",
      "\n",
      "1. Italy\n",
      "2. France\n",
      "3. Spain\n",
      "4. Greece\n",
      "5. Portugal\n"
     ]
    }
   ],
   "source": [
    "text = prompt.format(interest=\"food\")\n",
    "print(f\"{text=}\")\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "87e3e1e6-871b-42e3-9106-96249b3f726e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='what are the 5 best countries in Europe ranked by siteseeing'\n",
      "\n",
      "\n",
      "1. Italy\n",
      "2. France\n",
      "3. Spain\n",
      "4. Greece\n",
      "5. Germany\n"
     ]
    }
   ],
   "source": [
    "text = prompt.format(interest=\"siteseeing\")\n",
    "print(f\"{text=}\")\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db89d7fa-10d1-495d-8e89-24ab1dce738f",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ae61a5-823a-4585-b308-8e83ea8b6a7a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO  what is a chain </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d607ac-9d7f-40f3-86f4-243670f42fe2",
   "metadata": {},
   "source": [
    "---\n",
    "## Built-in chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0d68f8d0-4ee4-48c5-a6b6-5f515c581eea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mdef solution():\n",
      "    \"\"\"If my age is half of my dad's age and he is going to be 60 next year, what is my current age?\"\"\"\n",
      "    dads_age_next_year = 60\n",
      "    my_age_relative = 0.5\n",
      "    my_age_current = dads_age_next_year * my_age_relative\n",
      "    result = my_age_current\n",
      "    return result\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'30.0'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import PALChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "palchain = PALChain.from_math_prompt(llm=llm, verbose=True)\n",
    "\n",
    "\n",
    "text = \"\"\"If my age is half of my dad's age \n",
    "and he is going to be 60 next year, \n",
    "what is my current age?\"\"\"\n",
    "#palchain.run(\"If my age is half of my dad's age and he is going to be 60 next year, what is my current age?\")\n",
    "palchain.run(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ed407-348c-4c00-a405-c8233fe07bb4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "    TODO <br>\n",
    "    - different result each run <br>\n",
    "    - and should be 29.5\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8337725d-b79a-41b9-99f8-d7d55339c149",
   "metadata": {},
   "source": [
    "> Entering new  chain...\n",
    "def solution():\n",
    "    \"\"\"If my age is half of my dad's age and he is going to be 60 next year, what is my current age?\"\"\"\n",
    "    dad_age_next_year = 60\n",
    "    my_age_fraction = 0.5\n",
    "    my_age_now = dad_age_next_year * my_age_fraction\n",
    "    result = my_age_now\n",
    "    return result\n",
    "\n",
    "> Finished chain.\n",
    "'30.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8acdd9d-e9d5-4ca1-b3db-6e8b2a47d81b",
   "metadata": {},
   "source": [
    "> Entering new  chain...\n",
    "def solution():\n",
    "    \"\"\"If my age is half of my dad's age and he is going to be 60 next year, what is my current age?\"\"\"\n",
    "    dad_age_current = 59\n",
    "    my_age_current = dad_age_current / 2\n",
    "    result = my_age_current\n",
    "    return result\n",
    "\n",
    "> Finished chain.\n",
    "'29.5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b09463-2f2e-4840-9602-1a22478bdd5c",
   "metadata": {},
   "source": [
    "---\n",
    "## Multi-step workflow to feed prompt into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3fe1a4f0-4c08-47aa-9304-0b6a7002968b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# setup a prompt\n",
    "prompt = PromptTemplate (\n",
    "    input_variables=[\"interest\"],\n",
    "    template=\"what are the 5 best countries in Europe ranked on {interest}\"\n",
    ")\n",
    "\n",
    "# chain feeds the prompt into the langage mmodel.\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "97622a8b-eb22-4218-803d-34d1a76c1af2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?\\n\\n1. Germany\\n2. Switzerland\\n3. United Kingdom\\n4. Sweden\\n5. Finland'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7f73c33b-634d-48fb-9552-7329658c4e23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. United Kingdom \n",
      "2. Germany \n",
      "3. France \n",
      "4. Ireland \n",
      "5. Norway\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"tv shows\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c72f8b-7623-4e34-a2b2-c8bf785fa719",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Using OpenAI Chat API (less expensive)\n",
    "requires a chain to feed the prompt into the chat \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO  move to components + desribe resource </div>\n",
    "\n",
    "**Resources**\n",
    "> - Other Chat APIs: https://api.python.langchain.com/en/latest/modules/chat_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f18f9ea2-c494-462a-aa3e-5e7ca5952d31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ranking of the best countries in Europe for food can vary depending on personal preferences. However, considering the general consensus and culinary reputation, the following countries are often regarded as having exceptional cuisine:\n",
      "\n",
      "1. Italy: Italy is renowned for its diverse and delicious cuisine. From pasta and pizza to gelato and espresso, Italian food is loved worldwide. Each region in Italy has its own specialties, making it a food lover's paradise.\n",
      "\n",
      "2. France: French cuisine is known for its elegance and sophistication. With its rich sauces, cheeses, pastries, and wines, France offers a wide range of gastronomic delights. From escargots to croissants, French food is a true celebration of flavors.\n",
      "\n",
      "3. Spain: Spanish cuisine is vibrant, flavorful, and diverse. From tapas and paella to jamón ibérico and gazpacho, Spain has a diverse range of dishes that showcase its culinary heritage. Each region has its own unique specialties, making it a dynamic food destination.\n",
      "\n",
      "4. Greece: Greek cuisine is known for its fresh ingredients, simplicity, and Mediterranean flavors. From moussaka and souvlaki to feta cheese and baklava, Greek food offers a delightful combination of herbs, olive oil, and local produce.\n",
      "\n",
      "5. Turkey: Turkish cuisine is a fusion of Middle Eastern, Mediterranean, and Balkan flavors. From kebabs and mezes to baklava and Turkish tea, the country offers a diverse and delicious array of dishes. The vibrant street food culture and rich culinary history make Turkey a great destination for food enthusiasts.\n",
      "\n",
      "It's important to note that these rankings are subjective and there are many other countries in Europe with exceptional culinary traditions.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chatopenai = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "prompt = PromptTemplate (\n",
    "    input_variables=[\"interest\"],\n",
    "    template=\"what are the 5 best countries in Europe ranked on {interest}\"\n",
    ")\n",
    "\n",
    "llmchain_chat = LLMChain(llm=chatopenai, prompt=prompt)\n",
    "print(llmchain_chat.run(\"food\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3204392e-b1de-47f0-983e-ae05901179ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Leverage LLM Math\n",
    "\n",
    "Evaluating chains that know how to do math.\n",
    "\n",
    "**Resources**\n",
    "> - Langchain module LLM_Math: ttps://python.langchain.com/docs/guides/evaluation/llm_math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "26ed6528-f540-4b2f-9617-e5ae8ffe6d26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `_type` key found, defaulting to `prompt`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: 19\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "from langchain.chains import LLMMathChain\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "prompt = load_prompt('lc://prompts/llm_math/prompt.json')\n",
    "\n",
    "# deprecated\n",
    "##chain = LLMMathChain(llm=llm, prompt=prompt)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "print(chain.run(\"what is the largest prime number lower than 20\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899d9cb9-c483-4c79-ae73-cafbacb19b7d",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522dcef8-4941-4d62-94f0-6b8dd132589e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO  what is a tool </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ea5a95-b5c1-4221-afb1-b83f86ded7a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Leverage Goocle Search\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "Make sure:\n",
    "- Google API client is installed\n",
    "- a Custome Search Engine is available (CSE)\n",
    "- the API key has been setup up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f98c7adc-982e-4693-bb7c-ee7a3e1d2e1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Élisabeth Borne has served as Prime Minister since 16 May 2022. Fifth Republic recordsEdit. Length of the successive governments\\xa0... May 16, 2022 ... President Emmanuel Macron has named Labour Minister Elisabeth Borne as prime minister to lead his ambitious reform plans, the first woman to\\xa0... Feb 22, 2018 ... SEVEN months after their prime minister was appointed in May 2017, fully 35% of the French could not name him accurately in a poll. May 16, 2022 ... Elisabeth Borne has been named the new Prime Minister of France, the first time in 30 years that a woman has held the position. May 16, 2022 ... Élisabeth Borne, the minister of labor who previously was in charge of the environment, will be the second woman to hold the post in France. Jun 24, 2022 ... The name of Lafayette is famous and respected on both sides of the Atlantic. It is our third conversation in a month, which is quite a good\\xa0... Apr 25, 2022 ... Castex could have been asked to stay on as prime minister, but told French radio earlier this month that France would be looking for a “new\\xa0... Dec 6, 2016 ... French President Francois Hollande on Tuesday appointed Interior Minister Bernard Cazeneuve as prime minister until a new president is\\xa0... May 16, 2022 ... PARIS (AP) — Centrist politician Elisabeth Borne was appointed France's new prime minister on Monday, becoming only the second woman in\\xa0... ... attempted to expel the French in 1754, but were outnumbered and defeated by the French. When news of Washington's failure reached British Prime Minister\\xa0...\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "search = GoogleSearchAPIWrapper()\n",
    "\n",
    "tool = Tool(\n",
    "    name=\"Google Search\",\n",
    "    description=\"Search Google for recent results.\",\n",
    "    func=search.run,\n",
    ")\n",
    "\n",
    "tool.run(\"French Prime Minister name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb52e51-3733-4ff5-a8cb-1094a391b98f",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543f13ea-82c7-4522-91b9-453183bb7310",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO  what is an agent </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c68946-559e-497d-a971-45c473ea5770",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c64311c2-8e52-4278-a4ac-42a22bceec08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# create a model\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# load some tools\n",
    "tools = load_tools([\"google-search\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# setup an agent\n",
    "agent = initialize_agent(tools, \n",
    "                         llm, \n",
    "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "                         verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ff155cf5-99f9-4ba4-8dbc-9d9366825475",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out how many Teslas have been sold in 2022\n",
      "Action: google_search\n",
      "Action Input: \"how many Teslas have been sold in 2022\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApr 15, 2023 ... Tesla total revenue for 2022 was 81,462 billion USD. We show it from 2018 – 2022. Tesla annual revenue 2018 - 2022. Year, Annual ... Jun 7, 2023 ... How many Tesla vehicles were delivered in 2023? ... As of June 2022, Tesla was the most valuable brand within the global automotive sector. Jan 25, 2023 ... The Model 3 and Model Y make up around 95% of the 1.31 million Teslas sold in 2022. Tesla. Tesla's finished 2022 on a tear, bolstered by recent ... Jan 7, 2023 ... Overall, Tesla reported delivering about 1.25 million Model Y and Model 3 vehicles globally in 2022. The Model 3 ranked 13th in sales at 211,641 ... Jan 3, 2023 ... The electric automaker delivered 1.3 million vehicles in 2022, up 40% from 2021. It produced nearly 1.4 million vehicles, up 47% from the prior ... May 30, 2022 ... If every vehicle sold by Tesla since its inception up to the most recent Tesla sales figures released for Q1 of 2022 are accounted for, there ... Jul 23, 2022 ... In 2022, Tesla sold 1,2 million Model 3s and Ys combined. The last sales record was around 900K in 2021, which translates to a more than 30% ... May 26, 2023 ... It left behind BMW, which had worn this crown for the last three years but sold only 332,388 cars in the United States in 2022 and was thus ... Nov 4, 2022 ... As of October 2022, Tesla, the Texas-based EV automaker, has sold more than 3 million units on a global scale. The company has found that ... Jan 2, 2023 ... AUSTIN, Texas, January 2, 2023 – In the fourth quarter, we produced over 439000 vehicles and delivered over 405000 vehicles. In 2022 ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the number of Teslas sold in 2022\n",
      "Action: Calculator\n",
      "Action Input: 1,310,000 * 2\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2620000\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 2,620,000 Teslas were sold in 2022.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2,620,000 Teslas were sold in 2022.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"How many Teslas have been sold in 2022. Multiple by 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13982ee0-5296-4ee4-ad84-d7e8f795bf96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "20688891-5c6e-4279-bfa4-8914980abb90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out who the current prime minister is and then compare their age to the President.\n",
      "Action: google_search\n",
      "Action Input: \"current prime minister of France\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPresentEdit. Élisabeth Borne has served as Prime Minister since 16 May 2022. Fifth Republic records ... May 16, 2022 ... Who is France's new Prime Minister Elisabeth Borne? French President Emmanuel Macron picked Labour Minister Elisabeth Borne as his new prime ... The current Prime Minister of France is Élisabeth Borne. She was given the job by President Emmanuel Macron on 16 May 2022. May 16, 2022 ... President Emmanuel Macron has named Labour Minister Elisabeth Borne as prime minister to lead his ambitious reform plans, the first woman to ... Feb 8, 2023 ... France's prime minister, Élisabeth Borne, sat on a recent, rainy evening in a dim room at a Red Cross shelter, listening to young women ... May 2, 2014 ... On the recommendation of the Prime Minister, President Hollande has appointed the following ministers: ; Mr Benoit HAMON, Minister of Education ... May 16, 2022 ... Élisabeth Borne, the minister of labor who previously was in charge of the environment, will be the second woman to hold the post in France. Jan 10, 2023 ... France's Prime Minister Elisabeth Borne attends a press conference to present the government's plan for a. By —. Sylvie Corbet, Associated Press ... Jun 12, 2023 ... Élisabeth Borne, Prime Minister of France. France Élisabeth Borne · Olaf Scholz, Chancellor of the Federal Republic of Germany. May 16, 2022 ... Élisabeth Borne, the French minister for labour, has been appointed prime minister – the first woman to hold the post in more than 30 years ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now need to compare the Prime Minister's age to the President's.\n",
      "Action: google_search\n",
      "Action Input: \"age of Emmanuel Macron\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mEmmanuel Jean-Michel Frédéric Macron is a French politician serving as President of France ... At the age of 39, Macron became the youngest president in French history. In the second round, held on May 7, 2017, Macron won a convincing two-thirds of the vote, becoming, at age 39, France's youngest president. Voters still found ... In 1993, at the age of 40, she met the 15-year-old Emmanuel Macron in La Providence High School where she was a teacher and he was a student and a classmate of ... Mar 16, 2023 ... PARIS (AP) — French President Emmanuel Macron ordered his prime minister to wield a special constitutional power Thursday that skirts ... Aug 19, 2017 ... &#151; -- French first lady Brigitte Macron is speaking out about the 25-year age difference between her and husband Emmanuel Macron, ... Mar 22, 2023 ... Emmanuel Macron has insisted he will not back down over raising the French pension age as he came out fighting in a live TV interview ahead ... Mar 16, 2023 ... French President Emmanuel Macron on Thursday resorted to using special constitutional powers to push his plan to raise the retirement age to ... Apr 15, 2023 ... French President Emmanuel Macron has signed into law his government's highly unpopular pension reforms, which raise the state pension age from ... Apr 24, 2022 ... Now aged 69, Ms Macron is a grandmother-of-seven and 25 years her fresh-faced 44-year-old husband's senior. Even better, perhaps, they met when ... Mar 20, 2023 ... France has revolted against President Emmanuel Macron's move to raise the retirement age from 62 to 64. The protests offer some insight into ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the age of the President and the Prime Minister.\n",
      "Final Answer: The current Prime Minister of France is Élisabeth Borne and she is younger than the President Emmanuel Macron, who is 44 years old.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The current Prime Minister of France is Élisabeth Borne and she is younger than the President Emmanuel Macron, who is 44 years old.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"\"\"Who is the current prime minister of France. \n",
    "Is he or sheyounger than the President?\"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c456a279",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out who the current prime minister is and when they will be 70.\n",
      "Action: google_search\n",
      "Action Input: \"current prime minister of France\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPresentEdit. Élisabeth Borne has served as Prime Minister since 16 May 2022. Fifth Republic records ... May 16, 2022 ... Who is France's new Prime Minister Elisabeth Borne? French President Emmanuel Macron picked Labour Minister Elisabeth Borne as his new prime ... The current Prime Minister of France is Élisabeth Borne. She was given the job by President Emmanuel Macron on 16 May 2022. May 16, 2022 ... President Emmanuel Macron has named Labour Minister Elisabeth Borne as prime minister to lead his ambitious reform plans, the first woman to ... Feb 8, 2023 ... France's prime minister, Élisabeth Borne, sat on a recent, rainy evening in a dim room at a Red Cross shelter, listening to young women ... May 2, 2014 ... On the recommendation of the Prime Minister, President Hollande has appointed the following ministers: ; Mr Benoit HAMON, Minister of Education ... May 16, 2022 ... Élisabeth Borne, the minister of labor who previously was in charge of the environment, will be the second woman to hold the post in France. Jan 10, 2023 ... France's Prime Minister Elisabeth Borne attends a press conference to present the government's plan for a. By —. Sylvie Corbet, Associated Press ... Jun 12, 2023 ... Élisabeth Borne, Prime Minister of France. France Élisabeth Borne · Olaf Scholz, Chancellor of the Federal Republic of Germany. May 16, 2022 ... Élisabeth Borne, the French minister for labour, has been appointed prime minister – the first woman to hold the post in more than 30 years ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the current prime minister of France is Élisabeth Borne and she was appointed on 16 May 2022.\n",
      "Action: Calculator\n",
      "Action Input: 16 May 2022 + 70\u001b[0m"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "LLMMathChain._evaluate(\"\n16 May 2022 + 70\n\") raised error: invalid syntax (<expr>, line 1). Please try again with a valid numerical expression",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSyntaxError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/llm_math/base.py:80\u001b[0m, in \u001b[0;36mLLMMathChain._evaluate_expression\u001b[0;34m(self, expression)\u001b[0m\n\u001b[1;32m     78\u001b[0m     local_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpi\u001b[39m\u001b[38;5;124m\"\u001b[39m: math\u001b[38;5;241m.\u001b[39mpi, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124me\u001b[39m\u001b[38;5;124m\"\u001b[39m: math\u001b[38;5;241m.\u001b[39me}\n\u001b[1;32m     79\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\n\u001b[0;32m---> 80\u001b[0m         \u001b[43mnumexpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpression\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m            \u001b[49m\u001b[43mglobal_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# restrict access to globals\u001b[39;49;00m\n\u001b[1;32m     83\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# add common mathematical functions\u001b[39;49;00m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     )\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numexpr/necompiler.py:817\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(ex, local_dict, global_dict, out, order, casting, **kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expr_key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _names_cache:\n\u001b[0;32m--> 817\u001b[0m     _names_cache[expr_key] \u001b[38;5;241m=\u001b[39m \u001b[43mgetExprNames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m names, ex_uses_vml \u001b[38;5;241m=\u001b[39m _names_cache[expr_key]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numexpr/necompiler.py:704\u001b[0m, in \u001b[0;36mgetExprNames\u001b[0;34m(text, context)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetExprNames\u001b[39m(text, context):\n\u001b[0;32m--> 704\u001b[0m     ex \u001b[38;5;241m=\u001b[39m \u001b[43mstringToExpression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     ast \u001b[38;5;241m=\u001b[39m expressionToAST(ex)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numexpr/necompiler.py:274\u001b[0m, in \u001b[0;36mstringToExpression\u001b[0;34m(s, types, context)\u001b[0m\n\u001b[1;32m    273\u001b[0m     flags \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 274\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<expr>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;03m# make VariableNode's for the names\u001b[39;00m\n",
      "\u001b[0;31mSyntaxError\u001b[0m: invalid syntax (<expr>, line 1)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[38;5;124;43mWho is the current prime minister of France. \u001b[39;49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;43mWhen will he or she be 70?\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:290\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, *args, **kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m)\u001b[49m[_output_key]\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags)[_output_key]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:166\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    167\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    168\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    169\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    170\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    154\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    155\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    156\u001b[0m     inputs,\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 160\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py:987\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m--> 987\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    994\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m    995\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[1;32m    996\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[1;32m    997\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py:850\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    848\u001b[0m         tool_run_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_prefix\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;66;03m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[0;32m--> 850\u001b[0m     observation \u001b[38;5;241m=\u001b[39m \u001b[43mtool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43magent_action\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_run_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    858\u001b[0m     tool_run_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mtool_run_logging_kwargs()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/tools/base.py:299\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    298\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_error(e)\n\u001b[0;32m--> 299\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_end(\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;28mstr\u001b[39m(observation), color\u001b[38;5;241m=\u001b[39mcolor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    303\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/tools/base.py:271\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m     tool_args, tool_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_args_and_kwargs(parsed_input)\n\u001b[1;32m    270\u001b[0m     observation \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(\u001b[38;5;241m*\u001b[39mtool_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtool_kwargs)\n\u001b[1;32m    274\u001b[0m     )\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ToolException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_tool_error:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/tools/base.py:414\u001b[0m, in \u001b[0;36mTool._run\u001b[0;34m(self, run_manager, *args, **kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;124;03m\"\"\"Use the tool.\"\"\"\u001b[39;00m\n\u001b[1;32m    412\u001b[0m new_argument_supported \u001b[38;5;241m=\u001b[39m signature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 414\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_argument_supported\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    421\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:290\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, *args, **kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m)\u001b[49m[_output_key]\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags)[_output_key]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:166\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    167\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    168\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    169\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    170\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    154\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    155\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    156\u001b[0m     inputs,\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 160\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/llm_math/base.py:149\u001b[0m, in \u001b[0;36mLLMMathChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    143\u001b[0m _run_manager\u001b[38;5;241m.\u001b[39mon_text(inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key])\n\u001b[1;32m    144\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m    145\u001b[0m     question\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key],\n\u001b[1;32m    146\u001b[0m     stop\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```output\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    147\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m_run_manager\u001b[38;5;241m.\u001b[39mget_child(),\n\u001b[1;32m    148\u001b[0m )\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_llm_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_run_manager\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/llm_math/base.py:103\u001b[0m, in \u001b[0;36mLLMMathChain._process_llm_result\u001b[0;34m(self, llm_output, run_manager)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_match:\n\u001b[1;32m    102\u001b[0m     expression \u001b[38;5;241m=\u001b[39m text_match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 103\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_expression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpression\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_text(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;124m\"\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[1;32m    105\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_text(output, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myellow\u001b[39m\u001b[38;5;124m\"\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/llm_math/base.py:87\u001b[0m, in \u001b[0;36mLLMMathChain._evaluate_expression\u001b[0;34m(self, expression)\u001b[0m\n\u001b[1;32m     79\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\n\u001b[1;32m     80\u001b[0m         numexpr\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[1;32m     81\u001b[0m             expression\u001b[38;5;241m.\u001b[39mstrip(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m         )\n\u001b[1;32m     85\u001b[0m     )\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLLMMathChain._evaluate(\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpression\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m) raised error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please try again with a valid numerical expression\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m     )\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Remove any leading and trailing brackets from the output\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m[|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m]$\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, output)\n",
      "\u001b[0;31mValueError\u001b[0m: LLMMathChain._evaluate(\"\n16 May 2022 + 70\n\") raised error: invalid syntax (<expr>, line 1). Please try again with a valid numerical expression"
     ]
    }
   ],
   "source": [
    "agent.run(\"\"\"Who is the current prime minister of France. \n",
    "When will he or she be 70?\"\"\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f7db7b-634a-49cf-939d-7e40cd1dd4de",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> FIXME  cannot add up a date and a number </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28631b0b-d1ec-4d14-bf44-091f8abd1584",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Memory - Conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bde2f8-060f-4da3-a69f-24df8df6aa63",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO  what is a conversation </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "13541ec0-684d-48da-a205-ef1c6e3e7937",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi There\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi there! It's nice to meet you. How can I help you today?\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import OpenAI, ConversationChain\n",
    "\n",
    "# create a model\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "conversation.predict(input=\"Hi There\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7271c38f-0a3e-44fb-9d8a-1014667c3467",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi There\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: What is the first thing that I said to you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' You said \"Hi there!\"'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is the first thing that I said to you?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e2613f8b-beb3-47b3-b842-e5f2b4746143",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi There\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: What is the first thing that I said to you?\n",
      "AI:  You said \"Hi there!\"\n",
      "Human: What is an alternative for the first thing that I said to you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' An alternative for the first thing you said to me is \"Hello!\"'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is an alternative for the first thing that I said to you?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3912d8-050d-4c4a-89e8-1d9ce0a1f4cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "<div style=\"background-color:green;color:black;text-align:center;padding:1rem;font-size:1.5rem;\">LANGCHAIN COMPONENTS</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684914b4-7b16-4f0b-bde2-6ef6c7df5079",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Schemas\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO whay is a schema </div>\n",
    "\n",
    "\n",
    "There are 3 types of schemas\n",
    "- text (see above)\n",
    "- Messages \n",
    "- Document\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO  resource </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7bb2c3-a6cd-4fee-b6a8-964a5f811433",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e8843ce9-1f71-4b43-a704-d456dc5653e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Switzerland\n",
      "2. Germany\n",
      "3. Norway\n",
      "4. Finland\n",
      "5. Austria\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "# OPENAI_API_KEY is requested. Get it from the OpenAI site.\n",
    "# a paid account and available units are requested to be able to place a request.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "text = \"what are the 5 best countries in Europe\"\n",
    "\n",
    "# Actual API call - may tale a while.\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf610897-44b3-4d72-8e38-3dd50b0776bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Chat messages\n",
    "Chat messages are like text with a type\n",
    "\n",
    "There are 3 types\n",
    "- System: background context that tells the AI what to do\n",
    "- Human: inputs sent by the user\n",
    "- AI : response of the AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0c8d5a96-f105-45f2-ba15-0f0b108cd9ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2f7a0269-e45c-445b-b44f-b19fc4be5e53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [ SystemMessage(content=\"You are a nice AI and help users to feature out what to eat.\")]\n",
    "     \n",
    "messages.append( HumanMessage(content=\"I like tuna, list some recipes.\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3e203228-09cd-4405-9679-c4eebfbd7c1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are some delicious tuna recipes you might enjoy:\n",
      "\n",
      "1. Tuna Salad Sandwich: Mix canned tuna with mayonnaise, chopped celery, diced red onion, and a squeeze of lemon juice. Spread the mixture on your choice of bread, add lettuce and tomato, and enjoy a classic tuna salad sandwich.\n",
      "\n",
      "2. Tuna Poke Bowl: Combine diced fresh tuna with soy sauce, sesame oil, rice vinegar, and a touch of honey. Serve it over a bed of steamed rice, and add toppings such as avocado, cucumber, edamame, and sesame seeds.\n",
      "\n",
      "3. Grilled Tuna Steaks: Season fresh tuna steaks with salt, pepper, and a bit of olive oil. Grill them for a few minutes on each side until cooked to your desired level of doneness. Serve with a squeeze of lemon juice and a side of mixed greens or roasted vegetables.\n",
      "\n",
      "4. Tuna Pasta Bake: Cook pasta according to package instructions. In a separate pan, sauté diced onion, garlic, and bell peppers. Add canned tuna, marinara sauce, and cooked pasta to the pan. Mix it all together, transfer to a baking dish, top with cheese, and bake until golden and bubbly.\n",
      "\n",
      "5. Tuna Noodle Casserole: Combine cooked egg noodles, canned tuna, frozen peas, cream of mushroom soup, and shredded cheese in a baking dish. Mix well, sprinkle breadcrumbs on top, and bake until heated through and golden on the surface.\n",
      "\n",
      "Remember to adjust these recipes according to your personal preferences and dietary restrictions. Enjoy your tuna dishes!\n"
     ]
    }
   ],
   "source": [
    "response = chat(messages)\n",
    "messages.append( AIMessage(content=response.content) )\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d140f70f-a86d-4634-899e-12401f0315cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here's the recipe for a classic Tuna Salad Sandwich:\n",
      "\n",
      "Ingredients:\n",
      "- 2 cans of tuna, drained\n",
      "- 1/4 cup mayonnaise\n",
      "- 1 celery stalk, finely chopped\n",
      "- 1/4 cup red onion, diced\n",
      "- 1 tablespoon lemon juice\n",
      "- Salt and pepper, to taste\n",
      "- Bread slices\n",
      "- Lettuce leaves\n",
      "- Tomato slices\n",
      "\n",
      "Instructions:\n",
      "1. In a bowl, combine the drained tuna, mayonnaise, celery, red onion, and lemon juice. Mix well until all the ingredients are evenly combined.\n",
      "2. Taste and season with salt and pepper according to your preference.\n",
      "3. Take a slice of bread and spread a generous amount of the tuna mixture on it.\n",
      "4. Top with lettuce leaves and tomato slices.\n",
      "5. Place another slice of bread on top to complete the sandwich.\n",
      "6. Repeat the process to make more sandwiches, if desired.\n",
      "7. Cut the sandwiches diagonally or into halves, and they are ready to serve.\n",
      "\n",
      "You can also customize this recipe by adding other ingredients such as chopped pickles, diced bell peppers, or a pinch of dried herbs. Enjoy your delicious Tuna Salad Sandwich!\n"
     ]
    }
   ],
   "source": [
    "messages.append( HumanMessage(content=\"show the first one.\") )\n",
    "\n",
    "response = chat(messages)\n",
    "messages.append( AIMessage(content=response.content) )\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af92d18-1d2b-4c55-aa71-e5a91fde59bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Examples\n",
    "An list of input output pairs thet represent the input and expected output.\n",
    "\n",
    "Used to fine tune a model or do in-context learning.\n",
    "\n",
    "**Resources**\n",
    "> - Prompt Template:  https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/few_shot_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ca97e03b-8d6c-4156-8358-a9504901d733",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== exemple prompt ===\n",
      "question: red bold\n",
      "color:red; font-style:bold;\n",
      "\n",
      "=== prompt ===\n",
      "question: red bold\n",
      "color:red; font-style:bold;\n",
      "\n",
      "question: green italic\n",
      "color:green; font-style:italic;\n",
      "\n",
      "question: blue bold\n",
      "color:blue; font-style:bold;\n",
      "\n",
      "question: pink\n",
      "color:pink;\n",
      "\n",
      "question: green\n",
      "color:green;\n",
      "\n",
      "question: pink italic\n",
      "color:pink; font-style:italic;\n",
      "\n",
      "question: pink bold\n",
      "\n",
      "=== answer ===\n",
      "\n",
      "color:pink; font-style:bold;\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# create the example set\n",
    "\n",
    "examples = [\n",
    "    { \"question\": \"red bold\", \"answer\": \"color:red; font-style:bold;\"},\n",
    "    { \"question\": \"green italic\", \"answer\":  \"color:green; font-style:italic;\"},\n",
    "    { \"question\": \"blue bold\", \"answer\":  \"color:blue; font-style:bold;\"},\n",
    "    { \"question\": \"pink\", \"answer\":  \"color:pink;\"},\n",
    "    { \"question\": \"green\", \"answer\":  \"color:green;\"},\n",
    "    { \"question\": \"pink italic\", \"answer\":  \"color:pink; font-style:italic;\"}\n",
    "    \n",
    "]    \n",
    "\n",
    "# Configure a formatter that will format the few shot examples into a string. \n",
    "# This formatter should be a PromptTemplate object.\n",
    "\n",
    "example_prompt = PromptTemplate (\n",
    "    input_variables=[\"question\", \"answer\"], \n",
    "    template=\"question: {question}\\n{answer}\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== exemple prompt ===\")\n",
    "print(example_prompt.format(**examples[0]))\n",
    "\n",
    "\n",
    "# Finally, create a FewShotPromptTemplate object. \n",
    "# This object takes in the few shot examples and the formatter for the few shot examples.\n",
    "\n",
    "prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples, \n",
    "    example_prompt=example_prompt, \n",
    "    suffix=\"question: {input}\", \n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format(input=\"pink bold\")\n",
    "\n",
    "print(\"\\n=== prompt ===\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"\\n=== answer ===\")\n",
    "print(llm(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2471f9d0-6000-4f48-ad1a-f5e187a8aa54",
   "metadata": {},
   "source": [
    "---\n",
    "## Documents\n",
    "\n",
    "An unstructured object that conaints a pieces of text and metadatas.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO  resource </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648ce2f7-0d40-49ea-b0a8-6e45f9b5f2f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO how to use this concept? \n",
    "make some knowledge available?\n",
    "how to use metadata?\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "097d47df-18e7-4a58-b76c-d7438d3135bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='This is my document. it contains useful information', metadata={'author': 'Claude', 'identifier': '1234'})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "Document(\n",
    "    page_content=\"This is my document. it contains useful information\",\n",
    "    metadata={\n",
    "        'author':\"Claude\",\n",
    "        'identifier':\"1234\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee98eca-37a3-45c4-90e5-eb2360e1f5e1",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Models\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO  what is a model </div>\n",
    "\n",
    "**Resources**\n",
    "> - List of models: https://platform.openai.com/docs/models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6a1df5-7967-4841-b1eb-1ff58d0a15cb",
   "metadata": {},
   "source": [
    "---\n",
    "## Langage Model \n",
    "Text in Text out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ddd6cc95-2731-47d9-8624-2392e3a83315",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nMonday.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# additnal parameters to select a mode, pass the API key ...\n",
    "llm = OpenAI(model_name=\"text-ada-001\", temperature=0.7)\n",
    "\n",
    "llm(\"What day comes after Friday?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981bf185-6a93-4eb5-bdc4-779724aab952",
   "metadata": {},
   "source": [
    "---\n",
    "## Chat Model \n",
    "Takes a series of messages and return an AI response\n",
    "\n",
    "Also make sense for a unique interaction as Chat API is less expensive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c2099b96-d00a-4aa0-908d-e75a0e4b3a16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "daafb170-e231-4132-bcc2-c88aac2fe475",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sure! Here are a few recipes featuring tuna that you might enjoy:\\n\\n1. Tuna Salad: Mix canned tuna, mayonnaise, chopped celery, diced red onions, and a squeeze of lemon juice. Serve it on a bed of lettuce or between two slices of freshly baked bread.\\n\\n2. Tuna Pasta: Cook your favorite pasta and toss it with a sauce made from canned tuna, olive oil, minced garlic, cherry tomatoes, olives, and a sprinkle of crushed red pepper flakes.\\n\\n3. Tuna Poke Bowl: Marinate cubes of fresh tuna in a mixture of soy sauce, sesame oil, ginger, and garlic. Serve it over a bed of rice with toppings like avocado, cucumber, seaweed, and sesame seeds.\\n\\n4. Tuna Melt: Spread tuna salad (from the first recipe above) onto slices of bread, top it with sliced tomatoes and cheese, and toast it in the oven until the cheese is melted and bubbly.\\n\\n5. Tuna Steaks: Season fresh tuna steaks with salt, pepper, and a drizzle of olive oil. Sear them in a hot pan for a few minutes on each side until they are medium-rare. Serve with your choice of side dish like roasted vegetables or garlic butter rice.\\n\\n6. Tuna Noodle Casserole: Combine cooked noodles, canned tuna, cream of mushroom soup, peas, and shredded cheese in a casserole dish. Bake it in the oven until it's hot and bubbly.\\n\\nThese are just a few ideas to get you started. There are many more delicious tuna recipes out there, so feel free to explore and experiment with different flavors and ingredients. Enjoy your culinary adventures!\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [ \n",
    "    SystemMessage(content=\"You are a nice AI and help users to feature out what to eat.\"),\n",
    "    HumanMessage(content=\"I like tuna, list some recipes.\")\n",
    "]\n",
    "     \n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91de6b77-6cb7-414b-a475-c471c3ca6c5b",
   "metadata": {},
   "source": [
    "---\n",
    "### Text Embedding Model\n",
    "\n",
    "Convert text into a series of numbers (a vector) which holds the meaning of the text.\n",
    "\n",
    "Mainly used for text comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8e7d5813-a505-47d6-a8ef-98881a175c46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding length: 1536\n",
      "5 first values of the vector: [-0.0020272971596568823, -0.016961609944701195, 0.013975410722196102, -0.014824817888438702, 0.001639920868910849]\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "text=\"A leader should know all about truth and honesty, and when to see the difference. (Truck) - Bromeliad Trilogy\"\n",
    "\n",
    "text_embedding = embeddings.embed_query(text)\n",
    "\n",
    "print(f\"embedding length: {len(text_embedding)}\")\n",
    "print(f\"5 first values of the vector: {text_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab0bbcc-adc2-449f-97e4-839a36b3d4f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# 8. prompts\n",
    "Text sent the langage model\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO  resource </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc36ebe4-aba6-4cdb-850d-1937839c8608",
   "metadata": {},
   "source": [
    "---\n",
    "## Simple prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d37b8bcc-33ca-4deb-9845-81675d529758",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This statement is incorrect because tomorrow is Tuesday, not Wednesday.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0.9)\n",
    "\n",
    "# write a simple  prompt. use \"\"\" to allow multiline string.\n",
    "prompt = \"\"\"\n",
    "Today is Monday. Tomorrow is Wednesday.\n",
    "\n",
    "What is wrong with this statement?\n",
    "\"\"\"\n",
    "\n",
    "# query the model\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f8a344-70ab-471d-918e-ee98e7e4fdf1",
   "metadata": {},
   "source": [
    "---\n",
    "## Prompt with template and placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "837116ab-bf8b-49ed-bb3f-a896e14fb14d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt='\\n    Today is Monday. Tomorrow is Wednesday.\\n\\n    What is wrong with this statement?\\n    '\n",
      "\n",
      "This statement is incorrect because tomorrow is Tuesday, not Wednesday.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0.9)\n",
    "\n",
    "# setup a prompt. use \"\"\" to allow multiline string.\n",
    "template = PromptTemplate (\n",
    "    input_variables=[\"today\", \"tomorrow\"],\n",
    "    template=\"\"\"\n",
    "    Today is {today}. Tomorrow is {tomorrow}.\n",
    "\n",
    "    What is wrong with this statement?\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "prompt = template.format(today=\"Monday\", tomorrow=\"Wednesday\")\n",
    "print(f\"{prompt=}\")\n",
    "\n",
    "# query the model\n",
    "\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "aa35aff7-3105-4aed-82ab-7281c517eca3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt='\\n    Today is Thursday. Tomorrow is Friday.\\n\\n    What is wrong with this statement?\\n    '\n",
      "\n",
      "This statement is factually correct, so there is nothing wrong with it.\n"
     ]
    }
   ],
   "source": [
    "prompt = template.format(today=\"Thursday\", tomorrow=\"Friday\")\n",
    "print(f\"{prompt=}\")\n",
    "\n",
    "# query the model\n",
    "\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc54ed33-925e-405f-8eaf-a4a57ed78771",
   "metadata": {},
   "source": [
    "---\n",
    "## Example selectors and Few Shot Learning\n",
    "\n",
    "A way to select from a series of examples in few shot learning \n",
    "\n",
    "**Resources**\n",
    "> - Example Selector: https://api.python.langchain.com/en/latest/modules/example_selector.html\n",
    "> - Few shot learning: https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/few_shot_examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e40343-df4e-4b65-8951-8372060ca6d0",
   "metadata": {},
   "source": [
    "### Example selectors and Few Shot Learning with NGram\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> FIXME </div>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f315f07-af06-480f-80b0-23c56c61a67d",
   "metadata": {
    "tags": []
   },
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.example_selector import NGramOverlapExampleSelector\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# create the example set\n",
    "\n",
    "examples = [\n",
    "    { \"question\": \"red bold\", \"answer\": \"color:red; font-style:bold;\"},\n",
    "    { \"question\": \"green italic\", \"answer\":  \"color:green; font-style:italic;\"},\n",
    "    { \"question\": \"blue bold\", \"answer\":  \"color:blue; font-style:bold;\"},\n",
    "    { \"question\": \"pink\", \"answer\":  \"color:pink;\"},\n",
    "    { \"question\": \"green\", \"answer\":  \"color:green;\"},\n",
    "    { \"question\": \"pink italic\", \"answer\":  \"color:pink; font-style:italic;\"}\n",
    "    \n",
    "]    \n",
    "\n",
    "# Configure a formatter that will format the few shot examples into a string. \n",
    "# This formatter should be a PromptTemplate object.\n",
    "\n",
    "example_prompt = PromptTemplate (\n",
    "    input_variables=[\"question\", \"answer\"], \n",
    "    template=\"question: {question}\\n{answer}\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== exemple prompt ===\")\n",
    "print(example_prompt.format(**examples[0]))\n",
    "\n",
    "\n",
    "# Select and order examples based on ngram overlap score (sentence_bleu score).\n",
    "\n",
    "question = \"pink bold\"\n",
    "\n",
    "example_selector = NGramOverlapExampleSelector.select_examples(\n",
    "    examples,\n",
    "    question\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # This is the list of examples available to select from.\n",
    "    examples,\n",
    "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    OpenAIEmbeddings(),\n",
    "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    Chroma,\n",
    "    # This is the number of examples to produce.\n",
    "    k=1\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Finally, create a FewShotPromptTemplate object. \n",
    "# This object takes in the few shot examples and the formatter for the few shot examples.\n",
    "\n",
    "prompt_template = FewShotPromptTemplate(\n",
    "    #example_selector=example_selector, \n",
    "    examples=selected_examples, \n",
    "    example_prompt=example_prompt, \n",
    "    suffix=\"question: {input}\", \n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format(input=question)\n",
    "\n",
    "print(\"\\n=== prompt ===\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"\\n=== answer ===\")\n",
    "print(llm(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be5012c-2708-417f-a6e8-4a1beb5349df",
   "metadata": {},
   "source": [
    "### Example selectors and Few Shot Learning with similarities\n",
    "\n",
    "requires a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "be6e5756-6345-4b70-844b-f7959fe0d70e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== exemple prompt ===\n",
      "question: red bold\n",
      "color:red; font-style:bold;\n",
      "\n",
      "=== prompt ===\n",
      "question: red bold\n",
      "color:red; font-style:bold;\n",
      "\n",
      "question: pink italic\n",
      "color:pink; font-style:italic;\n",
      "\n",
      "question: pink bold\n",
      "\n",
      "=== answer ===\n",
      "\n",
      "color:pink; font-style:bold;\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Annoy\n",
    "#from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# create the example set\n",
    "\n",
    "examples = [\n",
    "    { \"question\": \"red bold\", \"answer\": \"color:red; font-style:bold;\"},\n",
    "    { \"question\": \"green italic\", \"answer\":  \"color:green; font-style:italic;\"},\n",
    "    { \"question\": \"blue bold\", \"answer\":  \"color:blue; font-style:bold;\"},\n",
    "    { \"question\": \"pink\", \"answer\":  \"color:pink;\"},\n",
    "    { \"question\": \"green\", \"answer\":  \"color:green;\"},\n",
    "    { \"question\": \"pink italic\", \"answer\":  \"color:pink; font-style:italic;\"}\n",
    "    \n",
    "]    \n",
    "\n",
    "# Configure a formatter that will format the few shot examples into a string. \n",
    "# This formatter should be a PromptTemplate object.\n",
    "\n",
    "example_prompt = PromptTemplate (\n",
    "    input_variables=[\"question\", \"answer\"], \n",
    "    template=\"question: {question}\\n{answer}\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== exemple prompt ===\")\n",
    "print(example_prompt.format(**examples[0]))\n",
    "\n",
    "# Example selector that selects examples based on SemanticSimilarity.\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # This is the list of examples available to select from.\n",
    "    examples,\n",
    "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    OpenAIEmbeddings(),\n",
    "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    #Chroma,\n",
    "    Annoy,\n",
    "    # This is the number of examples to produce.\n",
    "    k=2\n",
    ")\n",
    "\n",
    "# Finally, create a FewShotPromptTemplate object. \n",
    "# This object takes in the few shot examples and the formatter for the few shot examples.\n",
    "\n",
    "prompt_template = FewShotPromptTemplate(\n",
    "    example_selector=example_selector, \n",
    "    example_prompt=example_prompt, \n",
    "    suffix=\"question: {input}\", \n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format(input=\"pink bold\")\n",
    "\n",
    "print(\"\\n=== prompt ===\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"\\n=== answer ===\")\n",
    "print(llm(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2b5aae-d6c0-4ebe-a12e-521b2f807045",
   "metadata": {},
   "source": [
    "---\n",
    "## Output Parser and response format\n",
    "\n",
    "A way to format the outpu\n",
    "- Format nstructions: An autogenerated prompt telling how the result should be formatted\n",
    "- parser: a method which will extract the output int hte desired format. you may prvie a custom parser\n",
    "\n",
    "\n",
    "**Resources**\n",
    "> - OutputParser:https://docs.langchain.com/docs/components/prompts/output-parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "46e0cc93-9499-4ac7-a3b2-669bb3f121e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "format_instructions\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This is a poorly formatted string.\n",
      "\t\"good_string\": string  // This is a your string reformatted.\n",
      "}\n",
      "```\n",
      "\n",
      "prompt\n",
      "\n",
      "You will be given a poorly formatted string from a user. \n",
      "Reformat it and make sure all the words are spelled correctly.\n",
      "\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This is a poorly formatted string.\n",
      "\t\"good_string\": string  // This is a your string reformatted.\n",
      "}\n",
      "```\n",
      "\n",
      "% USER_INPUT:\n",
      "Wellcom to Californya!\n",
      "\n",
      "YOUR RESPONSE:\n",
      "\n",
      "\n",
      "response=\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": \"Wellcom to Californya!\"\n",
      "\t\"good_string\": \"Welcome to California!\"\n",
      "}\n",
      "```\n",
      "\n",
      "parsed output=\n"
     ]
    },
    {
     "ename": "OutputParserException",
     "evalue": "Got invalid JSON object. Error: Expecting ',' delimiter: line 3 column 2 (char 43)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/output_parsers/json.py:52\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[0;34m(text, expected_keys)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     json_obj \u001b[38;5;241m=\u001b[39m \u001b[43mparse_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/output_parsers/json.py:34\u001b[0m, in \u001b[0;36mparse_json_markdown\u001b[0;34m(json_string)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Parse the JSON string into a Python dictionary\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m parsed \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parsed\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 3 column 2 (char 43)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# gets the JSON document\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mparsed output=\u001b[39m\u001b[38;5;124m\"\u001b[39m)      \n\u001b[0;32m---> 59\u001b[0m \u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m                   \n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/output_parsers/structured.py:43\u001b[0m, in \u001b[0;36mStructuredOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     42\u001b[0m     expected_keys \u001b[38;5;241m=\u001b[39m [rs\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m rs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_schemas]\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_and_check_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/output_parsers/json.py:54\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[0;34m(text, expected_keys)\u001b[0m\n\u001b[1;32m     52\u001b[0m     json_obj \u001b[38;5;241m=\u001b[39m parse_json_markdown(text)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot invalid JSON object. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m expected_keys:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m json_obj:\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Got invalid JSON object. Error: Expecting ',' delimiter: line 3 column 2 (char 43)"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0.9)\n",
    "\n",
    "# how you would like the response to be structured\n",
    "# periods at the send of sentence are required. \n",
    "# If not there description ends up in the json text and break the JSON format\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"bad_string\", description=\"This is a poorly formatted string.\"),\n",
    "    ResponseSchema(name=\"good_string\", description=\"This is a your string reformatted.\")\n",
    "]\n",
    "\n",
    "# How you would like to parse your output\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# check instructions\n",
    "format_instructions =output_parser.get_format_instructions()\n",
    "print(\"\\nformat_instructions\")      \n",
    "print(format_instructions)      \n",
    "\n",
    "template = \"\"\"\n",
    "You will be given a poorly formatted string from a user. \n",
    "Reformat it and make sure all the words are spelled correctly.\n",
    "\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER_INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=['user_input'],\n",
    "    partial_variables={'format_instructions': format_instructions},\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# format the user input as a prompt\n",
    "# for whateveer reason it does not work well with format.\n",
    "# format_promt retruns an object, not a string and should be converted to a string \n",
    "prompt = prompt_template.format_prompt(user_input=\"Wellcom to Californya!\").to_string()\n",
    "print(\"\\nprompt\")\n",
    "print(prompt)\n",
    "\n",
    "# gets the response\n",
    "response = llm(prompt)\n",
    "print(\"\\nresponse=\")      \n",
    "print(response)      \n",
    "\n",
    "# gets the JSON document\n",
    "print(\"\\nparsed output=\")      \n",
    "output_parser.parse(response)                   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40448b27-5ee7-48a0-a86b-6192bc8e8869",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> FIXME  parser error </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3bc48a-c126-4f79-b8d8-5f65b3387ecd",
   "metadata": {},
   "source": [
    "---\n",
    "# 9. Indexes\n",
    "\n",
    "Help to structure documents so that they are easier to work with.\n",
    "\n",
    "- Loaders\n",
    "- Splitters\n",
    "- Rtrievers\n",
    "- VectorStores\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO  resource </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7a68d6-2561-4630-89b0-f543d3880746",
   "metadata": {},
   "source": [
    "## Document Loaders\n",
    "\n",
    "Easy ways to import documents from other sources \n",
    "and make it available for use in your language models.\n",
    "\n",
    "**Resources**\n",
    "> -  Document Loaders: https://python.langchain.com/docs/modules/data_connection/document_loaders\n",
    "> - List of loaders: https://github.com/hwchase17/langchain/tree/master/langchain/document_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "77127064-7c75-4561-81a7-a3d73f08cb44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76 comments\n",
      "\n",
      "Here's a sample (first 100 chars of the 3 first items)\n",
      "Ozzie_osman 5 months ago  \n",
      "             | next [–] \n",
      "\n",
      "LangChain is awesome. For people not sure what \n",
      "Ozzie_osman 5 months ago  \n",
      "             | parent | next [–] \n",
      "\n",
      "Also, another library to check out is \n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import HNLoader\n",
    " \n",
    "# Setup a Hacker News loader\n",
    "loader = HNLoader(\"https://news.ycombinator.com/item?id=34422627\")\n",
    " \n",
    "data = loader.load()\n",
    " \n",
    "print(f\"Found {len(data)} comments\")\n",
    "\n",
    "\n",
    "sample = '\\n'.join([x.page_content[:100] for x in data[:2]])\n",
    "print(\"\\nHere's a sample (first 100 chars of the 3 first items)\")\n",
    "print(sample)\n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd41d74-2bd1-4d5c-8409-b27c6cdb5d1a",
   "metadata": {},
   "source": [
    "---\n",
    "## Text Splitters\n",
    "\n",
    "allow you to split a document into smaller chunk\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO  resource </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c32da233-903e-4ae2-8c5e-42cd0c6f3d54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 document(s)\n",
      "docuument content\n",
      "ght Alice, \"without pictures or\n",
      "conversations?\"\n",
      "So she was considering in her own mind (as well as she could, for the\n",
      "day made her feel very sleepy and stupid), whether the pleasure of\n",
      "making a daisy-chain would be worth the trouble of getting up and\n",
      "picking the daisies, when suddenly a White Rabbit with pink eyes ran\n",
      "close by her.\n",
      "There was nothing so very remarkable in that, nor did Alice think it so\n",
      "[Pg 4]very much out of the way to hear the Rabbit say to itself, \"Oh dear! Oh\n",
      "dear! I shall be\n",
      "\n",
      "Splitted into 478 parts\n",
      "Preview:\n",
      "it, \"and what is the use of a book,\" thought Alice, \"without pictures or\n",
      "conversations?\"\n",
      "So she was considering in her own mind (as well as she could, for the \n",
      "-\n",
      "day made her feel very sleepy and stupid), whether the pleasure of\n",
      "making a daisy-chain would be worth the trouble of getting up and \n",
      "-\n",
      "picking the daisies, when suddenly a White Rabbit with pink eyes ran\n",
      "close by her.\n",
      "There was nothing so very remarkable in that, nor did Alice think it so\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import BSHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    " \n",
    "# This is a long document we can split up.    \n",
    "document_path = \"data/Alice's Adventures in Wonderland, by Lewis Carroll.html\"\n",
    " \n",
    "# Setup a HTML loader\n",
    "loader = BSHTMLLoader(document_path)\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Found {len(documents)} document(s)\")\n",
    "\n",
    "\n",
    "print(\"docuument content\")\n",
    "start = 2200\n",
    "print(documents[0].page_content[start-200:start+300])\n",
    "\n",
    " \n",
    "# The recommended TextSplitter is the RecursiveCharacterTextSplitter. \n",
    "# This will split documents recursively by different characters - starting with \"\\n\\n\", then \"\\n\", then \" \".\n",
    "# This is nice because it will try to keep all the semantically relevant content in the same place \n",
    "# for as long as possible.\n",
    "# Important parameters to know here are chunkSize and chunkOverlap. \n",
    "# chunkSize controls the max size (in terms of number of characters) of the final documents. \n",
    "# chunkOverlap specifies how much overlap there should be between chunks. \n",
    "# in practice they default to 4000 and 200 respectively.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    " \n",
    "texts = text_splitter.create_documents([document[0].page_content])\n",
    " \n",
    "print(f\"\\nSplitted into {len(texts)} parts\")\n",
    " \n",
    "print(\"Preview:\")\n",
    "i = int(start/150)\n",
    "print(texts[i+1].page_content, \"\\n-\")\n",
    "print(texts[i+2].page_content, \"\\n-\")\n",
    "print(texts[i+3].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a947a9d1-e893-4ac8-b854-e5a777363c27",
   "metadata": {},
   "source": [
    "---\n",
    "## Indexes and Retrievers \n",
    "A retriever is an interface that returns documents given an unstructured query. \n",
    "\n",
    "A retriever does not need to be able to store documents, only to return (or retrieve) it. \n",
    "\n",
    "It usually relies to a vector store as a document management backbone.\n",
    "\n",
    "A vector store is a particular type of database optimized for storing documents and their embeddings, and then fetching of the most relevant documents for a particular query, ie. those whose embeddings are most similar to the embedding of the query.\n",
    "\n",
    "- local : ChromaDB, FAISS, Annoy\n",
    "- Online: Pinecone, Weaviate\n",
    "\n",
    "However a retriever is more general than a vector store and there are other types of retrievers as well, e.g. Wikipedia or search engines like Elestic Search or Kendra.\n",
    "\n",
    "\n",
    "Question answering over documents consists of four steps:\n",
    "1. Create an index\n",
    "2. Create a Retriever from that index\n",
    "3. Create a question answering chain\n",
    "4. Ask questions\n",
    "\n",
    "**Resources**\n",
    "> - Lit of retrievers: https://python.langchain.com/docs/modules/data_connection/retrievers/\n",
    "> - LangChain Supported VectorStores: https://api.python.langchain.com/en/latest/modules/vectorstores.html\n",
    "> - Retrievers: https://github.com/hwchase17/langchain/tree/master/langchain/retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3769182e-3197-4cc3-8ab4-d05717fb2922",
   "metadata": {},
   "source": [
    "### Store document in a Vector Store and retrieve information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fedfae67-fba3-4e5d-ac09-d2d6b465dcb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 document(s)\n",
      "\n",
      "Splitted into 182 parts\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import BSHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Annoy\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    " \n",
    "# This is the source document.    \n",
    "document_path = \"data/Alice's Adventures in Wonderland, by Lewis Carroll.html\"\n",
    " \n",
    "# Setup a HTML loader\n",
    "loader = BSHTMLLoader(document_path)\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Found {len(documents)} document(s)\")\n",
    "\n",
    "\n",
    "# Get your splitter ready\n",
    "# Using small chunk for the sake of example. \n",
    "# in practice they default to 4000 and 200 respectively.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=25)\n",
    " \n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"\\nSplitted into {len(texts)} parts\")\n",
    "\n",
    "# Get embedding engine ready\n",
    "embeddings = OpenAIEmbeddings()\n",
    " \n",
    "# Embedd your texts andd store them in the vector database\n",
    "# dtabase is in memory. it might be savecd to a file and loader later on.\n",
    "db = Annoy.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "28e39c41-6df4-445f-8314-db8d7c49624a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 4\n",
      "the White Rabbit was still in sight, hurrying down it. There was not a\n",
      "moment to be lost. Away went Alice like the wind and was just in time to\n",
      "hear it say, as it turned a corner, \"Oh, my ears and whi\n",
      "\n",
      "IV—THE RABBIT SENDS IN A LITTLE BILL\n",
      "It was the White Rabbit, trotting slowly back again and looking\n",
      "anxiously about as it went, as if it had lost something; Alice heard it\n",
      "muttering to itself, \"The D\n",
      "\n",
      "\"Call the first witness,\" said the King; and the White Rabbit blew three\n",
      "blasts on the trumpet and called out, \"First witness!\"\n",
      "The first witness was the Hatter. He came in with[Pg 44] a teacup in one\n",
      "\n",
      "you please, sir—\" The Rabbit started violently, dropped the white\n",
      "kid-gloves and the fan and skurried away into the darkness as hard as he\n",
      "could go.\n",
      "Alice took up the fan and gloves and she kept fanni\n"
     ]
    }
   ],
   "source": [
    "# Init a retriever for this db\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# retrieve indexed documents relevant for the query\n",
    "query = \"who is the White Rabbit?\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "print(f\"\\nFound {len(docs)}\")\n",
    "\n",
    "samples = \"\\n\\n\".join([x.page_content[:200] for x in docs[:5]])\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "448aa357-9d47-4db2-af62-cd4acd58bd78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The White Rabbit is a character in Alice's Adventures in Wonderland by Lewis Carroll.\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Asking theLLM\n",
    "# the response will be based on the retrieved documents \n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b97504cf-53c0-4a59-848d-459279b1499f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The White Rabbit is a character in Alice's Adventures in Wonderland by Lewis Carroll.\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ee55b6-6c70-48d0-862b-8a411fc63b57",
   "metadata": {},
   "source": [
    "### One line index creation and information retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1c90307a-e8a0-4cb1-a514-38cd52f74685",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 5268, which is longer than the specified 500\n",
      "Created a chunk of size 1756, which is longer than the specified 500\n",
      "Created a chunk of size 5973, which is longer than the specified 500\n",
      "Created a chunk of size 2017, which is longer than the specified 500\n",
      "Created a chunk of size 1972, which is longer than the specified 500\n",
      "Created a chunk of size 1105, which is longer than the specified 500\n",
      "Created a chunk of size 2369, which is longer than the specified 500\n",
      "Created a chunk of size 4726, which is longer than the specified 500\n",
      "Created a chunk of size 3384, which is longer than the specified 500\n",
      "Created a chunk of size 2193, which is longer than the specified 500\n",
      "Created a chunk of size 4590, which is longer than the specified 500\n",
      "Created a chunk of size 1344, which is longer than the specified 500\n",
      "Created a chunk of size 1672, which is longer than the specified 500\n",
      "Created a chunk of size 3707, which is longer than the specified 500\n",
      "Created a chunk of size 2612, which is longer than the specified 500\n",
      "Created a chunk of size 584, which is longer than the specified 500\n",
      "Created a chunk of size 1933, which is longer than the specified 500\n",
      "Created a chunk of size 677, which is longer than the specified 500\n",
      "Created a chunk of size 1354, which is longer than the specified 500\n",
      "Created a chunk of size 983, which is longer than the specified 500\n",
      "Created a chunk of size 1185, which is longer than the specified 500\n",
      "Created a chunk of size 675, which is longer than the specified 500\n",
      "Created a chunk of size 582, which is longer than the specified 500\n",
      "Created a chunk of size 1046, which is longer than the specified 500\n",
      "Created a chunk of size 564, which is longer than the specified 500\n",
      "Created a chunk of size 659, which is longer than the specified 500\n",
      "Created a chunk of size 738, which is longer than the specified 500\n",
      "Created a chunk of size 1659, which is longer than the specified 500\n",
      "Created a chunk of size 635, which is longer than the specified 500\n",
      "Created a chunk of size 809, which is longer than the specified 500\n",
      "Created a chunk of size 801, which is longer than the specified 500\n",
      "Created a chunk of size 683, which is longer than the specified 500\n",
      "Created a chunk of size 596, which is longer than the specified 500\n",
      "Created a chunk of size 507, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 4\n",
      "the White Rabbit was still in sight, hurrying down it. There was not a\n",
      "moment to be lost. Away went Alice like the wind and was just in time to\n",
      "hear it say, as it turned a corner, \"Oh, my ears and whi\n",
      "\n",
      "IV—THE RABBIT SENDS IN A LITTLE BILL\n",
      "It was the White Rabbit, trotting slowly back again and looking\n",
      "anxiously about as it went, as if it had lost something; Alice heard it\n",
      "muttering to itself, \"The D\n",
      "\n",
      "\"Call the first witness,\" said the King; and the White Rabbit blew three\n",
      "blasts on the trumpet and called out, \"First witness!\"\n",
      "The first witness was the Hatter. He came in with[Pg 44] a teacup in one\n",
      "\n",
      "you please, sir—\" The Rabbit started violently, dropped the white\n",
      "kid-gloves and the fan and skurried away into the darkness as hard as he\n",
      "could go.\n",
      "Alice took up the fan and gloves and she kept fanni\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import BSHTMLLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Annoy\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "# This is the source document.    \n",
    "document_path = \"data/Alice's Adventures in Wonderland, by Lewis Carroll.html\"\n",
    " \n",
    "# Setup a HTML loader\n",
    "loader = BSHTMLLoader(document_path)\n",
    "\n",
    "# creating an indexer\n",
    "# default to Chroma as a vector database\n",
    "# Use CharacterTextSplitter. May also be RecursiveCharacterTextSplitter.\n",
    "index_creator = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=Annoy,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    text_splitter=CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    ")\n",
    "\n",
    "index = index_creator.from_loaders([loader])\n",
    "\n",
    "# retrieve indexed documents relevant for the query\n",
    "query = \"who is the White Rabbit?\"\n",
    "index.query(query)\n",
    "\n",
    "print(f\"\\nFound {len(docs)}\")\n",
    "\n",
    "samples = \"\\n\\n\".join([x.page_content[:200] for x in docs[:5]])\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e27cfddd-ed2d-4892-a521-aa473ccd2c67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The White Rabbit is a character in Alice's Adventures in Wonderland. He is a white rabbit wearing a waistcoat and carrying a watch. He is in a hurry and is looking for the Duchess' fan and gloves.\""
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Ask the question to the model \n",
    "# the response will be based on the retrieved documents \n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), \n",
    "                                 chain_type=\"stuff\", \n",
    "                                 retriever=index.vectorstore.as_retriever())\n",
    "\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1de6f5a1-9765-424c-819c-b79ccb8acc97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The White Rabbit is a character in Alice's Adventures in Wonderland. He is a white-furred rabbit with pink eyes who wears a waistcoat and carries a pocket watch. He is known for being late and speaks in a hurried and anxious manner.\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0081edf7-2762-4667-895f-5849b15424ea",
   "metadata": {},
   "source": [
    "---\n",
    "## Wikipedia retriever\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a7a9c8-d0bc-4a5c-9f9d-fff2f870d0ed",
   "metadata": {},
   "source": [
    "---\n",
    "# 9. Memory\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc922de-52de-4c30-8faa-4feded2137d2",
   "metadata": {},
   "source": [
    "---\n",
    "# 10. Chains\n",
    "\n",
    "A way to chain model and feed the output of one moodel to the input of another model\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ede334-ad80-49fe-82aa-c9b22c12ac8d",
   "metadata": {},
   "source": [
    "## Simple sequential model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babb2462-d3a2-4d88-b15a-2c811781175b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-3:615547856133:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
