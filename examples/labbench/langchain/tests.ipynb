{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00afbf75-6012-4c57-9dad-4080e36c5687",
   "metadata": {},
   "source": [
    "# Langchain Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dc41cd-690a-4c2d-8253-dd71b609560a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c1be20e-ba74-42e4-a144-3d05841998bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## What is LangChain?\n",
    "\n",
    "LangChain is an open source framework that allows AI developers to combine Large Language Models (LLMs) with external data. \n",
    "\n",
    "**Resources**\n",
    "\n",
    "> LangChain resources\n",
    "> - Landpage: https://readthedocs.org/projects/langchain/db2d\n",
    "> - Comonents: https://docs.langchain.com/docs/category/components\n",
    "> - git: https://github.com/hwchase17/langchain.git\n",
    "> - API Reference: https://api.python.langchain.com/en/latest/\n",
    "\n",
    "> LangChain applications\n",
    "> - [LangChain Awesome](https://github.com/kyrolabs/awesome-langchain)\n",
    "\n",
    "> This notebook is largely based on Greg Kamradt's videos and cookbooks\n",
    "> - [Langchain tuorial suite](https://www.youtube.com/playlist?list=PLqZXAkvF1bPNQER9mLmDbntNfSpzdDIU5)\n",
    "> - [LangChain cookbooks](https://github.com/gkamradt/langchain-tutorials)\n",
    "\n",
    "> Additonal resources and tutorial\n",
    "> - [Cookbook Comprehensive Guide](https://nathankjer.com/introduction-to-langchain/)\n",
    "> - [A Gentle Intro to Chaining LLMs, Agents, and utils via LangChain](https://towardsdatascience.com/a-gentle-intro-to-chaining-llms-agents-and-utils-via-langchain-16cd385fca81)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ab8418-6cca-4c79-9f4c-95604c87214c",
   "metadata": {},
   "source": [
    "## This notebook\n",
    "\n",
    "This notebook collects Python examples. The chapters are based oo the LangChain compoents documented here https://docs.langchain.com/docs/category/components.\n",
    "\n",
    "Some changes though:\n",
    "- use Annoy instead of FAISS as a vector database\n",
    "- use Google Search API instead of SerpAPI\n",
    "- change in examples and additional examples \n",
    "- change in API keys setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff9bffb-d536-4f22-b5dc-2153c49d8340",
   "metadata": {},
   "source": [
    "This notebook has been tested in June 2023 on AWS SageMaker using DataScience 3.0 image.\n",
    "\n",
    "Test environment:\n",
    "> - AWS SageMaker Studio's notebook \n",
    ">> - Kernel image Data Science 3.0\n",
    ">> - t3.medium 2CPU - 4GB\n",
    ">> - Python 3.9.15\n",
    ">> - Linux default 4.14.304-226.531.amzn2.x86_64\n",
    "> - installed packages:\n",
    ">> - langchain 0.0.218\n",
    ">> - openai 0.27.8\n",
    ">> - google_api_python_client 2.90.0\n",
    ">> - tikitoken 0.4.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562017b1-fd67-475a-bc77-2019581bf2d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "<div style=\"background-color:green;color:black;text-align:center;padding:1rem;font-size:1.5rem;\">NOTEBOOK SETUP</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf59e64-4e2f-4277-a3af-485c2bf7a8e8",
   "metadata": {},
   "source": [
    "**Instructions**\n",
    "\n",
    "All setups are at the top of the notebook so that you can run all this section initialize the notebook.\n",
    "\n",
    "Notebook chapters are not dependant on each other and may be run in isolation.\n",
    "\n",
    "Before running the setup you may need to create the following resources\n",
    "- request an OpenAI API keys. OpenAI APIs are not free.\n",
    "- create a Custom Search Engine in Google Search. it is free.\n",
    "- request an API key for the Google Search service. It is free.\n",
    "\n",
    "Confer to the setup sections for instruction on how to create those resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614b5320-9d11-4e53-9f04-4c3dba3b769d",
   "metadata": {},
   "source": [
    "---\n",
    "## API keys and environment\n",
    "\n",
    "Langchain will get the API keys from environment variables or function parameters.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Never show the keys in shared notebooks, whether it part of the code or a log. A simple way to avoid key leakage, is to use environement variables.  You set the environment variable in the terminal or some local configuration. If so you do not have to set the key here.\n",
    "\n",
    "- If it is easier for you to set the key here by assigning the value, do not forget to empty the string right after you run this block. The environment will be kept in memory as long as the kernel runs.\n",
    "\n",
    "- Be careful when printing the keys. Ensure that you remove the outputs. \n",
    "\n",
    "- Before sharing check that the keys are not printed out by some features of the libraries. Avoid to print libraries' objects. They often hold the API keys as a property and may disclose the key value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4397c800-3ee5-40aa-b0a7-5021985e334a",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "I Store API keys and configuration information in AWS Secrets Manager. The code below retrieves the secret holding the keys. The secret is a JSON string consisting in key/value pairs. It will be used later to set various environnement variables.\n",
    "\n",
    "When using Notebooks an SageMaker do not forget to give permissions to read this secret to SageMaker execution role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6671d484-9c5d-4818-8449-6df0189fd307",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash --out secrets \n",
    "# using AWS's Secret Manager to store keys\n",
    "# garb the keys and store it into a Pytthon variable\n",
    "export RESPONSE=$(aws secretsmanager get-secret-value --secret-id 'salvia/labbench/tests' )\n",
    "export SECRETS=$( echo $RESPONSE | jq '.SecretString | fromjson')\n",
    "\n",
    "echo $SECRETS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7188bf6d-17d8-41c0-8c24-6db9ff6e07bb",
   "metadata": {},
   "source": [
    "---\n",
    "## LangChain Setup\n",
    "\n",
    "**Resources**\n",
    "> - [LangChain GetStarted](https://python.langchain.com/docs/get_started/quickstart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8713a09-0a7a-416c-807a-e2560796f7d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install langchain==0.0.218\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51de5b76-5e76-4c8f-bd11-9445f98c6941",
   "metadata": {},
   "source": [
    "---\n",
    "## OpenAI Setup\n",
    "\n",
    "**Resources**\n",
    "> - [OpenAI tutorial on API keys](https://platform.openai.com/docs/quickstart)\n",
    "> - [OpenAI package on Pypi](https://pypi.org/project/openai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be008522-bd17-47d4-bd6d-58da90214050",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = eval(secrets)[\"OPENAI_API_KEY\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc38381-5ee0-4107-9b3a-40dd9cb9302e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install openai==0.27.8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31520149-8e0f-4b08-b4a5-08c118fb638f",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Google Search setup\n",
    "\n",
    "**Resources**\n",
    "\n",
    "> How to configure the Google search in LangChain \n",
    "> - https://python.langchain.com/docs/ecosystem/integrations/google_search\n",
    "\n",
    "> Custom Search Engine configuration \n",
    "> - https://stackoverflow.com/questions/37083058/programmatically-searching-google-in-python-using-custom-search\n",
    "\n",
    "> CSE API \n",
    "> - repo: https://github.com/google/google-api-python-client\n",
    "> - more info: https://developers.google.com/api-client-library/python/apis/customsearch/v1\n",
    "> - complete docs: https://api-python-client-doc.appspot.com/\n",
    "\n",
    "> Get an API key\n",
    "> - https://developers.google.com/custom-search/v1/introduction\n",
    "\n",
    "> Package information\n",
    "> - [Google API client package on Pypi](https://pypi.org/project/google-api-python-client/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c65606-95d2-4b96-813f-db44b9ffcbb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Unlock the API and get a key \n",
    "os.environ[\"GOOGLE_API_KEY\"] = eval(secrets)[\"GOOGLE_API_KEY\"]\n",
    "# Create or use an existing Custom Search Engine\n",
    "# on the CSE page under Searcg Engone ID\n",
    "os.environ[\"GOOGLE_CSE_ID\"] = eval(secrets)[\"GOOGLE_CSE_ID\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e169ba1-9d2d-4bfa-843c-afe61e1228e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install google-api-python-client==2.90.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eac58b-33e7-49cb-981a-a8c2a3dd573b",
   "metadata": {},
   "source": [
    "## Setup Annoy as a vector database \n",
    "\n",
    "Some examples requires a Vector Database (document selector, document retrieval).\n",
    "\n",
    "LangChain use ChromaDB by default. For whatever reason it failed to install. Used Annoy instead. An alterntive is FAIIS. You may also want to use online Vector database like Pinecone or Weaviate. \n",
    "\n",
    "Most of these packages include c++ code and requires GCC at the install time. It is not included in SageMaker DataScience 3 image. So the first step is installing GCC. \n",
    "\n",
    "NOTE: Annoy is read-only - once the index is built you cannot add any more emebddings.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Resources**\n",
    "> - [Annoy package on Pypi](https://pypi.org/project/annoy/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d6d859-637f-4187-8270-8063f2c4abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update && apt-get install -y build-essential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5d265a-34ef-4c53-940b-2b762c09859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install annoy==1.17.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465dcd67-f5de-4e37-a0c8-f61e8b3c6356",
   "metadata": {},
   "source": [
    "# Setup additionalm API tools\n",
    "<div class=\"alert alert-block alert-warning\"> \n",
    "    TODO <br>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abcaa39-8611-4564-a26a-fd39f2e5300d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04436cf8-3983-487f-9688-a929ba1ab86b",
   "metadata": {},
   "source": [
    "## Setup additional tools for embeddings\n",
    "\n",
    "When working with embeddings additonal packages are required.\n",
    "\n",
    "- tiktoken, as a encoder and tokenizer\n",
    "\n",
    "**Resources**\n",
    "> - [Tiktoken package on Pypi](https://pypi.org/project/tiktoken/)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189b27d4-8afc-481d-b5bb-610a28171566",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tiktoken==0.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6986800-5d16-496f-aaa2-3e48587a492f",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"background-color:green;color:black;text-align:center;padding:1rem;font-size:1.5rem;\">LANGCHAIN OVERVIEW</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6bd9f8-efef-4797-ba5d-9eb135e6b862",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Basic features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da068c2f-26b2-4c5e-b72c-23a1d50c8543",
   "metadata": {},
   "source": [
    "---\n",
    "## Get prediction from a langage model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e8dd21-8ada-4cd4-a532-43c2e1c3fc63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "# OPENAI_API_KEY is requested. Get it from the OpenAI site.\n",
    "# a paid account and available units are requested to be able to place a request.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "text = \"what are the 5 best countries in Europe\"\n",
    "\n",
    "# Actual API call - may tale a while.\n",
    "print(llm(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17250fa0-fe5d-47c1-9406-9822dd38fc9e",
   "metadata": {},
   "source": [
    "---\n",
    "## Manage prompts with templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdf0d54-abaa-4aa6-aeb5-275bebb5246b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# setup a prompt\n",
    "prompt = PromptTemplate (\n",
    "    input_variables=[\"interest\"],\n",
    "    template=\"what are the 5 best countries in Europe ranked by {interest}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09393d7-6674-42f3-97c0-08cd12439d3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = prompt.format(interest=\"food\")\n",
    "print(f\"{text=}\")\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e3e1e6-871b-42e3-9106-96249b3f726e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = prompt.format(interest=\"siteseeing\")\n",
    "print(f\"{text=}\")\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db89d7fa-10d1-495d-8e89-24ab1dce738f",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ae61a5-823a-4585-b308-8e83ea8b6a7a",
   "metadata": {},
   "source": [
    "Chains are sequences of modular components (or other chains) combined in a particular way to accomplish a common use case.\n",
    "\n",
    "\n",
    "Example:\n",
    "- chaining LLM and tool\n",
    "- summarization chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d607ac-9d7f-40f3-86f4-243670f42fe2",
   "metadata": {},
   "source": [
    "---\n",
    "## Built-in chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d68f8d0-4ee4-48c5-a6b6-5f515c581eea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import PALChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "palchain = PALChain.from_math_prompt(llm=llm, verbose=True)\n",
    "\n",
    "\n",
    "text = \"\"\"If my age is half of my dad's age \n",
    "and he is going to be 60 next year, \n",
    "what is my current age?\"\"\"\n",
    "#palchain.run(\"If my age is half of my dad's age and he is going to be 60 next year, what is my current age?\")\n",
    "palchain.run(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ed407-348c-4c00-a405-c8233fe07bb4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "    TODO <br>\n",
    "    - different result each run <br>\n",
    "    - and should be 29.5\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8337725d-b79a-41b9-99f8-d7d55339c149",
   "metadata": {},
   "source": [
    "> Entering new  chain...\n",
    "def solution():\n",
    "    \"\"\"If my age is half of my dad's age and he is going to be 60 next year, what is my current age?\"\"\"\n",
    "    dad_age_next_year = 60\n",
    "    my_age_fraction = 0.5\n",
    "    my_age_now = dad_age_next_year * my_age_fraction\n",
    "    result = my_age_now\n",
    "    return result\n",
    "\n",
    "> Finished chain.\n",
    "'30.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8acdd9d-e9d5-4ca1-b3db-6e8b2a47d81b",
   "metadata": {},
   "source": [
    "> Entering new  chain...\n",
    "def solution():\n",
    "    \"\"\"If my age is half of my dad's age and he is going to be 60 next year, what is my current age?\"\"\"\n",
    "    dad_age_current = 59\n",
    "    my_age_current = dad_age_current / 2\n",
    "    result = my_age_current\n",
    "    return result\n",
    "\n",
    "> Finished chain.\n",
    "'29.5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b09463-2f2e-4840-9602-1a22478bdd5c",
   "metadata": {},
   "source": [
    "---\n",
    "## Multi-step workflow to feed prompt into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe1a4f0-4c08-47aa-9304-0b6a7002968b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# setup a prompt\n",
    "prompt = PromptTemplate (\n",
    "    input_variables=[\"interest\"],\n",
    "    template=\"what are the 5 best countries in Europe ranked on {interest}\"\n",
    ")\n",
    "\n",
    "# chain feeds the prompt into the langage mmodel.\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97622a8b-eb22-4218-803d-34d1a76c1af2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain.run(\"science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f73c33b-634d-48fb-9552-7329658c4e23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chain.run(\"tv shows\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c72f8b-7623-4e34-a2b2-c8bf785fa719",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Using OpenAI Chat API (less expensive)\n",
    "requires a chain to feed the prompt into the chat \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO  move to components + desribe resource </div>\n",
    "\n",
    "**Resources**\n",
    "> - Other Chat APIs: https://api.python.langchain.com/en/latest/modules/chat_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18f9ea2-c494-462a-aa3e-5e7ca5952d31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chatopenai = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "prompt = PromptTemplate (\n",
    "    input_variables=[\"interest\"],\n",
    "    template=\"what are the 5 best countries in Europe ranked on {interest}\"\n",
    ")\n",
    "\n",
    "llmchain_chat = LLMChain(llm=chatopenai, prompt=prompt)\n",
    "print(llmchain_chat.run(\"food\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3204392e-b1de-47f0-983e-ae05901179ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Leverage LLM Math\n",
    "\n",
    "Evaluating chains that know how to do math.\n",
    "\n",
    "**Resources**\n",
    "> - Langchain module LLM_Math: ttps://python.langchain.com/docs/guides/evaluation/llm_math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ed6528-f540-4b2f-9617-e5ae8ffe6d26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "from langchain.chains import LLMMathChain\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "prompt = load_prompt('lc://prompts/llm_math/prompt.json')\n",
    "\n",
    "# deprecated\n",
    "##chain = LLMMathChain(llm=llm, prompt=prompt)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "print(chain.run(\"what is the largest prime number lower than 20\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb52e51-3733-4ff5-a8cb-1094a391b98f",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Agent\n",
    "\n",
    "LangChain define agents as decision making engines:\n",
    "> Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an unknown chain that depends on the user's input. In these types of chains, there is a “agent” which has access to a suite of tools. Depending on the user input, the agent can then decide which, if any, of these tools to call.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61edc5bd-e7fa-49ab-85b1-cf08351ea471",
   "metadata": {},
   "source": [
    "---\n",
    "## Test with LLM model only \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb08bb0-a363-40d2-b119-0a2bd0805272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "# OPENAI_API_KEY is requested. Get it from the OpenAI site.\n",
    "# a paid account and available units are requested to be able to place a request.\n",
    "# low temperature to avoid randomness\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "text = \"Who is the prime minister of France since may 2022\"\n",
    "\n",
    "# Actual API call - may tale a while.\n",
    "print(llm(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af68d10d-eaf3-4f27-b102-7b28e62a9b20",
   "metadata": {},
   "source": [
    "**OUTPUT**\n",
    "\n",
    "'The Prime Minister of France since May 2022 is Jean Castex.'\n",
    "\n",
    "This answer is wrong. Since the model has been trained mid 2021, it is not up-to-date. Elisabeth Borne is Prime Minister since may 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c68946-559e-497d-a971-45c473ea5770",
   "metadata": {},
   "source": [
    "---\n",
    "## Agent leveraging Google Search\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "Make sure:\n",
    "- Google API client is installed\n",
    "- a Custome Search Engine is available (CSE)\n",
    "- the API key has been setup up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d462acdb-fff1-449f-98f6-4b93916dbaff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64311c2-8e52-4278-a4ac-42a22bceec08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# create a model\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# load some tools\n",
    "tools = load_tools([\"google-search\"], llm=llm)\n",
    "\n",
    "# setup an agent\n",
    "agent = initialize_agent(tools, \n",
    "                         llm, \n",
    "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "                         verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff155cf5-99f9-4ba4-8dbc-9d9366825475",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent.run(\"Who is the prime minister of France since may 2022\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb7aeec-170c-4491-8344-1191a1102a80",
   "metadata": {},
   "source": [
    "**OUTPUT**\n",
    "\n",
    "'Élisabeth Borne is the prime minister of France since May 16, 2022.'\n",
    "\n",
    "This is true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28631b0b-d1ec-4d14-bf44-091f8abd1584",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Memory - Conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bde2f8-060f-4da3-a69f-24df8df6aa63",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO  what is a conversation </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13541ec0-684d-48da-a205-ef1c6e3e7937",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import OpenAI, ConversationChain\n",
    "\n",
    "# create a model\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "conversation.predict(input=\"Hi There\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7271c38f-0a3e-44fb-9d8a-1014667c3467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What is the first thing that I said to you?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2613f8b-beb3-47b3-b842-e5f2b4746143",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What is an alternative for the first thing that I said to you?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3912d8-050d-4c4a-89e8-1d9ce0a1f4cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "<div style=\"background-color:green;color:black;text-align:center;padding:1rem;font-size:1.5rem;\">LANGCHAIN COMPONENTS</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684914b4-7b16-4f0b-bde2-6ef6c7df5079",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Schemas\n",
    "\n",
    "Basic data types and schemas that are used throughout the codebase.\n",
    "\n",
    "There are 3 types of schemas\n",
    "- Text (see above)\n",
    "- Prompts\n",
    "- Messages \n",
    "- Document\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Resources**\n",
    "> - Schhemas component:  https://docs.langchain.com/docs/components/schema/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7bb2c3-a6cd-4fee-b6a8-964a5f811433",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8843ce9-1f71-4b43-a704-d456dc5653e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "# OPENAI_API_KEY is requested. Get it from the OpenAI site.\n",
    "# a paid account and available units are requested to be able to place a request.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "text = \"what are the 5 best countries in Europe\"\n",
    "\n",
    "# Actual API call - may tale a while.\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf610897-44b3-4d72-8e38-3dd50b0776bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Chat messages\n",
    "Chat messages are like text with a type\n",
    "\n",
    "There are 3 types\n",
    "- System: background context that tells the AI what to do\n",
    "- Human: inputs sent by the user\n",
    "- AI : response of the AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8d5a96-f105-45f2-ba15-0f0b108cd9ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7a0269-e45c-445b-b44f-b19fc4be5e53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [ SystemMessage(content=\"You are a nice AI and help users to feature out what to eat.\")]\n",
    "     \n",
    "messages.append( HumanMessage(content=\"I like tuna, list some recipes.\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e203228-09cd-4405-9679-c4eebfbd7c1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = chat(messages)\n",
    "messages.append( AIMessage(content=response.content) )\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d140f70f-a86d-4634-899e-12401f0315cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages.append( HumanMessage(content=\"show the first one.\") )\n",
    "\n",
    "response = chat(messages)\n",
    "messages.append( AIMessage(content=response.content) )\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af92d18-1d2b-4c55-aa71-e5a91fde59bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Examples\n",
    "An list of input output pairs thet represent the input and expected output.\n",
    "\n",
    "Used to fine tune a model or do in-context learning.\n",
    "\n",
    "**Resources**\n",
    "> - Prompt Template:  https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/few_shot_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca97e03b-8d6c-4156-8358-a9504901d733",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# create the example set\n",
    "\n",
    "examples = [\n",
    "    { \"question\": \"red bold\", \"answer\": \"color:red; font-style:bold;\"},\n",
    "    { \"question\": \"green italic\", \"answer\":  \"color:green; font-style:italic;\"},\n",
    "    { \"question\": \"blue bold\", \"answer\":  \"color:blue; font-style:bold;\"},\n",
    "    { \"question\": \"pink\", \"answer\":  \"color:pink;\"},\n",
    "    { \"question\": \"green\", \"answer\":  \"color:green;\"},\n",
    "    { \"question\": \"pink italic\", \"answer\":  \"color:pink; font-style:italic;\"}\n",
    "    \n",
    "]    \n",
    "\n",
    "# Configure a formatter that will format the few shot examples into a string. \n",
    "# This formatter should be a PromptTemplate object.\n",
    "\n",
    "example_prompt = PromptTemplate (\n",
    "    input_variables=[\"question\", \"answer\"], \n",
    "    template=\"question: {question}\\n{answer}\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== exemple prompt ===\")\n",
    "print(example_prompt.format(**examples[0]))\n",
    "\n",
    "\n",
    "# Finally, create a FewShotPromptTemplate object. \n",
    "# This object takes in the few shot examples and the formatter for the few shot examples.\n",
    "\n",
    "prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples, \n",
    "    example_prompt=example_prompt, \n",
    "    suffix=\"question: {input}\", \n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format(input=\"pink bold\")\n",
    "\n",
    "print(\"\\n=== prompt ===\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"\\n=== answer ===\")\n",
    "print(llm(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2471f9d0-6000-4f48-ad1a-f5e187a8aa54",
   "metadata": {},
   "source": [
    "---\n",
    "## Documents\n",
    "\n",
    "An unstructured object that conaints a pieces of text and metadatas.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO  resource </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648ce2f7-0d40-49ea-b0a8-6e45f9b5f2f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO how to use this concept? \n",
    "make some knowledge available?\n",
    "how to use metadata?\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097d47df-18e7-4a58-b76c-d7438d3135bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# temperature 0 means no randomness\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "\n",
    "document = Document(\n",
    "    page_content=\"\"\"\n",
    "\n",
    "        So she swallowed one of the cakes and was delighted to find that she\n",
    "        began shrinking directly. As soon as she was small enough to get through\n",
    "        the door, she ran out of the house and found quite a crowd of little\n",
    "        animals and birds waiting outside. They all made a rush at Alice the\n",
    "        moment she appeared, but she ran off as hard as she could and soon found\n",
    "        herself safe in a thick wood.\n",
    "        \"\"\",\n",
    "    metadata={\n",
    "        'author':\"Lewis Caroll\",\n",
    "        'identifier':\"1234\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Document\")\n",
    "print(document)\n",
    "\n",
    "# the attribute stuff instruct the run the chain once\n",
    "chain = load_summarize_chain(\n",
    "    llm, \n",
    "    chain_type=\"stuff\", \n",
    "    verbose=False)\n",
    "\n",
    "# run the chain against the documment\n",
    "summary = chain.run([document])\n",
    "    \n",
    "print(\"\\nSummary\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53092123-3262-48de-b082-e4bedd51432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Note, the default model is already 'text-davinci-003' \n",
    "# temperature 0 means no randomness\n",
    "llm = OpenAI(temperature=0, model_name='text-davinci-003')\n",
    "\n",
    "# check the number of tokens\n",
    "num_tokens = llm.get_num_tokens(text_sample)\n",
    "print(f\"{num_tokens=}\")\n",
    "\n",
    "# build a document reuse text sampke above\n",
    "doc = Document(\n",
    "    page_content=text_sample,\n",
    "    metadata={\n",
    "        'author':\"Lewis Caroll\",\n",
    "        'title':\"Alice in Wonderland\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# chain expect a list of documents\n",
    "docs = [doc]\n",
    "\n",
    "# setup. a custom prompt\n",
    "# a defaukt one is provide: write a concise summary\n",
    "prompt_template = \"\"\"\n",
    "%INSTRUCTIONS:\n",
    "Please summarize the following piece of text.\n",
    "Respond in a manner that a 5 year old would understand.\n",
    "\n",
    "%TEXT:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "# the attribute stuff instruct the run the chain once\n",
    "chain = load_summarize_chain(\n",
    "    llm, \n",
    "    chain_type=\"stuff\", \n",
    "    prompt=prompt, \n",
    "    verbose=False)\n",
    "\n",
    "# run the chain against the documment\n",
    "summary = chain.run(docs)\n",
    "    \n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee98eca-37a3-45c4-90e5-eb2360e1f5e1",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Models\n",
    "LangChain provides interfaces and integrations for two types of models:\n",
    "- LLMs: Models that take a text string as input and return a text string\n",
    "- Chat models: Models that are backed by a language model but take a list of Chat Messages as input and return a Chat Message\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Resources**\n",
    "> - Model Component: https://python.langchain.com/docs/modules/model_io/models/\n",
    "> - List of models: https://platform.openai.com/docs/models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6a1df5-7967-4841-b1eb-1ff58d0a15cb",
   "metadata": {},
   "source": [
    "---\n",
    "## Langage Model \n",
    "LLMs: Models that take a text string as input and return a text string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd6cc95-2731-47d9-8624-2392e3a83315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# additnal parameters to select a mode, pass the API key ...\n",
    "llm = OpenAI(model_name=\"text-ada-001\", temperature=0.7)\n",
    "\n",
    "llm(\"What day comes after Friday?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981bf185-6a93-4eb5-bdc4-779724aab952",
   "metadata": {},
   "source": [
    "---\n",
    "## Chat Model \n",
    "Chat models: Models that are backed by a language model but take a list of Chat Messages as input and return a Chat \n",
    "\n",
    "Also make sense for a unique interaction as Chat API is less expensive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2099b96-d00a-4aa0-908d-e75a0e4b3a16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daafb170-e231-4132-bcc2-c88aac2fe475",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [ \n",
    "    SystemMessage(content=\"You are a nice AI and help users to feature out what to eat.\"),\n",
    "    HumanMessage(content=\"I like tuna, list some recipes.\")\n",
    "]\n",
    "     \n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91de6b77-6cb7-414b-a475-c471c3ca6c5b",
   "metadata": {},
   "source": [
    "---\n",
    "### Text Embedding Model\n",
    "\n",
    "Convert text into a series of numbers (a vector) which holds the meaning of the text.\n",
    "\n",
    "Mainly used for text comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7d5813-a505-47d6-a8ef-98881a175c46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "text=\"A leader should know all about truth and honesty, and when to see the difference. (Truck) - Bromeliad Trilogy\"\n",
    "\n",
    "text_embedding = embeddings.embed_query(text)\n",
    "\n",
    "print(f\"embedding length: {len(text_embedding)}\")\n",
    "print(f\"5 first values of the vector: {text_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab0bbcc-adc2-449f-97e4-839a36b3d4f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# 7. prompts\n",
    "A \"prompt\" refers to the input to the model. This input is rarely hard coded, but rather is often constructed from multiple components. A PromptTemplate is responsible for the construction of this input. LangChain provides several classes and functions to make constructing and working with prompts easy.\n",
    "\n",
    "LangChain documentation is split into four sections:\n",
    "- PromptValue: The class representing an input to a model.\n",
    "- Prompt Templates: The class in charge of constructing a PromptValue.\n",
    "- Example Selectors: Often times it is useful to include examples in prompts. These examples can be hardcoded, but it is often more powerful if they are dynamically selected.\n",
    "- Output Parsers: Language models (and Chat Models) output text. But many times you may want to get more structured information than just text back. This is where output parsers come in. Output Parsers are responsible for (1) instructing the model how output should be formatted, (2) parsing output into the desired formatting (including retrying if necessary).\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Resources**\n",
    "> - Prompts Component: https://docs.langchain.com/docs/components/prompts/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc36ebe4-aba6-4cdb-850d-1937839c8608",
   "metadata": {},
   "source": [
    "---\n",
    "## Simple prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37b8bcc-33ca-4deb-9845-81675d529758",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0.9)\n",
    "\n",
    "# write a simple  prompt. use \"\"\" to allow multiline string.\n",
    "prompt = \"\"\"\n",
    "Today is Monday. Tomorrow is Wednesday.\n",
    "\n",
    "What is wrong with this statement?\n",
    "\"\"\"\n",
    "\n",
    "# query the model\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f8a344-70ab-471d-918e-ee98e7e4fdf1",
   "metadata": {},
   "source": [
    "---\n",
    "## Prompt with template and placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837116ab-bf8b-49ed-bb3f-a896e14fb14d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0.9)\n",
    "\n",
    "# setup a prompt. use \"\"\" to allow multiline string.\n",
    "template = PromptTemplate (\n",
    "    input_variables=[\"today\", \"tomorrow\"],\n",
    "    template=\"\"\"\n",
    "    Today is {today}. Tomorrow is {tomorrow}.\n",
    "\n",
    "    What is wrong with this statement?\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "prompt = template.format(today=\"Monday\", tomorrow=\"Wednesday\")\n",
    "print(f\"{prompt=}\")\n",
    "\n",
    "# query the model\n",
    "\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa35aff7-3105-4aed-82ab-7281c517eca3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = template.format(today=\"Thursday\", tomorrow=\"Friday\")\n",
    "print(f\"{prompt=}\")\n",
    "\n",
    "# query the model\n",
    "\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc54ed33-925e-405f-8eaf-a4a57ed78771",
   "metadata": {},
   "source": [
    "---\n",
    "## Example selectors and Few Shot Learning\n",
    "\n",
    "A way to select from a series of examples in few shot learning \n",
    "\n",
    "**Resources**\n",
    "> - Example Selector: https://api.python.langchain.com/en/latest/modules/example_selector.html\n",
    "> - Few shot learning: https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/few_shot_examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e40343-df4e-4b65-8951-8372060ca6d0",
   "metadata": {},
   "source": [
    "### Example selectors and Few Shot Learning with NGram\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> FIXME </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39afd4e6-4d1d-4348-b871-343e34cb0314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.example_selector import NGramOverlapExampleSelector\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# create the example set\n",
    "\n",
    "examples = [\n",
    "    { \"question\": \"red bold\", \"answer\": \"color:red; font-style:bold;\"},\n",
    "    { \"question\": \"green italic\", \"answer\":  \"color:green; font-style:italic;\"},\n",
    "    { \"question\": \"blue bold\", \"answer\":  \"color:blue; font-style:bold;\"},\n",
    "    { \"question\": \"pink\", \"answer\":  \"color:pink;\"},\n",
    "    { \"question\": \"green\", \"answer\":  \"color:green;\"},\n",
    "    { \"question\": \"pink italic\", \"answer\":  \"color:pink; font-style:italic;\"}\n",
    "    \n",
    "]    \n",
    "\n",
    "# Configure a formatter that will format the few shot examples into a string. \n",
    "# This formatter should be a PromptTemplate object.\n",
    "\n",
    "example_prompt = PromptTemplate (\n",
    "    input_variables=[\"question\", \"answer\"], \n",
    "    template=\"question: {question}\\n{answer}\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== exemple prompt ===\")\n",
    "print(example_prompt.format(**examples[0]))\n",
    "\n",
    "\n",
    "# Select and order examples based on ngram overlap score (sentence_bleu score).\n",
    "\n",
    "question = \"pink bold\"\n",
    "\n",
    "example_selector = NGramOverlapExampleSelector.select_examples(\n",
    "    examples,\n",
    "    question\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # This is the list of examples available to select from.\n",
    "    examples,\n",
    "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    OpenAIEmbeddings(),\n",
    "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    Chroma,\n",
    "    # This is the number of examples to produce.\n",
    "    k=1\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Finally, create a FewShotPromptTemplate object. \n",
    "# This object takes in the few shot examples and the formatter for the few shot examples.\n",
    "\n",
    "prompt_template = FewShotPromptTemplate(\n",
    "    #example_selector=example_selector, \n",
    "    examples=selected_examples, \n",
    "    example_prompt=example_prompt, \n",
    "    suffix=\"question: {input}\", \n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format(input=question)\n",
    "\n",
    "print(\"\\n=== prompt ===\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"\\n=== answer ===\")\n",
    "print(llm(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be5012c-2708-417f-a6e8-4a1beb5349df",
   "metadata": {},
   "source": [
    "### Example selectors and Few Shot Learning with similarities\n",
    "\n",
    "requires a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6e5756-6345-4b70-844b-f7959fe0d70e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Annoy\n",
    "#from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# create the example set\n",
    "\n",
    "examples = [\n",
    "    { \"question\": \"red bold\", \"answer\": \"color:red; font-style:bold;\"},\n",
    "    { \"question\": \"green italic\", \"answer\":  \"color:green; font-style:italic;\"},\n",
    "    { \"question\": \"blue bold\", \"answer\":  \"color:blue; font-style:bold;\"},\n",
    "    { \"question\": \"pink\", \"answer\":  \"color:pink;\"},\n",
    "    { \"question\": \"green\", \"answer\":  \"color:green;\"},\n",
    "    { \"question\": \"pink italic\", \"answer\":  \"color:pink; font-style:italic;\"}\n",
    "    \n",
    "]    \n",
    "\n",
    "# Configure a formatter that will format the few shot examples into a string. \n",
    "# This formatter should be a PromptTemplate object.\n",
    "\n",
    "example_prompt = PromptTemplate (\n",
    "    input_variables=[\"question\", \"answer\"], \n",
    "    template=\"question: {question}\\n{answer}\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== exemple prompt ===\")\n",
    "print(example_prompt.format(**examples[0]))\n",
    "\n",
    "# Example selector that selects examples based on SemanticSimilarity.\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # This is the list of examples available to select from.\n",
    "    examples,\n",
    "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    OpenAIEmbeddings(),\n",
    "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    #Chroma,\n",
    "    Annoy,\n",
    "    # This is the number of examples to produce.\n",
    "    k=2\n",
    ")\n",
    "\n",
    "# Finally, create a FewShotPromptTemplate object. \n",
    "# This object takes in the few shot examples and the formatter for the few shot examples.\n",
    "\n",
    "prompt_template = FewShotPromptTemplate(\n",
    "    example_selector=example_selector, \n",
    "    example_prompt=example_prompt, \n",
    "    suffix=\"question: {input}\", \n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format(input=\"pink bold\")\n",
    "\n",
    "print(\"\\n=== prompt ===\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"\\n=== answer ===\")\n",
    "print(llm(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2b5aae-d6c0-4ebe-a12e-521b2f807045",
   "metadata": {},
   "source": [
    "---\n",
    "## Output Parser and response format\n",
    "\n",
    "A way to format the outpu\n",
    "- Format nstructions: An autogenerated prompt telling how the result should be formatted\n",
    "- parser: a method which will extract the output int hte desired format. you may prvie a custom parser\n",
    "\n",
    "\n",
    "**Resources**\n",
    "> - OutputParser:https://docs.langchain.com/docs/components/prompts/output-parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "46e0cc93-9499-4ac7-a3b2-669bb3f121e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "format_instructions\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This is a poorly formatted string.\n",
      "\t\"good_string\": string  // This is a your string reformatted.\n",
      "}\n",
      "```\n",
      "\n",
      "prompt\n",
      "\n",
      "You will be given a poorly formatted string from a user. \n",
      "Reformat it and make sure all the words are spelled correctly.\n",
      "\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This is a poorly formatted string.\n",
      "\t\"good_string\": string  // This is a your string reformatted.\n",
      "}\n",
      "```\n",
      "\n",
      "% USER_INPUT:\n",
      "Wellcom to Californya!\n",
      "\n",
      "YOUR RESPONSE:\n",
      "\n",
      "\n",
      "response=\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": \"Wellcom to Californya!\",\n",
      "\t\"good_string\": \"Welcome to California!\"\n",
      "}\n",
      "```\n",
      "\n",
      "parsed output=\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bad_string': 'Wellcom to Californya!',\n",
       " 'good_string': 'Welcome to California!'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "\n",
    "# loads the model.\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0.9)\n",
    "\n",
    "# how you would like the response to be structured\n",
    "# periods at the send of sentence are required. \n",
    "# If not there description ends up in the json text and break the JSON format\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"bad_string\", description=\"This is a poorly formatted string.\"),\n",
    "    ResponseSchema(name=\"good_string\", description=\"This is a your string reformatted.\")\n",
    "]\n",
    "\n",
    "# How you would like to parse your output\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# check instructions\n",
    "format_instructions =output_parser.get_format_instructions()\n",
    "print(\"\\nformat_instructions\")      \n",
    "print(format_instructions)      \n",
    "\n",
    "template = \"\"\"\n",
    "You will be given a poorly formatted string from a user. \n",
    "Reformat it and make sure all the words are spelled correctly.\n",
    "\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER_INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=['user_input'],\n",
    "    partial_variables={'format_instructions': format_instructions},\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# format the user input as a prompt\n",
    "# for whateveer reason it does not work well with format.\n",
    "# format_promt retruns an object, not a string and should be converted to a string \n",
    "prompt = prompt_template.format_prompt(user_input=\"Wellcom to Californya!\").to_string()\n",
    "print(\"\\nprompt\")\n",
    "print(prompt)\n",
    "\n",
    "# gets the response\n",
    "response = llm(prompt)\n",
    "print(\"\\nresponse=\")      \n",
    "print(response)      \n",
    "\n",
    "# gets the JSON document\n",
    "print(\"\\nparsed output=\")     \n",
    "\n",
    "# comma sometimes missing\n",
    "response.replace('\"good_string\"',',\"good_string\"')\n",
    "\n",
    "output_parser.parse(response)                   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3bc48a-c126-4f79-b8d8-5f65b3387ecd",
   "metadata": {},
   "source": [
    "---\n",
    "# 8. Indexes\n",
    "\n",
    "Indexes refer to ways to structure documents so that LLMs can best interact with them. This module contains utility functions for working with documents, different types of indexes, and then examples for using those indexes in chains.\n",
    "\n",
    "LangChain documentation is split into four sections:\n",
    "\n",
    "- Document Loaders: Classes responsible for loading documents from various sources.\n",
    "- Text Splitters: Classes responsible for splitting text into smaller chunks.\n",
    "- VectorStores: The most common type of index. One that relies on embeddings.\n",
    "- Retrievers: Interface for fetching relevant documents to combine with language models.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Resource**\n",
    "> - Indexes Component: https://docs.langchain.com/docs/components/indexing/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdab962-d2e2-40c9-bcc7-26a07996a493",
   "metadata": {},
   "source": [
    "**Instructions**\n",
    "\n",
    "For the example below, make sure that:\n",
    "- a vector database client is installed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7a68d6-2561-4630-89b0-f543d3880746",
   "metadata": {},
   "source": [
    "---\n",
    "## Document Loaders\n",
    "\n",
    "Easy ways to import documents from other sources \n",
    "and make it available for use in your language models.\n",
    "\n",
    "**Resources**\n",
    "> -  Document Loaders: https://python.langchain.com/docs/modules/data_connection/document_loaders\n",
    "> - List of loaders: https://github.com/hwchase17/langchain/tree/master/langchain/document_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77127064-7c75-4561-81a7-a3d73f08cb44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import HNLoader\n",
    " \n",
    "# Setup a Hacker News loader\n",
    "loader = HNLoader(\"https://news.ycombinator.com/item?id=34422627\")\n",
    " \n",
    "data = loader.load()\n",
    " \n",
    "print(f\"Found {len(data)} comments\")\n",
    "\n",
    "\n",
    "sample = '\\n'.join([x.page_content[:100] for x in data[:2]])\n",
    "print(\"\\nHere's a sample (first 100 chars of the 3 first items)\")\n",
    "print(sample)\n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd41d74-2bd1-4d5c-8409-b27c6cdb5d1a",
   "metadata": {},
   "source": [
    "---\n",
    "## Text Splitters\n",
    "\n",
    "allow you to split a document into smaller chunk\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO  resource </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32da233-903e-4ae2-8c5e-42cd0c6f3d54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import BSHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    " \n",
    "# This is a long document we can split up.    \n",
    "document_path = \"data/Alice's Adventures in Wonderland, by Lewis Carroll.html\"\n",
    " \n",
    "# Setup a HTML loader\n",
    "loader = BSHTMLLoader(document_path)\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Found {len(documents)} document(s)\")\n",
    "\n",
    "\n",
    "print(\"docuument content\")\n",
    "start = 2200\n",
    "print(documents[0].page_content[start-200:start+300])\n",
    "\n",
    " \n",
    "# The recommended TextSplitter is the RecursiveCharacterTextSplitter. \n",
    "# This will split documents recursively by different characters - starting with \"\\n\\n\", then \"\\n\", then \" \".\n",
    "# This is nice because it will try to keep all the semantically relevant content in the same place \n",
    "# for as long as possible.\n",
    "# Important parameters to know here are chunkSize and chunkOverlap. \n",
    "# chunkSize controls the max size (in terms of number of characters) of the final documents. \n",
    "# chunkOverlap specifies how much overlap there should be between chunks. \n",
    "# in practice they default to 4000 and 200 respectively.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    " \n",
    "texts = text_splitter.create_documents([document[0].page_content])\n",
    " \n",
    "print(f\"\\nSplitted into {len(texts)} parts\")\n",
    " \n",
    "print(\"Preview:\")\n",
    "i = int(start/150)\n",
    "print(texts[i+1].page_content, \"\\n-\")\n",
    "print(texts[i+2].page_content, \"\\n-\")\n",
    "print(texts[i+3].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a947a9d1-e893-4ac8-b854-e5a777363c27",
   "metadata": {},
   "source": [
    "---\n",
    "## Vextor Store and Retrievers \n",
    "A retriever is an interface that returns documents given an unstructured query. \n",
    "\n",
    "A retriever does not need to be able to store documents, only to return (or retrieve) it. \n",
    "\n",
    "It usually relies to a vector store as a document management backbone.\n",
    "\n",
    "A vector store is a particular type of database optimized for storing documents and their embeddings, and then fetching of the most relevant documents for a particular query, ie. those whose embeddings are most similar to the embedding of the query.\n",
    "\n",
    "- local : ChromaDB, FAISS, Annoy\n",
    "- Online: Pinecone, Weaviate\n",
    "\n",
    "However a retriever is more general than a vector store and there are other types of retrievers as well, e.g. Wikipedia or search engines like Elastic Search or Kendra.\n",
    "\n",
    "\n",
    "Question answering over documents consists of four steps:\n",
    "1. Create an index\n",
    "2. Create a Retriever from that index\n",
    "3. Create a question answering chain\n",
    "4. Ask questions\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Resources**\n",
    "> - Lit of retrievers: https://python.langchain.com/docs/modules/data_connection/retrievers/\n",
    "> - LangChain Supported VectorStores: https://api.python.langchain.com/en/latest/modules/vectorstores.html\n",
    "> - Retrievers: https://github.com/hwchase17/langchain/tree/master/langchain/retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3769182e-3197-4cc3-8ab4-d05717fb2922",
   "metadata": {},
   "source": [
    "### Store document in a Vector Store and retrieve information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedfae67-fba3-4e5d-ac09-d2d6b465dcb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import BSHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Annoy\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    " \n",
    "# This is the source document.    \n",
    "document_path = \"data/Alice's Adventures in Wonderland, by Lewis Carroll.html\"\n",
    " \n",
    "# Setup a HTML loader\n",
    "loader = BSHTMLLoader(document_path)\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Found {len(documents)} document(s)\")\n",
    "\n",
    "\n",
    "# Get your splitter ready\n",
    "# Using small chunk for the sake of example. \n",
    "# in practice they default to 4000 and 200 respectively.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=25)\n",
    " \n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"\\nSplitted into {len(texts)} parts\")\n",
    "\n",
    "# Get embedding engine ready\n",
    "embeddings = OpenAIEmbeddings()\n",
    " \n",
    "# Embedd your texts andd store them in the vector database\n",
    "# dtabase is in memory. it might be savecd to a file and loader later on.\n",
    "db = Annoy.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e39c41-6df4-445f-8314-db8d7c49624a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Init a retriever for this db\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# retrieve indexed documents relevant for the query\n",
    "query = \"who is the White Rabbit?\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "print(f\"\\nFound {len(docs)}\")\n",
    "\n",
    "samples = \"\\n\\n\".join([x.page_content[:200] for x in docs[:5]])\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448aa357-9d47-4db2-af62-cd4acd58bd78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Asking theLLM\n",
    "# the response will be based on the retrieved documents \n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97504cf-53c0-4a59-848d-459279b1499f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a99a8f-58db-4ef3-b75e-0f584222f7c1",
   "metadata": {},
   "source": [
    "### Save and load db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fb324b-98dc-449d-8906-ebd536635ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Annoy\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "docstore_file_path = \"alice_docstore\"\n",
    "\n",
    "db.save_local(docstore_file_path)\n",
    "\n",
    "loaded_vector_store = Annoy.load_local(\n",
    "   docstore_file_path, embeddings=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# same document similar to White Red abbit\n",
    "loaded_vector_store.similarity_search_with_score(\"White Rabbit\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ee55b6-6c70-48d0-862b-8a411fc63b57",
   "metadata": {},
   "source": [
    "### One line index creation and information retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c90307a-e8a0-4cb1-a514-38cd52f74685",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import BSHTMLLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Annoy\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "# This is the source document.    \n",
    "document_path = \"data/Alice's Adventures in Wonderland, by Lewis Carroll.html\"\n",
    " \n",
    "# Setup a HTML loader\n",
    "loader = BSHTMLLoader(document_path)\n",
    "\n",
    "# creating an indexer\n",
    "# default to Chroma as a vector database\n",
    "# Use CharacterTextSplitter. May also be RecursiveCharacterTextSplitter.\n",
    "index_creator = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=Annoy,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    text_splitter=CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    ")\n",
    "\n",
    "index = index_creator.from_loaders([loader])\n",
    "\n",
    "# retrieve indexed documents relevant for the query\n",
    "query = \"who is the White Rabbit?\"\n",
    "index.query(query)\n",
    "\n",
    "print(f\"\\nFound {len(docs)}\")\n",
    "\n",
    "samples = \"\\n\\n\".join([x.page_content[:200] for x in docs[:5]])\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27cfddd-ed2d-4892-a521-aa473ccd2c67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Ask the question to the model \n",
    "# the response will be based on the retrieved documents \n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), \n",
    "                                 chain_type=\"stuff\", \n",
    "                                 retriever=index.vectorstore.as_retriever())\n",
    "\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de6f5a1-9765-424c-819c-b79ccb8acc97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0081edf7-2762-4667-895f-5849b15424ea",
   "metadata": {},
   "source": [
    "---\n",
    "## Wikipedia retriever\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO wikipedia retriever </div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> \n",
    "    Move to tools agent_excutor example  <br>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272625be-5241-4278-9c8f-85a4d81cfed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "\n",
    "# model_name='gpt-4'\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "wikipedia = WikipediaAPIWrapper()\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Wikipedia\",\n",
    "        func=wikipedia.run,\n",
    "        description=\"Useful for when you need to get information from wikipedia about a single topic\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "agent_executor = initialize_agent(tools, llm, agent='zero-shot-react-description', verbose=True)\n",
    "\n",
    "output = agent_executor.run(\"Can you please provide a quick summary of Napoleon Bonaparte? \\\n",
    "                          Then do a separate search and tell me what the commonalities are with Serena Williams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a7a9c8-d0bc-4a5c-9f9d-fff2f870d0ed",
   "metadata": {},
   "source": [
    "---\n",
    "# 9. Memory\n",
    "\n",
    "\n",
    "Memory is the concept of storing and retrieving data in the process of a conversation. \n",
    "\n",
    "There are two main methods:\n",
    "- Based on input, fetch any relevant pieces of data\n",
    "- Based on the input and output, update state accordingly\n",
    "\n",
    "There are two main types of memory: short term and long term.\n",
    "- Short term memory generally refers to how to pass data in the context of a singular conversation (generally is previous ChatMessages or summaries of them).\n",
    "- Long term memory deals with how to fetch and update information between conversations.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Resource**\n",
    "> - Memory Component: https://docs.langchain.com/docs/components/memory/\n",
    "> - Chat Message History: https://docs.langchain.com/docs/components/memory/chat_message_history\n",
    "> - [LangChain: Enhancing Performance with Memory Capacity](https://towardsdatascience.com/langchain-enhancing-performance-with-memory-capacity-c7168e097f81)\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO vs Conversation and buffer memory (check blog)?</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO Long term memory</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ed82a7-7392-4637-85f2-a85b86d40378",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from pprint import pprint\n",
    " \n",
    "chat = ChatOpenAI(temperature=0)\n",
    " \n",
    "history = ChatMessageHistory()\n",
    " \n",
    "history.add_ai_message(\"hi!\")\n",
    " \n",
    "history.add_user_message(\"what is the capital of france?\")\n",
    "\n",
    "#After adding messages to the history, you can pass this history to the language model \n",
    "#to generate context-aware responses:\n",
    "\n",
    "ai_response = chat(history.messages)\n",
    "history.add_ai_message(ai_response.content)\n",
    "\n",
    "print(f\"{ai_response=}\")\n",
    "print(f\"\\nhistory.messages:\")\n",
    "pprint(history.messages, compact=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbd121c-3290-4133-a9c6-c55113fd139f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history.add_user_message(\"what is the population os this city?\")\n",
    "\n",
    "ai_response = chat(history.messages)\n",
    "history.add_ai_message(ai_response.content)\n",
    "\n",
    "print(f\"{ai_response.content=}\")\n",
    "print(f\"\\nhistory.messages:\")\n",
    "pprint(history.messages, compact=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc922de-52de-4c30-8faa-4feded2137d2",
   "metadata": {},
   "source": [
    "---\n",
    "# 10. Chains\n",
    "Chains are sequences of modular components (or other chains) combined in a particular way to accomplish a common use case.\n",
    "\n",
    "\n",
    "Example:\n",
    "- chaining LLM and tool\n",
    "- summarization chain\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Resources**\n",
    "> - Chain Component: https://docs.langchain.com/docs/components/chains/\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO index related chain https://docs.langchain.com/docs/components/chains/index_related_chains  </div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ede334-ad80-49fe-82aa-c9b22c12ac8d",
   "metadata": {},
   "source": [
    "## Simple sequential model\n",
    "\n",
    "A Simple Sequential Chain helps break up tasks to avoid language models getting distracted, confused, or hallucinating when asked to perform too many tasks in a row.\n",
    "\n",
    "In this example, the chain first receives the user location (Rome) and outputs a classic dish from Rome. Then, it provides a simple recipe for that classic dish. The verbose=True parameter ensures that the chain prints statements during its execution, making it easier to debug and understand the chain’s progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b15c453-5b4a-4857-a232-0ddbfe12067e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain\n",
    " \n",
    "# Cretae a model with high randomness\n",
    "llm = OpenAI(temperature=1)\n",
    " \n",
    "# Step 1 - dish for location\n",
    "\n",
    "template = \"\"\"\n",
    "Your job is to come up with a classic dish from the area that the users suggests. \n",
    "\n",
    "% USER LOCATION {user_location} \n",
    "\n",
    "YOUR RESPONSE: \n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_location\"], template=template)\n",
    "\n",
    "location_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    " \n",
    "\n",
    "# Step 2 - Recipe\n",
    "template = \"\"\"\n",
    "Given a meal, give a short and simple recipe on how to make that dish at home. \n",
    "\n",
    "% MEAL {user_meal} \n",
    "\n",
    "YOUR RESPONSE: \n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
    " \n",
    "meal_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# chain the steps\n",
    "# set verbose to True to check what happes\n",
    "overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=False)\n",
    " \n",
    "review = overall_chain.run(\"Rome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cec1cc-9ce8-4ddc-82c1-69d3a60c977b",
   "metadata": {},
   "source": [
    "## Summarization Chain\n",
    "\n",
    "The Summarization Chain breaks the text into smaller chunks and summarizing each chunk, creating a final summary based on the individual summaries.\n",
    "\n",
    "In this example, the chain first splits the essay into chunks of 700 characters. It then generates summaries for each chunk and creates a final concise summary based on these individual summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cfde91-71be-42ed-91d1-d3154f8e4ab3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import BSHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Cretae a model with low randomness\n",
    "llm = OpenAI(temperature=1)\n",
    "\n",
    "# This is the source document.    \n",
    "document_path = \"data/Alice's Adventures in Wonderland, by Lewis Carroll.html\"\n",
    " \n",
    "# Setup a HTML loader\n",
    "loader = BSHTMLLoader(document_path)\n",
    "documents = loader.load()\n",
    " \n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=50)\n",
    " \n",
    "# Split your docs into texts\n",
    "# only kept first 1 000 characters of the document to save computing\n",
    "texts = text_splitter.split_documents(documents[:1000])\n",
    " \n",
    "# There is a lot of complexity hidden in this one line. \n",
    "# the attribute map_reduce instruct the chain to \n",
    "# - first apply the model to each chunck (map stage) \n",
    "# - then all map results and apply the model (reduce stage)\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)\n",
    "summary = chain.run(texts)\n",
    "    \n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4996a3b2-f9d0-496a-abf4-b28fcb300b5e",
   "metadata": {},
   "source": [
    "**OUTPUT**\n",
    " \n",
    "Some map summaries\n",
    "\n",
    "> Alice hears the White Rabbit muttering to itself, concerned that the Duchess will execute it for losing the fan and pair of white kid-gloves. Alice offers to help the Rabbit search for them, but they are nowhere to be found because everything has changed since Alice's dip in the pool.\n",
    "\n",
    "> Alice meets a Rabbit who accuses her of being his housemaid Mary Ann and orders her to fetch his gloves and fan. She finds a neat little house with the Rabbit's name on a brass plate and goes in without knocking. She is afraid of meeting the real Mary Ann before she can find the fan and gloves.\n",
    "\n",
    "> Alice finds her way into a room with a table in the window, containing a fan and some gloves. She notices a bottle and drinks from it, hoping it will make her grow large again. When she drinks half of the bottle she finds her head pressing against the ceiling, so she hastily puts it down.\n",
    "\n",
    "> A character wishes she wouldn't grow anymore, but sadly she continues to grow rapidly. As a result, she kneels on the floor, puts her arm out the window and her foot up the chimney, and is uncertain of her fate.\n",
    "\n",
    " \n",
    "Final summary\n",
    "\n",
    "> In Lewis Carroll's Alice's Adventures in Wonderland, Alice follows a White Rabbit into a strange world and has to navigate unexpected events and peculiar characters. She eventually meets a Caterpillar who helps her regain control of her changing size. Project Gutenberg is a non-profit organization committed to making electronic books free to the public. Donations up to $5,000 are available, and the full license stipulates amounts and terms of use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80463e50-7e7f-49ed-9596-cdbef0250ced",
   "metadata": {},
   "source": [
    "## Summarize stored documents\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO  make use of the vector db</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706f5599-b76f-45e3-a8fe-566f0ea9b328",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 11. Agents\n",
    "\n",
    "LangChain define agents as decision making engines:\n",
    "> Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an unknown chain that depends on the user's input. In these types of chains, there is a “agent” which has access to a suite of tools. Depending on the user input, the agent can then decide which, if any, of these tools to call.\n",
    "\n",
    "It splits the documentation into the following sections:\n",
    "> - Tools: How language models interact with other resources.\n",
    "> - Agents: The language model that drives decision making.\n",
    "> - Toolkits: Sets of tools that when used together can accomplish a specific task.\n",
    "> - Agent Executor: The logic for running agents with tools.\n",
    "\n",
    "\n",
    "**Resources**\n",
    "> - Agents: https://docs.langchain.com/docs/components/agents/\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c830fd4-b796-4226-b087-1cef2bdd7efb",
   "metadata": {},
   "source": [
    "## Tool\n",
    "Tools are interfaces an agent can call to interact with other services\n",
    "\n",
    "**Resources**\n",
    "> - Tools: https://python.langchain.com/docs/modules/agents/tools/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a158512-085d-4782-bc72-8575f83630a2",
   "metadata": {},
   "source": [
    "**Instructions**\n",
    "\n",
    "For the example below, make sure that:\n",
    "- Google API client is installed\n",
    "- a Custome Search Engine is available (CSE)\n",
    "- the API key has been setup up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce2ddb0-06b1-4703-9b96-093a66b921da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "search = GoogleSearchAPIWrapper()\n",
    "\n",
    "tool = Tool(\n",
    "    name=\"Google Search\",\n",
    "    description=\"Search Google for recent results.\",\n",
    "    func=search.run,\n",
    ")\n",
    "\n",
    "tool.run(\"Who is the French Prime Minister name since May 2022?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e130c84-a1f4-4621-93c9-5e2e1632da08",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Agent leveraging tools\n",
    "\n",
    "Google Search and LLM-math are predefined tools:\n",
    "- LLM-Math is a langage model trained to do math logic.\n",
    "- Google)search tool allow to place queries on Google Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4811ff27-c48a-4c9e-a5a1-161dde97802d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# create a model\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# load some tools\n",
    "tools = load_tools([\"google-search\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# setup an agent\n",
    "agent = initialize_agent(tools, \n",
    "                         llm, \n",
    "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "                         verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a780cbf-a82f-4e8d-95c1-3d89f04e59b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent.run(\"How many Teslas have been sold in 2022. Multiple by 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a201cd59-8deb-43b5-a547-172ed6233ffe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent.run(\"Multiply by 2 the population of the capital of Frannce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016567b4-fb8e-4260-836a-8cd8fd47ecf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent.run(\"\"\"Who is the current prime minister of France. \n",
    "Is he or she younger than the President?\"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0345f8ca-a107-4999-ac88-8286c70044a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    # too complex\n",
    "    # either fails because it tries to add dates and nulber\n",
    "    # or give weird results like\n",
    "    # 'Élisabeth Borne will be 70 in the year 2215.'\n",
    "    agent.run(\"\"\"Who is the current prime minister of France. \n",
    "    When will he or she be 70?\"\"\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d0b205-68f8-4ad4-b0be-c47e3107b836",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"background-color:green;color:black;text-align:center;padding:1rem;font-size:1.5rem;\">LANGCHAIN USE CASES</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a119b54e-5cc1-4069-9175-dda6bda8f422",
   "metadata": {},
   "source": [
    "---\n",
    "# [UC] 1. Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8567af48-3fea-45ba-b915-0dc9c50fddfc",
   "metadata": {},
   "source": [
    "---\n",
    "## Summaries Of Short Text\n",
    "Just write a summarization prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfe6ee3-0eed-4af6-8300-7f28b766ed81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# text to be summarized\n",
    "text_sample = \"\"\"\n",
    "The first thing she heard was a general chorus of \"There goes Bill!\"\n",
    "then the Rabbit's voice alone—\"Catch him, you by the hedge!\" Then\n",
    "silence and then another confusion of voices—\"Hold up his head—Brandy\n",
    "now—Don't choke him—What happened to you?\"\n",
    "\n",
    "Last came a little feeble, squeaking voice, \"Well, I hardly know—No\n",
    "more, thank ye. I'm better now—all I know is, something comes at me\n",
    "like a Jack-in-the-box and up I goes like a sky-rocket!\"\n",
    "\n",
    "After a minute or two of silence, they began moving about again, and\n",
    "Alice heard the Rabbit say, \"A barrowful will do, to begin with.\"\n",
    "\n",
    "\"A barrowful of what?\" thought Alice. But she had not long to doubt,\n",
    "for the next moment a shower of little pebbles came rattling in at the\n",
    "window and some of them hit her in the face. Alice noticed, with some\n",
    "surprise, that the pebbles were all turning into little cakes as they\n",
    "lay on the floor and a bright idea came into her head. \"If I eat one of\n",
    "these cakes,\" she thought, \"it's sure to make some< change in my size.\"\n",
    "\n",
    "So she swallowed one of the cakes and was delighted to find that she\n",
    "began shrinking directly. As soon as she was small enough to get through\n",
    "the door, she ran out of the house and found quite a crowd of little\n",
    "animals and birds waiting outside. They all made a rush at Alice the\n",
    "moment she appeared, but she ran off as hard as she could and soon found\n",
    "herself safe in a thick wood.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9646acb1-8cc3-4665-a938-5e6a5fac1d08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Note, the default model is already 'text-davinci-003' \n",
    "# temperature 0 means no randomness\n",
    "llm = OpenAI(temperature=0, model_name='text-davinci-003')\n",
    "\n",
    "# check the number of tokens\n",
    "num_tokens = llm.get_num_tokens(text_sample)\n",
    "print(f\"{num_tokens=}\")\n",
    "\n",
    "# Summarization prompt template\n",
    "template = \"\"\"\n",
    "%INSTRUCTIONS:\n",
    "Please summarize the following piece of text.\n",
    "Respond in a manner that a 5 year old would understand.\n",
    "\n",
    "%TEXT:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "# Create a LangChain prompt template that we can insert values to later\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format(text=text_sample)\n",
    "\n",
    "#print(\"\\nPrompt\")\n",
    "#print(prompt)\n",
    "\n",
    "# run the model\n",
    "output = llm(prompt)\n",
    "\n",
    "print(\"\\nOutput\")\n",
    "print (output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481c1b47-ce62-4d6f-a282-a669747fca0f",
   "metadata": {},
   "source": [
    "---\n",
    "## Summaries of Short text leveraging Summarization Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d45981-0b9c-4ec3-842f-c2a05713441b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Note, the default model is already 'text-davinci-003' \n",
    "# temperature 0 means no randomness\n",
    "llm = OpenAI(temperature=0, model_name='text-davinci-003')\n",
    "\n",
    "# check the number of tokens\n",
    "num_tokens = llm.get_num_tokens(text_sample)\n",
    "print(f\"{num_tokens=}\")\n",
    "\n",
    "# build a document reuse text sampke above\n",
    "doc = Document(\n",
    "    page_content=text_sample,\n",
    "    metadata={\n",
    "        'author':\"Lewis Caroll\",\n",
    "        'title':\"Alice in Wonderland\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# chain expect a list of documents\n",
    "docs = [doc]\n",
    "\n",
    "# the attribute stuff instruct the run the chain once\n",
    "chain = load_summarize_chain(\n",
    "    llm, \n",
    "    chain_type=\"stuff\", \n",
    "    verbose=False)\n",
    "\n",
    "# run the chain against the documment\n",
    "summary = chain.run(docs)\n",
    "    \n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec956f86-893a-495c-afc6-6a37143fd6d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Summaries of Short text leveraging Summarization Chain and custom prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6c1edb-423a-4e89-9551-42eaccfcbdd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Note, the default model is already 'text-davinci-003' \n",
    "# temperature 0 means no randomness\n",
    "llm = OpenAI(temperature=0, model_name='text-davinci-003')\n",
    "\n",
    "# check the number of tokens\n",
    "num_tokens = llm.get_num_tokens(text_sample)\n",
    "print(f\"{num_tokens=}\")\n",
    "\n",
    "# build a document reuse text sampke above\n",
    "doc = Document(\n",
    "    page_content=text_sample,\n",
    "    metadata={\n",
    "        'author':\"Lewis Caroll\",\n",
    "        'title':\"Alice in Wonderland\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# chain expect a list of documents\n",
    "docs = [doc]\n",
    "\n",
    "# setup. a custom prompt\n",
    "# a defaukt one is provide: write a concise summary\n",
    "prompt_template = \"\"\"\n",
    "%INSTRUCTIONS:\n",
    "Please summarize the following piece of text.\n",
    "Respond in a manner that a 5 year old would understand.\n",
    "\n",
    "%TEXT:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "# the attribute stuff instruct the run the chain once\n",
    "chain = load_summarize_chain(\n",
    "    llm, \n",
    "    chain_type=\"stuff\", \n",
    "    prompt=prompt, \n",
    "    verbose=False)\n",
    "\n",
    "# run the chain against the documment\n",
    "summary = chain.run(docs)\n",
    "    \n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee66787c-12f5-4c15-a9fa-78fea9bc7c2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Summaries Of longer Text\n",
    "If the text is longer than the limit in tokens, the text must be splitted in chunks. \n",
    "Langchain components will take care of splitting and chaining the summarization tasks.\n",
    "\n",
    "The Summarization Chain breaks the text into smaller chunks and summarizing each chunk, creating a final summary based on the individual summaries.\n",
    "\n",
    "In this example, the chain first splits the essay into chunks of 700 characters. It then generates summaries for each chunk and creates a final concise summary based on these individual summaries.\n",
    "\n",
    "<br/>\n",
    "**Resources**\n",
    "> - Qummarization quickstart: https://python.langchain.com/docs/modules/chains/popular/summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60af1966-a821-4ee4-807c-387037ae395c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import BSHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    " \n",
    "# Note, the default model is already 'text-davinci-003' \n",
    "# temperature 0 means no randomness\n",
    "llm = OpenAI(temperature=0, model_name='text-davinci-003')\n",
    "\n",
    "# This is the source document.    \n",
    "document_path = \"data/Alice's Adventures in Wonderland, by Lewis Carroll.html\"\n",
    " \n",
    "# Setup a HTML loader\n",
    "loader = BSHTMLLoader(document_path)\n",
    "documents = loader.load()\n",
    " \n",
    "# check the number of tokens\n",
    "num_tokens = llm.get_num_tokens(documents[0].page_content)\n",
    "print(f\"{num_tokens=}\")\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    " \n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    " \n",
    "# setup. a custom prompt\n",
    "# the Summarization Chain provides a defaults prompt: write a concise summary.\n",
    "prompt_template = \"\"\"Write a concise summary of the following text. \n",
    "Focus on the story and ignore details of Project Gutenberg. \n",
    "\n",
    "% TEXT:\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "# the attribute map_reduce instruct the chain to \n",
    "# - first apply the model to each chunck (map stage) \n",
    "# - then all map results and apply the model (reduce stage)\n",
    "chain = load_summarize_chain(\n",
    "    llm, \n",
    "    chain_type=\"map_reduce\", \n",
    "    map_prompt=prompt, \n",
    "    combine_prompt=prompt, \n",
    "    verbose=False)\n",
    "\n",
    "# run the chain against all the document chunks\n",
    "summary = chain.run(texts)\n",
    "\n",
    "# save the final summary\n",
    "with open('alice_summary.txt', 'w') as file:\n",
    "    file.write(summary)\n",
    "    \n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de2aa30-1529-4e4e-b951-d9ac2fd2bec6",
   "metadata": {},
   "source": [
    "**OUTPUT (default prompt)**\n",
    "\n",
    "Alice's Adventures in Wonderland is a classic novel by Lewis Carroll, originally published in 1916. \n",
    "It follows Alice as she falls down a rabbit hole and embarks on a series of strange and wonderful adventures \n",
    "in the magical world of Wonderland. \n",
    "Project Gutenberg is a library of free electronic works owned by the Project Gutenberg Literary Archive Foundation, \n",
    "which allows users to copy, distribute, perform, display or create derivative works based on the work \n",
    "as long as all references to Project Gutenberg are removed. \n",
    "Professor Michael S. Hart was the originator of the concept and has been producing and distributing \n",
    "Project Gutenberg eBooks for 40 years.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f928a54e-871d-4c9d-a633-ff060d237a90",
   "metadata": {},
   "source": [
    "**OUTPUT (custom prompt)**\n",
    "\n",
    "Alice visits the Queen's Croquet Ground and is asked to play a game of croquet with the Queen. \n",
    "Alice is surprised to find that the balls are live hedgehogs and the mallets are flamingos. \n",
    "After winning the game, Alice is invited to join the Queen's procession and finds herself in a court of justice, \n",
    "where she is put on trial for stealing the Queen's tarts. \n",
    "Alice is defended by the White Rabbit and the jury is made up of animals and birds. \n",
    "Alice is found not guilty, but the Queen is furious and orders Alice to leave. \n",
    "Alice is saved by the Cheshire Cat who appears and tells the Queen that she can't do anything to Alice. \n",
    "Alice then meets the Duchess who is out of prison and they walk off together, \n",
    "but the Queen appears and gives Alice a warning. \n",
    "Alice then has a dream in which she encounters a pack of cards that come to life and try to attack her. \n",
    "She wakes up to find her sister and they go home."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f45e8a-c856-439b-a72b-5256c11ab67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "# [UC] 2.  Question & Answering Using Documents As Context\n",
    "Question answering in this context refers to question answering over your document data. F\n",
    "\n",
    "It is basically the example in Indexes.\n",
    "\n",
    "In order to use LLMs for question and answer we must:\n",
    "- Pass the LLM relevant context it needs to answer a question\n",
    "- Pass it our question that we want answered\n",
    "\n",
    "<br/>\n",
    "\n",
    "++Resources**\n",
    "> - [QA] LangChain Question & Answer Docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae034b5-2238-436b-8fa4-33618e0d8f6b",
   "metadata": {},
   "source": [
    "---\n",
    "# [UC] 2.  Question & Answering Using Documents As Context\n",
    "It is basically the example in Indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b21e6ae-1cbb-40b0-baab-95d9dae12a6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import BSHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Annoy\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    " \n",
    "# This is the source document.    \n",
    "document_path = \"data/Alice's Adventures in Wonderland, by Lewis Carroll.html\"\n",
    " \n",
    "# Setup a HTML loader\n",
    "loader = BSHTMLLoader(document_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# Get your splitter ready\n",
    "# in practice they default to 4000 and 200 respectively.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    " \n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(f\"Generated {len(texts)} parts\")\n",
    "\n",
    "# Get embedding engine ready\n",
    "embeddings = OpenAIEmbeddings()\n",
    " \n",
    "# Embedd your texts andd store them in the vector database\n",
    "# dtabase is in memory. it might be savecd to a file and loader later on.\n",
    "db = Annoy.from_documents(texts, embeddings)\n",
    "\n",
    "# Init a retriever for this db\n",
    "#retriever = db.as_retriever()\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":4})\n",
    "\n",
    "# ra query\n",
    "query = \"who is the White Rabbit?\"\n",
    "\n",
    "# retrieve and count indexed documents relevant for the query\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "print(f\"\\nFound {len(docs)} relevant documen(s)\")\n",
    "\n",
    "#samples = \"\\n\\n\".join([x.page_content[:200] for x in docs[:5]])\n",
    "#print(samples)\n",
    "\n",
    "# create a chain to answer questions \n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(), \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    return_source_documents=True)\n",
    "\n",
    "response = qa({\"query\": query})\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f12348f-d1b6-46c4-84d8-8bf320034547",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# using instructions to get a more interesting reponse\n",
    "instructions = \". Give a funny answer 30 words long.\"\n",
    "response = qa({\"query\": query + instructions})\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46668964-0f30-4221-8447-5ef99c56f10f",
   "metadata": {},
   "source": [
    "---\n",
    "### Questions and Answer using a loaded vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c491b0-75c1-4d73-abc1-fb46b5e2fc19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# saving the database for future use\n",
    "\n",
    "docstore_file_path = \"alice_docstore_2\"\n",
    "\n",
    "db.save_local(docstore_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e2276-1c2e-4025-905b-09d2c1532689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loading the database \n",
    "\n",
    "docstore_file_path = \"alice_docstore_2\"\n",
    "\n",
    "loaded_vector_store = Annoy.load_local(\n",
    "   docstore_file_path, embeddings=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# expose this index in a retriever interface\n",
    "retriever = loaded_vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":4})\n",
    "\n",
    "# ra query\n",
    "query = \"who is the White Rabbit?\"\n",
    "\n",
    "# create a chain to answer questions \n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(), \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    return_source_documents=True)\n",
    "\n",
    "instructions = \". Give a pedantic answer 50 words long.\"\n",
    "\n",
    "response = qa({\"query\": query + instructions})\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2523dc19-823e-48fd-9714-c0f5e799963a",
   "metadata": {},
   "source": [
    "---\n",
    "### Complex search on large document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1593a427-460f-4e51-955a-208b273f2ca3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import BSHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    " \n",
    "# Note, the default model is already 'text-davinci-003' \n",
    "# temperature 0 means no randomness\n",
    "llm = OpenAI(temperature=0.3, model_name='text-davinci-003')\n",
    "\n",
    "# This is the source document.    \n",
    "document_path = \"data/Alice's Adventures in Wonderland, by Lewis Carroll.html\"\n",
    " \n",
    "# Setup a HTML loader\n",
    "loader = BSHTMLLoader(document_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    " \n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(f\"\\nFound {len(texts)} part(s)\")\n",
    "\n",
    "\n",
    "# setup. a custom prompt\n",
    "# the Summarization Chain provides a defaults prompt: write a concise summary.\n",
    "map_prompt_template = \"\"\"\n",
    "Make a detailled summary.\n",
    "Focus on the story and ignore details of Project Gutenberg.\n",
    "List all the characters.\n",
    "Output the list of characters as a bullet points lust which showsthe name and description of the characters. \n",
    "\n",
    "% TEXT:\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "\n",
    "# setup. a custom prompt\n",
    "# the Summarization Chain provides a defaults prompt: write a concise summary.\n",
    "# elements of the list of sample will diseapper from the list \n",
    "combine_prompt_template = \"\"\"\n",
    "Make a summary of summaries and merge all character lists.\n",
    "List all the characters.\n",
    "Output the list of characters as a bullet points list which shows the name and description of the characters. \n",
    "\n",
    "% TEXT:\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "combine_prompt = PromptTemplate(template=combine_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "\n",
    "# the attribute map_reduce instruct the chain to \n",
    "# - first apply the model to each chunck (map stage) \n",
    "# - then all map results and apply the model (reduce stage)\n",
    "chain = load_summarize_chain(\n",
    "    llm, \n",
    "    chain_type=\"map_reduce\", \n",
    "    map_prompt=map_prompt, \n",
    "    combine_prompt=combine_prompt, \n",
    "    verbose=False)\n",
    "\n",
    "# run the chain against all the document chunks\n",
    "summary = chain.run(texts)\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a079881-939f-4c89-9a21-9e6c3d319f44",
   "metadata": {},
   "source": [
    "Merged List of Characters: \n",
    "```\n",
    "• Alice – A young girl who falls down a rabbit hole into a fantasy world.\n",
    "• White Rabbit – A rabbit who wears a waistcoat and is always in a hurry.\n",
    "• The Cheshire Cat – A mysterious cat with a wide grin who can disappear and reappear at will.\n",
    "• The Mad Hatter – A strange man who wears a top hat and throws tea parties.\n",
    "• The Queen of Hearts – A tyrannical ruler who is always shouting \"Off with their heads!\"\n",
    "• The March Hare – A hare who is always late and attends the Mad Hatter's tea parties.\n",
    "• The Caterpillar – A wise creature who smokes a hookah and gives Alice advice.\n",
    "• The Duchess – A rude woman who lives in a chaotic house.\n",
    "• The Mock Turtle – A sad creature who tells Alice stories of his past.\n",
    "• The Gryphon – A strange creature with the head of an eagle and the body of a lion.\n",
    "• Dinah – Alice's cat who she misses and hopes will get her saucer of milk at tea-time.\n",
    "• Little Table – A table made of solid glass with a\n",
    "• King of Hearts – Character in the trial mentioned in\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67c9786-5bfc-429b-9552-659f9ed96fd6",
   "metadata": {},
   "source": [
    "- Alice: A young girl who falls down a rabbit hole and enters a strange and magical world.\n",
    "- White Rabbit: A white rabbit who Alice follows down the rabbit hole.\n",
    "- Mad Hatter: A strange character who hosts a tea party with the March Hare and the Dormouse.\n",
    "- March Hare: A hare who attends the Mad Hatter's tea party.\n",
    "- Dormouse: A sleepy mouse who attends the Mad Hatter's tea party.\n",
    "- Queen of Hearts: A tyrannical ruler who orders the beheading of anyone who offends her.\n",
    "- King of Hearts: The Queen of Hearts' husband who is easily manipulated by her.\n",
    "- Caterpillar: A large caterpillar who smokes a hookah and speaks in riddles.\n",
    "- Gryphon: A creature with the head and wings of an eagle and the body of a lion.\n",
    "- Mock Turtle: A sad creature who tells Alice a story about his life."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7374d6cb-ab33-46b8-9711-0f111e9f4e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dd08e56-c9c7-4ef4-8c52-67761da569d1",
   "metadata": {},
   "source": [
    "- Caterpillar: A wise caterpillar who smokes a hookah and offers advice to Alice.\n",
    "- Queen of Hearts: A tyrannical queen who demands to have everyone executed for trivial offenses.\n",
    "- Cheshire Cat: A mysterious, mischievous cat with a wide grin that appears and disappears at will.\n",
    "- Mad Hatter: An eccentric character who hosts a tea party and speaks in nonsensical riddles.\n",
    "- Dormouse: A sleepy character that sits near the Mad Hatter at the tea party.\n",
    "- March Hare: A hare that is often seen with the Mad Hatter and Dormouse at the tea party.\n",
    "- Mock Turtle: A turtle with the head of an ox that talks of its school days."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e191ce-8d91-406d-a125-4721df22bc4a",
   "metadata": {},
   "source": [
    "- Luke Skywalker – A young farm boy from Tatooine who discovers his destiny as a Jedi Knight.\n",
    "- Princess Leia – A brave and resourceful leader of the Rebel Alliance.\n",
    "- Han Solo – A roguish smuggler and pilot who joins forces with the Rebel Alliance.\n",
    "- Obi-Wan Kenobi – A wise and powerful Jedi Master who mentors Luke Skywalker.\n",
    "- Darth Vader – A powerful Sith Lord and the main antagonist of the original trilogy.\n",
    "- C-3PO – A protocol droid built by Anakin Skywalker and programmed for etiquette and protocol.\n",
    "- R2-D2 – An astromech droid who serves as a companion to Luke Skywalker.\n",
    "- Chewbacca – A loyal Wookiee and Han Solo's co-pilot.\n",
    "- Yoda – A wise and powerful Jedi Master who trains Luke Skywalker in the ways of the Force.\n",
    "- Jabba the Hutt – A powerful crime lord who controls much of the criminal underworld in the galaxy.\n",
    "- Boba Fett – A bounty hunter hired by Darth Vader to capture Han Solo.\n",
    "- Lando Calrissian – A smooth-talking smuggler and former friend of Han Solo.\n",
    "- Emperor Palpatine – The evil Sith Lord who"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0040f1bb-18a7-4ebb-a1ec-48407e47e432",
   "metadata": {},
   "source": [
    "---\n",
    "### Complex search on large document using DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44038a09-5b06-4ffc-ad6f-0f1e90446c7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.vectorstores import Annoy\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    " \n",
    "# Note, the default model is already 'text-davinci-003' \n",
    "# temperature 0 means no randomness\n",
    "#model_name=\"gpt-3.5-turbo\" # fdoes not work with map reduce\n",
    "model_name='text-davinci-003'\n",
    "llm = OpenAI(temperature=0.5, model_name=model_name)\n",
    "\n",
    "# loading the database \n",
    "\n",
    "docstore_file_path = \"alice_docstore_2\"\n",
    "\n",
    "loaded_vector_store = Annoy.load_local(\n",
    "   docstore_file_path, embeddings=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# expose this index in a retriever interface\n",
    "# retriev all the docuùents\n",
    "retriever = loaded_vector_store.as_retriever(search_type=\"similarity\", \n",
    "                                             search_kwargs={\"k\":2000, \n",
    "                                                           \"score_threshold\": 0})\n",
    "\n",
    "# retrieve and count indexed documents to ensure all the documents are selected\n",
    "docs = retriever.get_relevant_documents(\"all the story\")\n",
    "print(f\"\\nFound {len(docs)} documen(s)\")\n",
    " \n",
    "# setup. a custom prompt\n",
    "# the Summarization Chain provides a defaults prompt: write a concise summary.\n",
    "map_prompt_template = \"\"\"\n",
    "Make a detailled summary.\n",
    "Focus on the story and ignore details of Project Gutenberg.\n",
    "List all the characters.\n",
    "Output the list of characters as a bullet points lust which showsthe name and description of the characters. \n",
    "\n",
    "% TEXT:\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "\n",
    "# setup. a custom prompt\n",
    "# the Summarization Chain provides a defaults prompt: write a concise summary.\n",
    "combine_prompt_template = \"\"\"\n",
    "Make a summary of summaries and merge all character lists.\n",
    "List all the characters.\n",
    "Output the list of characters as a bullet points list which shows the name and description of the characters. \n",
    "\n",
    "% TEXT:\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "combine_prompt = PromptTemplate(template=combine_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "\n",
    "# the attribute map_reduce instruct the chain to \n",
    "# - first apply the model to each chunck (map stage) \n",
    "# - then all map results and apply the model (reduce stage)\n",
    "chain = load_summarize_chain(\n",
    "    llm, \n",
    "    chain_type=\"map_reduce\", \n",
    "    map_prompt=map_prompt, \n",
    "    combine_prompt=combine_prompt, \n",
    "    verbose=False)\n",
    "\n",
    "# run the chain against all the document chunks\n",
    "summary = chain.run(docs)\n",
    "    \n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58660588-3230-4945-8431-53604f077ec3",
   "metadata": {},
   "source": [
    "- Lory: A bird who is looking for a way out of the wood\n",
    "- Duck: A bird who is looking for a way out of the wood\n",
    "- Eaglet: A bird who is looking for a way out of the wood\n",
    "- Dodo: A bird who is looking for a way out of the wood\n",
    "- Caterpillar: A creature who is smoking a hookah\n",
    "- King and Queen of Hearts: The rulers of the court\n",
    "- Knave of Hearts: A character who is accused of stealing the tarts\n",
    "- Three Gardeners: Characters who are painting white roses red\n",
    "- Five and Seven: Two characters who are guarding the Queen\n",
    "- Two: A character who is the White Rabbit's servant\n",
    "- Ten Soldiers: Characters who are guarding the Queen\n",
    "- Ten Courtiers: Characters who are attending the Queen\n",
    "- Royal Children: Characters who are attending the Queen\n",
    "- Guests: Characters who are attending the Queen\n",
    "- Small Door in a Wall: A mysterious door that Alice finds\n",
    "- Mushroom: A mysterious mushroom that Alice finds\n",
    "- Bottle with the Words \"DRINK ME\" Printed on It: A mysterious bottle that Alice finds\n",
    "- Fan: A mysterious fan that Alice finds\n",
    "- Pair of White Kid-Gloves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dc1ab5-a9e6-4c04-8b55-1bae6564a3a9",
   "metadata": {},
   "source": [
    "Characters:\n",
    "- Alice: The protagonist of the story. She is a young girl who finds herself in a strange world.\n",
    "- White Rabbit: A talking rabbit who is always in a hurry and is late for important appointments.\n",
    "- Cake: A cake with the words \"Eat Me\" written on it.\n",
    "- Key: A tiny golden key found under the table. \n",
    "- Rabbit-Hole: A deep dark hole leading into a corridor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad973be-fc89-4349-afff-2b6515082696",
   "metadata": {},
   "source": [
    "### get the list then query the characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "be94f05e-1bea-4804-ad35-a873e408d02a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 50 part(s)\n",
      "\n",
      "Map Prompt\n",
      "input_variables=['text'] output_parser=None partial_variables={'format_instructions': 'Your response should be a list of comma separated values, eg: `foo, bar, baz`'} template='\\nMake a detailled summary.\\nFocus on the story and ignore details of Project Gutenberg.\\nFind the characters and list thier names.\\n\\n{format_instructions}\\n\\n% TEXT:\\n\\n{text}\\n' template_format='f-string' validate_template=True\n",
      "\n",
      "Combine Prompt\n",
      "input_variables=['text'] output_parser=None partial_variables={} template='\\nTo make a summary, keep all lines starting with the word characters.\\n\\n% TEXT:\\n\\n{text}\\n' template_format='f-string' validate_template=True\n",
      "\n",
      "Response\n",
      "No summary.\n",
      "\n",
      "Characters\n",
      "['No answer required']\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import BSHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "\n",
    "# Note, the default model is already 'text-davinci-003' \n",
    "# temperature 0 means no randomness\n",
    "#model_name=\"gpt-3.5-turbo\" # fdoes not work with map reduce\n",
    "model_name='text-davinci-003'\n",
    "llm = OpenAI(temperature=0.3, model_name=model_name)\n",
    "\n",
    "# This is the source document.    \n",
    "document_path = \"data/Alice's Adventures in Wonderland, by Lewis Carroll.html\"\n",
    " \n",
    "# Setup a HTML loader\n",
    "loader = BSHTMLLoader(document_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    " \n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(f\"\\nFound {len(texts)} part(s)\")\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# setup. a custom prompt\n",
    "# the Summarization Chain provides a defaults prompt: write a concise summary.\n",
    "map_prompt_template = \"\"\"\n",
    "Make a detailled summary.\n",
    "Focus on the story and ignore details of Project Gutenberg.\n",
    "Find the characters and list their names.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% TEXT:\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "map_prompt = PromptTemplate(template=map_prompt_template, \n",
    "                        input_variables=[\"text\"],\n",
    "                        partial_variables={\"format_instructions\": format_instructions}\n",
    "                       )\n",
    "\n",
    "print(\"\\nMap Prompt\")\n",
    "print(map_prompt)\n",
    "\n",
    "# setup. a custom prompt\n",
    "# the Summarization Chain provides a defaults prompt: write a concise summary.\n",
    "combine_prompt_template = \"\"\"\n",
    "To make a summary, keep all lines starting with the word characters.\n",
    "\n",
    "% TEXT:\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "combine_prompt = PromptTemplate(template=combine_prompt_template, \n",
    "                        input_variables=[\"text\"]\n",
    "                        #partial_variables={\"format_instructions\": format_instructions}\n",
    "                       )\n",
    "\n",
    "print(\"\\nCombine Prompt\")\n",
    "print(combine_prompt)\n",
    "\n",
    "\n",
    "# the attribute map_reduce instruct the chain to \n",
    "# - first apply the model to each chunck (map stage) \n",
    "# - then all map results and apply the model (reduce stage)\n",
    "chain = load_summarize_chain(\n",
    "    llm, \n",
    "    chain_type=\"map_reduce\", \n",
    "    map_prompt=map_prompt, \n",
    "    combine_prompt=combine_prompt, \n",
    "    verbose=False)\n",
    "\n",
    "# run the chain against all the document chunks\n",
    "response = chain.run(texts)\n",
    "\n",
    "print(\"\\nResponse\")\n",
    "print(response)\n",
    "\n",
    "#characters = output_parser.parse(response)\n",
    "\n",
    "print(\"\\nCharacters\")\n",
    "print(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "29567160-24b7-4f48-aa50-f95679f686a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 50 part(s)\n",
      "\n",
      "Map Prompt\n",
      "input_variables=['text'] output_parser=None partial_variables={'format_instructions': 'Your response should be a list of comma separated values, eg: `foo, bar, baz`'} template='\\nou will be given a text.\\nExtract the characters\\'s names.\\nIgnore details of Project Gutenberg.\\n\\n{format_instructions}. Add \"characters:\" in front of the list.\\n\\n% TEXT:\\n\\n{text}\\n' template_format='f-string' validate_template=True\n",
      "\n",
      "Part Characters 1\n",
      "['Alice', 'The Duchess', 'The White Rabbit', 'The Cheshire Cat', 'The Mad Hatter', 'The March Hare', 'The Queen of Hearts.']\n",
      "\n",
      "Part Characters 2\n",
      "['Alice', 'White Rabbit', \"Alice's sister\"]\n",
      "\n",
      "Part Characters 3\n",
      "['Alice', 'Dinah', 'White Rabbit']\n",
      "\n",
      "Part Characters 4\n",
      "['Alice']\n",
      "\n",
      "Part Characters 5\n",
      "['Alice,']\n",
      "\n",
      "Part Characters 6\n",
      "['Alice', 'White Rabbit']\n",
      "\n",
      "Part Characters 7\n",
      "['Alice', 'Mouse']\n",
      "\n",
      "Part Characters 8\n",
      "['Alice', 'William the Conqueror', 'Mouse', 'Dinah']\n",
      "\n",
      "Part Characters 9\n",
      "['Duck', 'Dodo', 'Lory', 'Eaglet']\n",
      "\n",
      "Part Characters 10\n",
      "['III', 'Caucus-Race', 'Long Tale', 'birds', 'animals']\n",
      "\n",
      "Part Characters 11\n",
      "['Alice', 'Mouse', 'Lory', 'Duck', 'Edwin', 'Morcar', 'Stigand', 'Edgar Atheling', 'William', 'Dodo', 'Eaglet']\n",
      "\n",
      "Part Characters 12\n",
      "['Alice', 'Dodo']\n",
      "\n",
      "Part Characters 13\n",
      "['Alice', 'Dodo', 'C', 'D', 'Mouse']\n",
      "\n",
      "Part Characters 14\n",
      "['Alice', 'Mouse', 'Canary', 'Dinah']\n",
      "\n",
      "Part Characters 15\n",
      "['White Rabbit', 'Alice', 'Mary Ann', 'W. Rabbit']\n",
      "\n",
      "Part Characters 16\n",
      "['Alice', 'White Rabbit', 'Queen of Hearts', 'King of Hearts']\n",
      "\n",
      "Part Characters 17\n",
      "['Alice', 'Rabbit', 'Mary Ann', 'Pat']\n",
      "\n",
      "Part Characters 18\n",
      "['Alice', 'Bill', 'Rabbit', 'Jack-in-the-box', 'birds', 'animals.']\n",
      "\n",
      "Part Characters 19\n",
      "['Alice', 'Duchess', 'Caterpillar']\n",
      "\n",
      "Part Characters 20\n",
      "['Alice', 'Caterpillar']\n",
      "\n",
      "Part Characters 21\n",
      "['Alice', 'Caterpillar']\n",
      "\n",
      "Part Characters 22\n",
      "['Alice', 'Caterpillar', 'pigeon']\n",
      "\n",
      "Part Characters 23\n",
      "['Alice', 'Pigeon']\n",
      "\n",
      "Part Characters 24\n",
      "['Alice']\n",
      "\n",
      "Part Characters 25\n",
      "['Pig', 'Pepper', 'Footman (2x)']\n",
      "\n",
      "Part Characters 26\n",
      "['Alice', 'Fish-Footman', 'Frog-Footman', 'Duchess', 'Cook', 'Baby', 'Cheshire-Cat.']\n",
      "\n",
      "Part Characters 27\n",
      "['Alice', 'Duchess', 'Cook', 'Cheshire-Cat', 'Baby', 'Queen']\n",
      "\n",
      "Part Characters 28\n",
      "['Alice', 'Cheshire-Puss', 'Hatter', 'March Hare', 'Queen']\n",
      "\n",
      "Part Characters 29\n",
      "['Alice', 'March Hare', 'Hatter', 'Dormouse']\n",
      "\n",
      "Part Characters 30\n",
      "['Alice', 'Hatter', 'March Hare', 'Dormouse', 'Knave of Hearts']\n",
      "\n",
      "Part Characters 31\n",
      "['Alice', 'Five', 'Seven', 'Two', 'Ten Soldiers', 'Ten Courtiers', 'Royal Children', 'White Rabbit', 'Knave of Hearts', 'King', 'Queen of Hearts.']\n",
      "\n",
      "Part Characters 32\n",
      "['Alice', 'White Rabbit', 'Queen', 'Duchess', 'Cheshire-Cat']\n",
      "\n",
      "Part Characters 33\n",
      "['Alice', 'Hedgehog', 'Flamingo', 'Duchess', 'Queen']\n",
      "\n",
      "Part Characters 34\n",
      "['King', 'Queen', 'Alice']\n",
      "\n",
      "Part Characters 35\n",
      "['King of Hearts', 'Queen of Hearts', 'Knave of Hearts', 'White Rabbit', 'Judge', 'Twelve Creatures', 'Herald']\n",
      "\n",
      "Part Characters 36\n",
      "['Hatter', 'March Hare', 'Dormouse', 'King', 'Queen', 'Alice', \"Duchess's Cook\", 'White Rabbit']\n",
      "\n",
      "Part Characters 37\n",
      "['Alice', 'King', 'Queen']\n",
      "\n",
      "Part Characters 38\n",
      "['Alice', 'King', 'White Rabbit', 'Knave', 'Queen']\n",
      "\n",
      "Part Characters 39\n",
      "['Alice', 'Queen', \"Alice's sister\"]\n",
      "\n",
      "Part Characters 40\n",
      "['None']\n",
      "\n",
      "Part Characters 41\n",
      "['None']\n",
      "\n",
      "Part Characters 42\n",
      "['None']\n",
      "\n",
      "Part Characters 43\n",
      "['']\n",
      "\n",
      "Part Characters 44\n",
      "['None']\n",
      "\n",
      "Part Characters 45\n",
      "['None']\n",
      "\n",
      "Part Characters 46\n",
      "['']\n",
      "\n",
      "Part Characters 47\n",
      "['None']\n",
      "\n",
      "Part Characters 48\n",
      "['None']\n",
      "\n",
      "Part Characters 49\n",
      "['']\n",
      "\n",
      "Part Characters 50\n",
      "['Professor Michael S. Hart', 'Project Gutenberg™', 'U.S.', 'donors', 'volunteers', 'Project Gutenberg™ eBooks.']\n",
      "\n",
      "All Characters 50\n",
      "{'', 'Professor Michael S. Hart', 'Pepper', 'Cook', 'Judge', 'The Queen of Hearts.', 'animals', 'King of Hearts', 'Cheshire-Cat.', 'Lory', 'Bill', 'U.S.', 'Alice,', 'Edwin', 'Cheshire-Cat', 'Baby', 'Hatter', 'Dinah', 'Cheshire-Puss', 'Jack-in-the-box', 'Dormouse', 'animals.', 'Caucus-Race', 'Mouse', 'Footman (2x)', 'W. Rabbit', 'D', 'Alice', 'volunteers', 'Duchess', 'pigeon', 'Hedgehog', 'donors', 'Ten Courtiers', 'Edgar Atheling', 'Stigand', 'Mary Ann', 'C', 'Queen of Hearts', 'Herald', 'Royal Children', 'Morcar', 'The Mad Hatter', 'Eaglet', 'None', 'Canary', 'The March Hare', 'King', 'Flamingo', 'Knave of Hearts', 'III', 'Fish-Footman', 'Duck', 'Seven', 'Long Tale', 'Frog-Footman', 'Ten Soldiers', 'Knave', 'Caterpillar', \"Duchess's Cook\", 'Two', 'The Duchess', 'Queen', 'Dodo', 'William the Conqueror', 'Pig', 'William', 'White Rabbit', 'Pigeon', 'Project Gutenberg™', 'Pat', 'Queen of Hearts.', \"Alice's sister\", 'March Hare', 'birds', 'Twelve Creatures', 'Five', 'Project Gutenberg™ eBooks.', 'The Cheshire Cat', 'Rabbit', 'The White Rabbit'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import BSHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Note, the default model is already 'text-davinci-003' \n",
    "# temperature 0 means no randomness\n",
    "#model_name=\"gpt-3.5-turbo\" # fdoes not work with map reduce\n",
    "model_name='text-davinci-003'\n",
    "llm = OpenAI(temperature=0.3, model_name=model_name)\n",
    "\n",
    "# This is the source document.    \n",
    "document_path = \"data/Alice's Adventures in Wonderland, by Lewis Carroll.html\"\n",
    " \n",
    "# Setup a HTML loader\n",
    "loader = BSHTMLLoader(document_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    " \n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(f\"\\nFound {len(texts)} part(s)\")\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# setup. a custom prompt\n",
    "# the Summarization Chain provides a defaults prompt: write a concise summary.\n",
    "map_prompt_template = \"\"\"\n",
    "ou will be given a text.\n",
    "Extract the characters's names.\n",
    "Ignore details of Project Gutenberg.\n",
    "\n",
    "{format_instructions}. Add \"characters:\" in front of the list.\n",
    "\n",
    "% TEXT:\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "map_prompt = PromptTemplate(template=map_prompt_template, \n",
    "                        input_variables=[\"text\"],\n",
    "                        partial_variables={\"format_instructions\": format_instructions}\n",
    "                       )\n",
    "\n",
    "print(\"\\nMap Prompt\")\n",
    "print(map_prompt)\n",
    "\n",
    "\n",
    "# the attribute map_reduce instruct the chain to \n",
    "# - first apply the model to each chunck (map stage) \n",
    "# - then all map results and apply the model (reduce stage)\n",
    "chain = load_summarize_chain(\n",
    "    llm, \n",
    "    chain_type=\"stuff\", \n",
    "    prompt=map_prompt,  \n",
    "    verbose=False)\n",
    "\n",
    "i = 0\n",
    "characters = []\n",
    "prefix = \"Characters:\"\n",
    "for text in texts:\n",
    "    i += 1\n",
    "    \n",
    "    # run the chain against all the document chunks\n",
    "    # chain expect a list of documents\n",
    "    response = chain.run([text])\n",
    "\n",
    "    #print(f\"\\nResponse {i}\")\n",
    "    #print(response)\n",
    "\n",
    "    lines = response.split('\\n')\n",
    "    #print(f\"\\nlines {i}\")\n",
    "    #print(lines)\n",
    "    for line in lines:\n",
    "        #print(f\"\\nline {i}\")\n",
    "        #print(line)\n",
    "        if line.startswith(prefix):\n",
    "            part_characters = output_parser.parse(line.replace(prefix, ''))\n",
    "\n",
    "            print(f\"\\nPart Characters {i}\")\n",
    "            print(part_characters)\n",
    "\n",
    "            characters.extend(part_characters)\n",
    "    \n",
    "\n",
    "print(f\"\\nAll Characters {i}\")\n",
    "print(set(characters))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d47e5a-cb1e-4370-891a-a8c24b8b3828",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO lookup each character + filter relevant part of the document</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ac82b0-5d0e-4533-b281-782cc65a7e26",
   "metadata": {},
   "source": [
    "Characters:\n",
    "- Alice: The protagonist of the story. She is a young girl who finds herself in a strange world.\n",
    "- White Rabbit: A talking rabbit who is always in a hurry and is late for important appointments.\n",
    "- Cake: A cake with the words \"Eat Me\" written on it.\n",
    "- Key: A tiny golden key found under the table. \n",
    "- Rabbit-Hole: A deep dark hole leading into a corridor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786bf073-1cc2-4dbe-a49d-b647ce451451",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e87062-5498-42fd-8d37-237a9fd3fddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5922193-5d59-4aa6-8c04-3daed68f690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO essayer sequence mapper list + qa par personnage</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889128fa-f108-4e4a-8d75-aeb7b43bace9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO and FIXME</div>\n",
    "\n",
    "vector store backed retriever \n",
    "\n",
    "https://python.langchain.com/docs/modules/data_connection/retrievers/how_to/vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a5c5a0-01f8-4cf7-8b67-9fa9db8749a6",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/modules/chains/additional/question_answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db949ae-3aaa-400a-b7d8-d84f87aff51b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bc34ab-72dd-4aa3-9bdb-cd9e8c060100",
   "metadata": {},
   "outputs": [],
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO explain refine and mapreduce </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41e4ceb-0127-417a-87cc-f1e3a50a8a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fa8320b-3668-4f6a-8e6f-56ef5462c7e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TOdo get a list and show each character to demo output prser and seq chain\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO </div>\n",
    "\n",
    "prompt\n",
    "parse and map\n",
    "seq chain\n",
    "```python\n",
    "output_parser = RegexParser(\n",
    "    regex=r\"(.*?)\\nScore: (.*)\",\n",
    "    output_keys=[\"answer\", \"score\"],\n",
    ")\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    "    output_parser=output_parser,\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598856aa-2626-4080-8087-c5e3718d3a88",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> TODO \n",
    "how to use qa in chain and do something like make a list and gie details.\n",
    "Another option parsed output and browse the list Ouotput parser as list ?\n",
    "alternative conversation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269fd6e6-7e08-41b1-a006-cc326d1a3b2a",
   "metadata": {},
   "source": [
    "# [UC] ...\n",
    "AAnalyzing stuctured data\n",
    "\n",
    "https://python.langchain.com/docs/use_cases/tabular.html\n",
    "\n",
    "https://python.langchain.com/docs/modules/agents/toolkits/csv.html\n",
    "\n",
    "https://python.langchain.com/docs/modules/agents/toolkits/sql_database.html\n",
    "\n",
    "https://python.langchain.com/docs/modules/agents/toolkits/pandas.html\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6247896-89cb-4b0d-9e57-de435db0c233",
   "metadata": {},
   "source": [
    "# [UC] ...\n",
    "API Chains\n",
    "\n",
    "https://python.langchain.com/docs/modules/chains/popular/api.html\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3ba49b-c9cf-48ee-80f0-9294354b8d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [UC] ...\n",
    "graph index creator\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> TODO </div>"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-3:615547856133:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
