{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ee83b6f-8759-4733-bae7-955d5829435b",
   "metadata": {},
   "source": [
    "# Generation G - S1E4 - QA eval\n",
    "\n",
    "This notebook is the companion of posts about Generative AI.\n",
    "\n",
    "This episode shows how to evaluate QA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b198bd5-97e8-4943-ae31-85cac5698004",
   "metadata": {},
   "source": [
    "# Material"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a287d35-40f0-47cd-93af-a7326b03f167",
   "metadata": {},
   "source": [
    "## Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4604152f-f587-48d0-adf8-9a3f653c1fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Update environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f30f2f91-cc9a-42f9-9c77-972e72526e6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://deb.debian.org/debian bullseye InRelease [116 kB]\n",
      "Get:2 http://deb.debian.org/debian bullseye-updates InRelease [44.1 kB]\n",
      "Get:3 http://security.debian.org/debian-security bullseye-security InRelease [48.4 kB]\n",
      "Get:4 http://deb.debian.org/debian bullseye/main amd64 Packages [8183 kB]\n",
      "Get:5 http://deb.debian.org/debian bullseye-updates/main amd64 Packages [17.3 kB]\n",
      "Get:6 http://security.debian.org/debian-security bullseye-security/main amd64 Packages [251 kB]\n",
      "Fetched 8659 kB in 1s (5896 kB/s)                         \n",
      "Reading package lists... Done\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n"
     ]
    }
   ],
   "source": [
    "!apt-get update && apt-get install -y build-essential 1>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b81739d-58b1-4a53-831c-f12978f001f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://deb.debian.org/debian bullseye InRelease\n",
      "Hit:2 http://deb.debian.org/debian bullseye-updates InRelease\n",
      "Hit:3 http://security.debian.org/debian-security bullseye-security InRelease\n",
      "Reading package lists... Done\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n"
     ]
    }
   ],
   "source": [
    "!apt-get update && apt-get install -y jq 1>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dde1920f-5820-4f4f-89ce-0cfc297f6854",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip  1>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a345dc0-c97e-4beb-b501-b4d573fb2ce6",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ac7587f-c5e7-423f-af34-d4189caa0be1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip install langchain==0.0.230 1>/dev/null\n",
    "!pip install langchain==0.0.266 1>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1a92f5a-0d75-42ad-9e41-74dd1e61cd68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install openai==0.27.8 1>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84479774-4a0f-4d3e-998f-8d23ba34cf30",
   "metadata": {},
   "source": [
    "## Secrets and credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "006d1f64-ed18-4c4e-a4df-6f192488b8ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash --out secrets \n",
    "# using AWS's Secret Manager to store keys\n",
    "# garb the keys and store it into a Pytthon variable\n",
    "export RESPONSE=$(aws secretsmanager get-secret-value --secret-id 'salvia/labbench/tests' )\n",
    "export SECRETS=$( echo $RESPONSE | jq '.SecretString | fromjson')\n",
    "\n",
    "echo $SECRETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10065ce2-fbb0-4f51-9bc5-e3ef4d44dede",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = eval(secrets)[\"OPENAI_API_KEY\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff0c66c-6183-4812-901e-34e3deff6ad6",
   "metadata": {},
   "source": [
    "# Code session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ea4a621-1b2b-493d-8d65-adecb0523dd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.llms.fake import FakeListLLM\n",
    "\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"] \n",
    "\n",
    "def get_llm_model():\n",
    "    llm = OpenAI(temperature=0.7, openai_api_key=openai_api_key)\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f10fe2-0095-4ddf-ae6f-8afd5c069128",
   "metadata": {},
   "source": [
    "## custom criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b82a813-e488-448d-8e3c-5a895493df8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.evaluation.criteria import CriteriaEvalChain\n",
    "\n",
    "llm = get_llm_model()\n",
    "\n",
    "criteria = {\"humor-criterion\": \"Is it funny?\", \"accuracy-criterion\": \"Is it accurate?\"}  \n",
    "evaluator = CriteriaEvalChain.from_llm(llm=llm, criteria=criteria)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5863bf56-6010-4583-a43e-e1c81b7141ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The average distance from the Earth to the Moon is 238,855 miles (384,400 kilometers).\n",
      "\n",
      " {'reasoning': 'The criterion is conciseness. This means the submission should '\n",
      "              'be brief and to the point. \\n'\n",
      "              '\\n'\n",
      "              'Looking at the submission, it directly answers the question '\n",
      "              '\"What is the distance to the Moon?\" by stating the average '\n",
      "              'distance in both miles and kilometers. \\n'\n",
      "              '\\n'\n",
      "              'The submission does not include any unnecessary information or '\n",
      "              'details that are not directly related to the question. \\n'\n",
      "              '\\n'\n",
      "              'Therefore, the submission meets the criterion of conciseness. \\n'\n",
      "              '\\n'\n",
      "              'Y',\n",
      " 'score': 1,\n",
      " 'value': 'Y'} \n",
      "\n",
      "Tokens Used: 305\n",
      "\tPrompt Tokens: 196\n",
      "\tCompletion Tokens: 109\n",
      "Successful Requests: 2\n",
      "Total Cost (USD): $0.01146\n",
      "CPU times: user 17.5 ms, sys: 4.27 ms, total: 21.7 ms\n",
      "Wall time: 7.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pprint import pformat\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "\n",
    "    query = \"What is the distance to the Moon?\"\n",
    "    response = llm(query)\n",
    "    print(response)\n",
    "\n",
    "    evaluation = evaluator.evaluate_strings(prediction=response, input=query)\n",
    "    print(f\"\\n {pformat(evaluation)} \\n\")\n",
    "\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee6e668-61cc-4ceb-ab9c-d3a247d2db04",
   "metadata": {},
   "source": [
    "1 request + 1 request per evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb7116c2-c2c3-4214-b8cc-ef4c2e6b6020",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "384,400 kilometers! But it feels like a million miles away when you're trying to find a parking spot.\n",
      "\n",
      " {'reasoning': 'Step 1: Analyze the humor-criterion: Is it funny?\\n'\n",
      "              'The submission is humorous in that it adds a humorous spin to '\n",
      "              'the answer by referring to the difficulty of finding a parking '\n",
      "              'spot.\\n'\n",
      "              '\\n'\n",
      "              'Step 2: Analyze the accuracy-criterion: Is it accurate?\\n'\n",
      "              'The submission accurately states the distance to the Moon as '\n",
      "              '384,400 kilometers.\\n'\n",
      "              '\\n'\n",
      "              'Step 3: Conclusion\\n'\n",
      "              'The submission meets both criteria.\\n'\n",
      "              'Y',\n",
      " 'score': 1,\n",
      " 'value': 'Y'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pprint import pformat\n",
    "\n",
    "query = \"Give a funny answer to What is the distance to the Moon? It must be accurate though.\"\n",
    "response = llm(query)\n",
    "print(response)\n",
    "\n",
    "evaluation = evaluator.evaluate_strings(prediction=response, input=query)\n",
    "print(f\"\\n {pformat(evaluation)} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf9b63a-455c-4a14-92ce-295e684159ad",
   "metadata": {},
   "source": [
    "## Labeled "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9935d78-8e7c-4622-bd20-efc6cfade654",
   "metadata": {},
   "source": [
    "evaluation\n",
    "https://docs.langchain.com/docs/use-cases/evaluation\n",
    "https://docs.langchain.com/docs/use-cases/evaluation\n",
    "\n",
    "\n",
    "In scenarios where you wish to assess a model's output using a specific rubric or criteria set, the criteria evaluator proves to be a handy tool. It allows you to verify if an LLM or Chain's output complies with a defined set of criteri\n",
    "\n",
    "https://python.langchain.com/docs/guides/evaluation/string/criteria_eval_chain\n",
    "\n",
    "named criteria \n",
    "https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.criteria.eval_chain.Criteria.html#\n",
    "\n",
    "auto-evaluator\n",
    "https://github.com/langchain-ai/auto-evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc12f9f2-a4ee-4fed-baf0-1fd7b4a2a3f9",
   "metadata": {},
   "source": [
    "       The criteria to evaluate the runs against. It can be:\n",
    "                -  a mapping of a criterion name to its description\n",
    "                -  a single criterion name present in one of the default criteria\n",
    "                -  a single `ConstitutionalPrinciple` instance\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db49f873-4535-4ce6-86db-9a7c48c7b239",
   "metadata": {},
   "source": [
    "## named\n",
    "\n",
    "   Criteria.CONCISENESS: \"Is the submission concise and to the point?\",\n",
    "    Criteria.RELEVANCE: \"Is the submission referring to a real quote from the text?\",\n",
    "    Criteria.CORRECTNESS: \"Is the submission correct, accurate, and factual?\",\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "869a63dc-54b4-4e59-b810-53a99cf21cc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The White Rabbit is a character from Lewis Carroll's novel Alice's Adventures in Wonderland, who leads Alice into a fantastical world.\n",
      "\n",
      " {'reasoning': 'The criterion for this task is conciseness. \\n'\n",
      "              '\\n'\n",
      "              'The submission is \"The White Rabbit is a character from Lewis '\n",
      "              \"Carroll's novel Alice's Adventures in Wonderland, who leads \"\n",
      "              'Alice into a fantastical world.\"\\n'\n",
      "              '\\n'\n",
      "              'The submission is concise as it provides a brief and direct '\n",
      "              'answer to the question. It identifies the White Rabbit as a '\n",
      "              'character from a specific novel and gives a brief description '\n",
      "              'of his role in the story. \\n'\n",
      "              '\\n'\n",
      "              'The submission does not include any unnecessary information or '\n",
      "              'details that would make it less concise. \\n'\n",
      "              '\\n'\n",
      "              'Therefore, the submission meets the criterion of conciseness. \\n'\n",
      "              '\\n'\n",
      "              'Y',\n",
      " 'score': 1,\n",
      " 'value': 'Y'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.evaluation import EvaluatorType\n",
    "from pprint import pformat\n",
    "\n",
    "llm = get_llm_model()\n",
    "evaluator = load_evaluator(EvaluatorType.CRITERIA, criteria=\"conciseness\")\n",
    "\n",
    "query =  \"Who is the White Rabbit? Be concise.\"\n",
    "response = llm(query)\n",
    "print(response)\n",
    "\n",
    "evaluation = evaluator.evaluate_strings(prediction=response, input=query)\n",
    "print(f\"\\n {pformat(evaluation)} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4022c47-1014-44a7-8517-65e99f78f921",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The White Rabbit is a fictional character in Lewis Carroll's 1865 novel Alice's Adventures in Wonderland. He is seen in the beginning of the novel at the bottom of the rabbit-hole, talking to himself about being late before noticing Alice and scurrying away. He is portrayed as a harried and flustered character due to being constantly late. The White Rabbit is one of the first characters Alice meets in her fantastical journey.\n",
      "\n",
      " {'reasoning': 'The criterion for this task is conciseness. This means the '\n",
      "              'submission should be brief and to the point, without '\n",
      "              'unnecessary details or filler.\\n'\n",
      "              '\\n'\n",
      "              'Looking at the submission, it provides a detailed explanation '\n",
      "              'of who the White Rabbit is, including his role in the story, '\n",
      "              'his character traits, and his interactions with Alice. While '\n",
      "              'this information is relevant and accurate, it is not '\n",
      "              'necessarily concise. The question asked for a pedantic answer, '\n",
      "              \"which means it should be precise and accurate, but it doesn't \"\n",
      "              'necessarily mean it should be lengthy or detailed.\\n'\n",
      "              '\\n'\n",
      "              'Therefore, the submission does not meet the criterion of '\n",
      "              'conciseness. It provides more information than necessary to '\n",
      "              'answer the question, making it less concise.\\n'\n",
      "              '\\n'\n",
      "              'N',\n",
      " 'score': 0,\n",
      " 'value': 'N'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.evaluation import EvaluatorType\n",
    "from pprint import pformat\n",
    "\n",
    "llm = get_llm_model()\n",
    "evaluator = load_evaluator(EvaluatorType.CRITERIA, criteria=\"conciseness\")\n",
    "\n",
    "query =  \"Who is the White Rabbit? Be pedantic.\"\n",
    "response = llm(query)\n",
    "print(response)\n",
    "\n",
    "evaluation = evaluator.evaluate_strings(prediction=response, input=query)\n",
    "print(f\"\\n {pformat(evaluation)} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a015b2-4867-48ae-b21e-db05e24f6594",
   "metadata": {},
   "source": [
    "In Anthropic can evaluate against a ConstitutionalPrinciple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05cc222-1318-4108-98d5-cc8bff140adb",
   "metadata": {},
   "source": [
    "## Check against reference - labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e2cd26e-b982-4016-b586-209cd655453d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The average distance from Earth to the Moon is 238,855 miles (384,400 kilometers).\n",
      "\n",
      " {'reasoning': 'Step 1: Compare the submission with the reference.\\n'\n",
      "              '\\n'\n",
      "              'The submission states that the distance is 384,400 kilometers '\n",
      "              'while the reference states 384,000 km. \\n'\n",
      "              '\\n'\n",
      "              'Step 2: Make a determination.\\n'\n",
      "              '\\n'\n",
      "              'The submission is not correct and accurate, since it differs '\n",
      "              'from the reference.\\n'\n",
      "              '\\n'\n",
      "              'N',\n",
      " 'score': 0,\n",
      " 'value': 'N'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.evaluation.criteria import LabeledCriteriaEvalChain\n",
    "from langchain.evaluation.criteria import CriteriaEvalChain\n",
    "\n",
    "llm = get_llm_model()\n",
    "\n",
    "criteria = \"correctness\"\n",
    "eval_chain = LabeledCriteriaEvalChain.from_llm(\n",
    "        llm=llm,\n",
    "        criteria=criteria,\n",
    "        requires_reference=True\n",
    "    )\n",
    "\n",
    "query = \"What is the distance to the Moon?\"\n",
    "response = llm(query)\n",
    "print(response)\n",
    "\n",
    "evaluation = eval_chain.evaluate_strings(prediction=response, \n",
    "                                         input=query, \n",
    "                                         reference=\"384,000 km\")\n",
    "print(f\"\\n {pformat(evaluation)} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff76c1c-1289-4160-bbff-af70f4e56510",
   "metadata": {},
   "source": [
    "## Check a bunch of answers - prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "920aacdf-db39-4b76-b2c6-e4f70a068242",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json \n",
    "from random import randrange\n",
    "\n",
    "def get_qa_sample(size):\n",
    "    # loads the dataset\n",
    "    squad_dataset_path = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\"\n",
    "    with urllib.request.urlopen(squad_dataset_path) as url:\n",
    "        data= json.load(url)\n",
    "\n",
    "    # randomly pick answers\n",
    "    answers = []\n",
    "    while len(answers) < size:\n",
    "        a = randrange(len(data['data']))\n",
    "        p = randrange(len(data['data'][a]['paragraphs']))\n",
    "        q = randrange(len(data['data'][a]['paragraphs'][p]['qas']))\n",
    "        nr_t = len(data['data'][a]['paragraphs'][p]['qas'][q]['answers'])\n",
    "        if nr_t > 0:\n",
    "            t = randrange(nr_t)\n",
    "            question = data['data'][a]['paragraphs'][p]['qas'][q]['question']\n",
    "            answer = data['data'][a]['paragraphs'][p]['qas'][q]['answers'][0]['text']\n",
    "            answers.append({'question' : question, 'answer': answer})\n",
    "    return answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ece40872-f534-417b-b07f-0b939fc22918",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'answer': 'academic',\n",
      "  'question': 'Along with sport and art, what is a type of talent '\n",
      "              'scholarship?'},\n",
      " {'answer': 'chemical energy',\n",
      "  'question': 'What does oxygen the basis for in combustion?'},\n",
      " {'answer': 'Jacksonville Consolidation',\n",
      "  'question': 'What political group began to gain support following the '\n",
      "              'corruption scandal?'},\n",
      " {'answer': 'Zachęta National Gallery of Art',\n",
      "  'question': 'What is the oldest exhibition site in Warsaw?'},\n",
      " {'answer': '7',\n",
      "  'question': 'What article of the Grundgesetz grants the right to make '\n",
      "              'private schools?'}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "questions_answers = get_qa_sample(5)\n",
    "pprint(questions_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e0febb-4c6f-47cc-b75c-5765f750b2ed",
   "metadata": {},
   "source": [
    "## Check a bunch od answers - batch eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e5ec1e3-fcee-4461-9255-ff6bf0daf5aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "questions ['Along with sport and art, what is a type of talent scholarship?', 'What does oxygen the basis for in combustion?', 'What political group began to gain support following the corruption scandal?', 'What is the oldest exhibition site in Warsaw?', 'What article of the Grundgesetz grants the right to make private schools?']\n",
      "predictions ['\\n\\nAcademic talent scholarships are a type of talent scholarship. These scholarships reward students who have demonstrated excellence in their academic studies, typically in areas such as math, science, and humanities.', '\\n\\nOxygen is the basis for combustion because it is a necessary component for oxidation (the combination of a fuel with oxygen in order to release energy in the form of heat). Without oxygen, combustion would not occur.', '\\n\\nThe populist party began to gain support following the corruption scandal.', '\\n\\nThe oldest exhibition site in Warsaw is the Zachęta National Gallery of Art, which was founded in 1860.', '\\n\\nArticle 7 of the Grundgesetz grants the right to establish private schools in Germany. It states: \"All Germans shall have the right to establish private schools within the framework of the law.\"']\n",
      "Tokens Used: 223\n",
      "\tPrompt Tokens: 60\n",
      "\tCompletion Tokens: 163\n",
      "Successful Requests: 5\n",
      "Total Cost (USD): $0.00446\n",
      "token used=223 total cost (USD)=0.00446 \n",
      "\n",
      "CPU times: user 15.1 ms, sys: 4.08 ms, total: 19.2 ms\n",
      "Wall time: 1.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "\n",
    "    llm = get_llm_model()\n",
    "\n",
    "    questions = [ qa['question'] for qa in questions_answers]\n",
    "    print(f\"questions {questions}\")\n",
    "    predictions = llm.batch(questions)\n",
    "    #predictions = [ {'result': result['text']} for result in llm.generate(questions)]\n",
    "    print(f\"\\npredictions {predictions}\\n\")\n",
    "\n",
    "    print(cb)\n",
    "    print(f\"token used={cb.total_tokens} total cost (USD)={cb.total_cost} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3447927c-f44e-4d95-ad40-ec0e9bca79c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer': 'q1', 'question': 'a1', 'result': 'r1'},\n",
       " {'answer': 'q2', 'question': 'a2', 'result': 'r2'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "qas = [{'answer': 'a1', 'question': 'q1'},\n",
    "       {'answer': 'a2', 'question': 'q2'}]\n",
    "results = ['r1', 'r2']\n",
    "merge = [ {'answer': qa['question'], 'question': qa['answer'], 'result': r} for (qa, r) in zip(qas, results)]         \n",
    "list(merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955b4c32-6829-4d98-9032-98ae806038d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check a bunch od answers - batcheval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5fd8fad5-745d-460c-ac0d-5f7b07fcbb1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " questions \n",
      " ['Along with sport and art, what is a type of talent scholarship?',\n",
      " 'What does oxygen the basis for in combustion?',\n",
      " 'What political group began to gain support following the corruption scandal?',\n",
      " 'What is the oldest exhibition site in Warsaw?',\n",
      " 'What article of the Grundgesetz grants the right to make private schools?'] \n",
      "\n",
      "\n",
      " predictions \n",
      " [{'answer': 'academic',\n",
      "  'question': 'Along with sport and art, what is a type of talent scholarship?',\n",
      "  'result': '\\n'\n",
      "            '\\n'\n",
      "            'A music talent scholarship is another type of talent '\n",
      "            'scholarship.'},\n",
      " {'answer': 'chemical energy',\n",
      "  'question': 'What does oxygen the basis for in combustion?',\n",
      "  'result': '\\n'\n",
      "            '\\n'\n",
      "            'Oxygen is the basis for combustion because it is necessary for '\n",
      "            'the process of burning fuel to occur. When fuel is burned, '\n",
      "            'chemical bonds within the fuel are broken down and the energy '\n",
      "            'that was stored in those bonds is released. The oxygen in the air '\n",
      "            'combines with the fuel molecules to form new compounds, releasing '\n",
      "            'energy in the form of heat and light.'},\n",
      " {'answer': 'Jacksonville Consolidation',\n",
      "  'question': 'What political group began to gain support following the '\n",
      "              'corruption scandal?',\n",
      "  'result': '\\n'\n",
      "            '\\n'\n",
      "            'The political group that began to gain support following the '\n",
      "            'corruption scandal was the anti-corruption movement. This '\n",
      "            'movement was led by a coalition of civil society groups, '\n",
      "            'opposition politicians, and independent activists who called for '\n",
      "            'reforms in the government and greater transparency in public '\n",
      "            'decision-making.'},\n",
      " {'answer': 'Zachęta National Gallery of Art',\n",
      "  'question': 'What is the oldest exhibition site in Warsaw?',\n",
      "  'result': '\\n'\n",
      "            '\\n'\n",
      "            'The Royal Łazienki Park is the oldest exhibition site in Warsaw. '\n",
      "            'It was established in the 17th century by King Stanisław August '\n",
      "            'Poniatowski and is now one of the most popular parks in the city. '\n",
      "            'The park is home to a variety of historic buildings, monuments, '\n",
      "            'and sculptures, as well as a number of museums and galleries.'},\n",
      " {'answer': '7',\n",
      "  'question': 'What article of the Grundgesetz grants the right to make '\n",
      "              'private schools?',\n",
      "  'result': '\\n'\n",
      "            '\\n'\n",
      "            'Article 7 of the Grundgesetz (Basic Law for the Federal Republic '\n",
      "            'of Germany) grants the right to establish private schools. '\n",
      "            'Private schools in Germany are subject to the supervision of the '\n",
      "            'state. Parents have the right to choose a school for their '\n",
      "            'children, and the state must ensure that the schools offer a '\n",
      "            'quality education.'}] \n",
      "\n",
      "\n",
      " graded output \n",
      " [{'results': ' INCORRECT'},\n",
      " {'results': ' CORRECT'},\n",
      " {'results': ' INCORRECT'},\n",
      " {'results': ' INCORRECT'},\n",
      " {'results': ' CORRECT'}] \n",
      "\n",
      "Tokens Used: 1557\n",
      "\tPrompt Tokens: 1262\n",
      "\tCompletion Tokens: 295\n",
      "Successful Requests: 10\n",
      "Total Cost (USD): $0.031139999999999998\n",
      "token used=1557 total cost (USD)=0.031139999999999998 \n",
      "\n",
      "CPU times: user 14.5 ms, sys: 0 ns, total: 14.5 ms\n",
      "Wall time: 2.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pprint import pformat\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "from langchain.evaluation.criteria import LabeledCriteriaEvalChain\n",
    "from langchain.evaluation.criteria import CriteriaEvalChain\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "\n",
    "    qa_llm = get_llm_model()\n",
    "\n",
    "    questions = [ qa['question'] for qa in questions_answers]\n",
    "    print(f\"\\n questions \\n {pformat(questions)} \\n\")\n",
    "\n",
    "    results = llm.batch(questions)\n",
    "    ## reshape the result so that it fits QAEval expectations\n",
    "    predictions = [ {'question': qa['question'], 'answer': qa['answer'], 'result': r} \n",
    "                   for (qa, r) in zip(questions_answers, results)]         \n",
    "    print(f\"\\n predictions \\n {pformat(predictions)} \\n\")\n",
    "\n",
    "    # Start your eval chain\n",
    "    eval_llm = OpenAI(temperature=0.7)\n",
    "\n",
    "    eval_chain = QAEvalChain.from_llm(llm)\n",
    "\n",
    "    # Have it grade itself. The code below helps the eval_chain know where the different parts are\n",
    "    graded_outputs = eval_chain.evaluate(questions_answers,\n",
    "                                         predictions,\n",
    "                                         question_key=\"question\",\n",
    "                                         prediction_key=\"result\",\n",
    "                                         answer_key='answer')\n",
    "\n",
    "    print(f\"\\n graded output \\n {pformat(graded_outputs)} \\n\")\n",
    "\n",
    "    print(cb)\n",
    "    print(f\"token used={cb.total_tokens} total cost (USD)={cb.total_cost} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2f6cd9-1c49-4c9e-adf3-b37eb03aadf0",
   "metadata": {},
   "source": [
    "apply allows you run the chain against a list of inputs:\n",
    "\n",
    "llm_chain.apply(input_list)\n",
    "\n",
    "    [{'text': '\\n\\nSocktastic!'},\n",
    "     {'text': '\\n\\nTechCore Solutions.'},\n",
    "     {'text': '\\n\\nFootwear Factory.'}]\n",
    "\n",
    "generate is similar to apply, except it return an LLMResult instead of string. LLMResult often contains useful generation such as token usages and finish reason.\n",
    "llm_chain.generate(input_list)\n",
    "\n",
    "    LLMResult(generations=[[Generation(text='\\n\\nSocktastic!', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nTechCore Solutions.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nFootwear Factory.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'prompt_tokens': 36, 'total_tokens': 55, 'completion_tokens': 19}, 'model_name': 'text-davinci-003'})\n",
    "\n",
    "\n",
    "predict is similar to run method except that the input keys are specified as keyword arguments instead of a Python dict.\n",
    "# Single input example\n",
    "llm_chain.predict(product=\"colorful socks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff22010e-88f8-4ed4-8fca-5ac438e87336",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain \n",
    "https://python.langchain.com/docs/modules/chains/\n",
    "\n",
    "LLMChain \n",
    "https://python.langchain.com/docs/modules/chains/foundational/llm_chain\n",
    "\n",
    "QAEvalChain\n",
    "https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.qa.eval_chain.QAEvalChain.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d755963e-a160-4841-9d17-58f33be71489",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-3:615547856133:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
